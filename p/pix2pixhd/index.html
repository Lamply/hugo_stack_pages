<!doctype html><html lang=zh-cn dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content='图像翻译的经典论文'><title>Pix2PixHD</title>
<link rel=canonical href=/p/pix2pixhd/><link rel=stylesheet href=/scss/style.min.d8118f156935b64eca93aca758476adca858d2c47354971654d9bd2933a0e45f.css><meta property='og:title' content='Pix2PixHD'><meta property='og:description' content='图像翻译的经典论文'><meta property='og:url' content='/p/pix2pixhd/'><meta property='og:site_name' content='次二小栈'><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:published_time' content='2020-11-09T00:00:00+00:00'><meta property='article:modified_time' content='2020-11-09T00:00:00+00:00'><meta property='og:image' content='/p/pix2pixhd/pix2pix_arch.webp'><meta name=twitter:title content="Pix2PixHD"><meta name=twitter:description content="图像翻译的经典论文"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content='/p/pix2pixhd/pix2pix_arch.webp'><link rel="shortcut icon" href=/letter-l.png></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label=切换菜单>
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/easy-peasy_50UJjNJE17_hua2b247eca74a7b47aad15a4079221f7e_48600_300x0_resize_box_3.png width=300 height=300 class=site-logo loading=lazy alt=Avatar></a></figure><div class=site-meta><h1 class=site-name><a href=/>次二小栈</a></h1><h2 class=site-description>现在可以公开的笔记</h2></div></header><ol class=social-menu><li><a href=https://github.com/Lamply target=_blank title=GitHub rel=me><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>主页</span></a></li><li><a href=/archives/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>归档</span></a></li><li><a href=/search/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>搜索</span></a></li><li><a href=/%E9%93%BE%E6%8E%A5/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg>
<span>链接</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>暗色模式</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">目录</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#pix2pixhd>Pix2PixHD</a><ol><li><a href=#main-contribution>Main Contribution</a></li><li><a href=#architecture>Architecture</a></li><li><a href=#implementation-notes>Implementation Notes</a></li><li><a href=#experiments>Experiments</a></li></ol></li><li><a href=#vid2vid>Vid2Vid</a></li></ol></nav></div></section></aside><main class="main full-width"><article class="has-image main-article"><header class=article-header><div class=article-image><a href=/p/pix2pixhd/><img src=/p/pix2pixhd/pix2pix_arch_hu6199731767d3a35c48a188e67800c4b9_22146_800x0_resize_q75_h2_box_2.webp srcset="/p/pix2pixhd/pix2pix_arch_hu6199731767d3a35c48a188e67800c4b9_22146_800x0_resize_q75_h2_box_2.webp 800w, /p/pix2pixhd/pix2pix_arch_hu6199731767d3a35c48a188e67800c4b9_22146_1600x0_resize_q75_h2_box_2.webp 1600w" width=800 height=225 loading=lazy alt="Featured image of post Pix2PixHD"></a></div><div class=article-details><header class=article-category><a href=/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/ style=background-color:#2a9d8f;color:#fff>论文笔记</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/p/pix2pixhd/>Pix2PixHD</a></h2><h3 class=article-subtitle>图像翻译的经典论文</h3></div><footer class=article-time><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>Nov 09, 2020</time></div><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>阅读时长: 4 分钟</time></div></footer></div></header><section class=article-content><h2 id=pix2pixhd>Pix2PixHD</h2><p>《High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs》： <a class=link href=https://arxiv.org/pdf/1711.11585.pdf target=_blank rel=noopener>https://arxiv.org/pdf/1711.11585.pdf</a></p><h3 id=main-contribution>Main Contribution</h3><ol><li>增加一个 loss：真假样本的 Discriminator 的中间特征距离（Feature matching loss），再配合 VGG 之类 Perceptual Loss。<em>算是最可看的一点。</em></li><li>高分辨率生成。Generator 使用 Coarse-to-Fine 逐步生成高分辨率，也就是论文里的先训练低分辨率 Global，然后在外面套一层 Local Enhancer 把中间特征跳过去加起来以求得到高清高分辨率。Discriminator 则送进两种尺度的图像。<em>经过实际验证多尺度输出+多尺度监督会更好，这些都算是落地标配了。</em></li></ol><h3 id=architecture>Architecture</h3><p>pix2pix：使用 UNet generator + patch-based discriminator，输出 256x256，直接输出高分辨率的话不稳定且质量低，于是进行改良。</p><ol><li>Coarse-to-fine generator
<img src=/p/pix2pixhd/pix2pix_arch.webp width=1021 height=287 srcset="/p/pix2pixhd/pix2pix_arch_hu6199731767d3a35c48a188e67800c4b9_22146_480x0_resize_q75_h2_box_2.webp 480w, /p/pix2pixhd/pix2pix_arch_hu6199731767d3a35c48a188e67800c4b9_22146_1024x0_resize_q75_h2_box_2.webp 1024w" loading=lazy alt=arch class=gallery-image data-flex-grow=355 data-flex-basis=853px></li></ol><p>G1 为 global generator 处理 1024x512 图像，G2 为 local enhancer 处理 2048x1024 图像，同样的可以扩增出 G3 等等<br>G1 采用架构为：https://arxiv.org/abs/1603.08155 ，其中提到的两个 Perceptual Loss Functions</p><div align=center><img src=FeatureReconstructionLoss.webp width=50%>
<img src=StyleReconstructionLoss.webp width=50%></div><ol start=2><li><p>Multi-scale discriminators<br>高分辨率需要大的感受野，而更大的卷积核或者更深的网络会有潜在过拟合的可能，且内存增长也很大。于是采用图像金字塔，对输入图缩小几次，得出几种尺度的图像，再根据缩放次数建立多个相同架构的 discriminators 来分别处理各自尺度（Patched Base），最后综合起来得出结果</p></li><li><p>Feature matching loss</p></li></ol><div align=center><img src=FeatureMatchingLoss.webp width=60%></div>使用各个 discriminator 各层特征的 L1 loss 来让生成图像和真实图像在判别器特征层面上相似，这可以说是类似 Perceptual Loss（在超分辨率和风格迁移很有用的方法）。实验进一步表明，一起使用会有更多提升
结合 Feature matching loss 和 GAN loss，得到最终的 loss：<div align=center><img src=FinalLoss.webp width=60%></div><ol start=4><li><p>Instance maps<br>为了解决 semantic label 无法区分物体的缺点，需要引入 instance maps，但是由于事先不确定 instance 个数，所以不好实现。基于此，作者指出 boundary 才是其中最重要的信息，先计算出 instance boundary map（四邻域里有不同的 label 则为 1，否则为 0），再 concat 一起送入 generator 和 discriminator</p></li><li><p>Image manipulation<br>为了使 manipulation 结果多样化且合理，加入 instance-level feature embedding，和 semantic label 一起作为 generator 输入。具体来说，需要额外训练一个 encoder-decoder，最后一层按 instance 进行平均池化，再将池化结果 broadcast 到 instance 每个像素。这样处理完整个训练集后，对各类别使用 K-means 就可以得出多种 instance feature，推理时随机选取一种 concat 输入进行 generate 就可以完成目的。encoder-decoder 的具体训练方法见论文 3.4。</p></li></ol><h3 id=implementation-notes>Implementation Notes</h3><ol><li>先训练 G1 再训练 G2，最后合在一起 fine-tune，作者提到此多分辨率 pipeline 易于建立，而且一般两种尺度就足够了</li><li>为了配合 Instance maps，semantic label 采用的 one-hot 形式</li><li>训练方法使用 LSGAN，Feature matching loss 的系数为 10</li></ol><h3 id=experiments>Experiments</h3><ol><li>使用了语义分割网络对真实图像和生成图像进行分割，比较两者的 mIoU 等差异，结果 pix2pixHD 的方法得到的分割指标接近使用真实图像的指标：0.6389 : 0.6857</li><li>Human A/B tests</li></ol><ul><li>非限制时间，500 张 Cityscapes 图像比较 10 次，产生 5000 个结果，统计在两种方法中选取其中一个的概率。结果：未使用 instance map 下 pix2pixHD 和 pix2pix 是 94% : 6%，pix2pixHD 和 CRN 是 85% : 15%，VGG Perceptual Loss 似乎没有起到明显的正负面倾向，对结果影响在 1% 内</li><li>限制时间，随机在 1/8s - 8s 间选取时间来展示给受试者，判断那张图片更好，据称可以看出需要多长时间才能意识到两者的差异，大概就是某种程度上能比较差异的粗略显著性</li></ul><div align=center><img src=exp2.webp width=70%></div>- 非限制 loss 比较，GAN + Feature matching + VGG Perceptual loss 比上单独 GAN loss、GAN + Feature matching loss 的 preference rate 分别为 68.55%，58.90%，稍微有一些提升，但不是很明显<p><em>还剩下了几个实验，但这些实际效果都存疑，其实作用都不是特别明显，实验有些偏颇，generator 和 loss 方面的提升应该是最明显的</em></p><p><strong>参考：</strong></p><ol><li><a class=link href=https://zhuanlan.zhihu.com/p/56808180 target=_blank rel=noopener>https://zhuanlan.zhihu.com/p/56808180</a></li><li><a class=link href="https://zhuanlan.zhihu.com/p/68779906?from_voters_page=true" target=_blank rel=noopener>https://zhuanlan.zhihu.com/p/68779906?from_voters_page=true</a></li></ol><h2 id=vid2vid>Vid2Vid</h2><p>Pix2PixHD 的视频生成改进，主要是针对 temporally incoherent，我没有看论文，只看了一下代码和博客，Generator 大致的修改就是：</p><ol><li>Pix2PixHD Global 的本体不变，输入改成前后多帧 concat 起来的图像，并且增加一支将过去几帧生成图像提取出特征加到主支上的分支，大概就是为了利用过去生成的图像来生成现在的图像。这样的话头几帧理论上就需要单独另外提供。</li><li>分出一支用来生成光流图和表示模糊程度？的掩码，用光流图 warp 最近一帧生成图像，根据掩码在 warp 的前一帧生成图和当前生成图中做加权，得出最终结果。</li><li>如果有前背景的掩码的话，再加一支类似 2 的分支，生成前景图和前景图掩码然后加权。</li></ol><p>也就是说如果不使用前背景的话，实际需要两个 Pix2PixHD 的网络，输入增加前几帧，以及中间提取前几次生成图的特征后的耦合，还有一个光流 warp 的后处理。下面的图非常直观：
<a class=link href=https://img-blog.csdnimg.cn/2019030516345690.gif target=_blank rel=noopener>https://img-blog.csdnimg.cn/2019030516345690.gif</a></p><p><strong>参考：</strong></p><ol><li><a class=link href=https://blog.csdn.net/maqunfi/article/details/88186935 target=_blank rel=noopener>https://blog.csdn.net/maqunfi/article/details/88186935</a></li></ol></section><footer class=article-footer><section class=article-copyright><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>Licensed under CC BY-NC-SA 4.0</span></section></footer></article><aside class=related-content--wrapper><h2 class=section-title>相关文章</h2><div class=related-content><div class="flex article-list--tile"><article class=has-image><a href=/p/stylegan/><div class=article-image><img src=/p/stylegan/arch1.66d8dda14bf460b7493b1d8a7ab276a7_hub988efc80824a295e46270c7ce0704e1_32816_250x150_fill_q75_h2_box_smart1_2.webp width=250 height=150 loading=lazy alt="Featured image of post StyleGAN" data-hash="md5-ZtjdoUv0YLdJOx2KerJ2pw=="></div><div class=article-details><h2 class=article-title>StyleGAN</h2></div></a></article><article><a href=/p/deeplab%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E7%AE%80%E7%95%A5%E8%AE%B0%E5%BD%95/><div class=article-details><h2 class=article-title>DeepLab系列论文简略记录</h2></div></a></article><article><a href=/p/mobilenet-v2/><div class=article-details><h2 class=article-title>MobileNet V2</h2></div></a></article><article><a href=/p/shufflenet/><div class=article-details><h2 class=article-title>ShuffleNet</h2></div></a></article><article class=has-image><a href=/p/coordinating-filters-for-faster-deep-neural-networks/><div class=article-image><img src=/p/coordinating-filters-for-faster-deep-neural-networks/ForceRegularization.dd5533f1f514e3cb3f518d33d272c68a_hucc896f4ea16421fc559b00a1346895c8_187355_250x150_fill_box_smart1_3.png width=250 height=150 loading=lazy alt="Featured image of post Coordinating Filters for Faster Deep Neural Networks" data-hash="md5-3VUz8fUU48s/UY0z0nLGig=="></div><div class=article-details><h2 class=article-title>Coordinating Filters for Faster Deep Neural Networks</h2></div></a></article></div></div></aside><script src=https://giscus.app/client.js data-repo=Lamply/hugo_stack_pages data-repo-id data-category data-category-id data-mapping=title data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=top data-theme=light data-lang=en crossorigin=anonymous async></script><script>function setGiscusTheme(e){let t=document.querySelector("iframe.giscus-frame");t&&t.contentWindow.postMessage({giscus:{setConfig:{theme:e}}},"https://giscus.app")}(function(){addEventListener("message",t=>{if(event.origin!=="https://giscus.app")return;e()}),window.addEventListener("onColorSchemeChange",e);function e(){setGiscusTheme(document.documentElement.dataset.scheme==="light"?"light":"dark_dimmed")}})()</script><footer class=site-footer><section class=copyright>&copy;
2020 -
2024 次二小栈</section><section class=powerby>使用 <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> 构建<br>主题 <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.23.0>Stack</a></b> 由 <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a> 设计</section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.js defer></script><script>(function(){const e=document.createElement("link");e.href="/local.google.fonts.css",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>
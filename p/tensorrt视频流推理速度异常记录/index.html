<!doctype html><html lang=zh-cn dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content='想要极致的N卡加速或许需要英伟达的协助'><title>TensorRT视频流推理速度异常记录</title>
<link rel=canonical href=/p/tensorrt%E8%A7%86%E9%A2%91%E6%B5%81%E6%8E%A8%E7%90%86%E9%80%9F%E5%BA%A6%E5%BC%82%E5%B8%B8%E8%AE%B0%E5%BD%95/><link rel=stylesheet href=/scss/style.min.d8118f156935b64eca93aca758476adca858d2c47354971654d9bd2933a0e45f.css><meta property='og:title' content='TensorRT视频流推理速度异常记录'><meta property='og:description' content='想要极致的N卡加速或许需要英伟达的协助'><meta property='og:url' content='/p/tensorrt%E8%A7%86%E9%A2%91%E6%B5%81%E6%8E%A8%E7%90%86%E9%80%9F%E5%BA%A6%E5%BC%82%E5%B8%B8%E8%AE%B0%E5%BD%95/'><meta property='og:site_name' content='次二小栈'><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='TensorRT'><meta property='article:tag' content='模型加速和压缩'><meta property='article:published_time' content='2022-06-28T00:00:00+00:00'><meta property='article:modified_time' content='2022-06-28T00:00:00+00:00'><meta name=twitter:title content="TensorRT视频流推理速度异常记录"><meta name=twitter:description content="想要极致的N卡加速或许需要英伟达的协助"><link rel="shortcut icon" href=/letter-l.png></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label=切换菜单>
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/easy-peasy_50UJjNJE17_hua2b247eca74a7b47aad15a4079221f7e_48600_300x0_resize_box_3.png width=300 height=300 class=site-logo loading=lazy alt=Avatar></a></figure><div class=site-meta><h1 class=site-name><a href=/>次二小栈</a></h1><h2 class=site-description>现在可以公开的笔记</h2></div></header><ol class=social-menu><li><a href=https://github.com/Lamply target=_blank title=GitHub rel=me><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>主页</span></a></li><li><a href=/archives/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>归档</span></a></li><li><a href=/search/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>搜索</span></a></li><li><a href=/%E9%93%BE%E6%8E%A5/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg>
<span>链接</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>暗色模式</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">目录</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#实验一>实验一</a></li><li><a href=#实验二>实验二</a></li><li><a href=#实验三>实验三</a></li></ol></nav></div></section></aside><main class="main full-width"><article class=main-article><header class=article-header><div class=article-details><header class=article-category><a href=/categories/%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95/>问题记录</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/p/tensorrt%E8%A7%86%E9%A2%91%E6%B5%81%E6%8E%A8%E7%90%86%E9%80%9F%E5%BA%A6%E5%BC%82%E5%B8%B8%E8%AE%B0%E5%BD%95/>TensorRT视频流推理速度异常记录</a></h2><h3 class=article-subtitle>想要极致的N卡加速或许需要英伟达的协助</h3></div><footer class=article-time><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>Jun 28, 2022</time></div><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>阅读时长: 4 分钟</time></div></footer></div></header><section class=article-content><p>links: [[2022-06-27-Week]], [[103-问题汇总#^7d1ab9]]</p><p>这个问题是在优化云端模型时出现的，设计了个更小的模型准备替换大的模型，结果发现模拟视频推理时耗时会出现异常波动，最终达不到优化预期。经过分析发现只在两次推理之间存在一点时间间隔时才会出现这个问题，很是诡异。因为是在云端容器上跑的，没法进行 Nsight 分析，而且我也不懂 GPU，只能默默记录下来。</p><p>环境：</p><ul><li>Ubuntu 16.04</li><li>TensorRT 7.2.1</li><li>CUDA 10.2</li><li>cuDNN 8.0</li><li>Python 3.6</li><li>PyTorch 1.7.1</li><li>torch2trt 0.1.0</li><li>Tesla T4</li><li>Intel(R) Xeon(R) Platinum 8255C CPU @ 2.50GHz（up to 10-core）</li></ul><p>模型构成：</p><ol><li>localL（优化 nnt 版本） 和 localC 模型转换成 TensorRT 引擎</li><li>FP16 和 INT8</li><li>1024x576 和 768x448 推理分辨率</li><li>初始化预热 20 次</li></ol><p>分解测试流程为几个步骤：</p><ol><li>OpenCV 从 nas 盘读取视频（I/O），转换成 YUV 字节流，以及其余模拟线上环境的配置和 log（I/O）</li><li>脚本前处理，涉及 to device 数据传输和少量 CUDA 计算</li><li>模型推理，里面是黑箱，涉及显卡内部计算和传输，不涉及 to host 数据传输</li><li>脚本后处理，涉及 to host 数据传输和少量 CUDA 计算</li><li>后续 log（I/O），以及调用 <code>time.sleep</code> 系统睡眠模拟流式处理保持帧率</li></ol><p>存在三种场景，未能达成可预见的统一：</p><ol><li>测试模型推理，采用相同输入的连续推理的形式</li><li>测试脚本，不涉及线上环境模拟部分</li><li>测试线上环境</li></ol><p>测试方法只记录脚本内部（步骤 2，3，4）耗时，采用 <code>torch.cuda.synchronize()</code>+<code>timeit.default_timer()</code> 的方式严格记录每一步耗时</p><h2 id=实验一>实验一</h2><p>localL 1024x576 FP16 和 INT8 做 baseline</p><p>测试线上环境，52 帧视频流：</p><ul><li>FP16 耗时 19ms（模型 17ms），GPU 占用 40% 左右，模型推理在一定范围内波动<ul><li><img src=/p/tensorrt%E8%A7%86%E9%A2%91%E6%B5%81%E6%8E%A8%E7%90%86%E9%80%9F%E5%BA%A6%E5%BC%82%E5%B8%B8%E8%AE%B0%E5%BD%95/20220628163611.webp width=374 height=247 srcset="/p/tensorrt%E8%A7%86%E9%A2%91%E6%B5%81%E6%8E%A8%E7%90%86%E9%80%9F%E5%BA%A6%E5%BC%82%E5%B8%B8%E8%AE%B0%E5%BD%95/20220628163611_hu9b4516e03a97615eb57f73f74e85dd06_9778_480x0_resize_q75_h2_box_2.webp 480w, /p/tensorrt%E8%A7%86%E9%A2%91%E6%B5%81%E6%8E%A8%E7%90%86%E9%80%9F%E5%BA%A6%E5%BC%82%E5%B8%B8%E8%AE%B0%E5%BD%95/20220628163611_hu9b4516e03a97615eb57f73f74e85dd06_9778_1024x0_resize_q75_h2_box_2.webp 1024w" loading=lazy class=gallery-image data-flex-grow=151 data-flex-basis=363px></li><li>前后处理耗时在个别时候有剧烈变化</li></ul></li><li>INT8 耗时 15ms（模型 13.2ms），模型推理波动很小<ul><li><img src=/p/tensorrt%E8%A7%86%E9%A2%91%E6%B5%81%E6%8E%A8%E7%90%86%E9%80%9F%E5%BA%A6%E5%BC%82%E5%B8%B8%E8%AE%B0%E5%BD%95/20220628170236.webp width=386 height=244 srcset="/p/tensorrt%E8%A7%86%E9%A2%91%E6%B5%81%E6%8E%A8%E7%90%86%E9%80%9F%E5%BA%A6%E5%BC%82%E5%B8%B8%E8%AE%B0%E5%BD%95/20220628170236_hu5b635d502ec56feee62cd684edacef57_7148_480x0_resize_q75_h2_box_2.webp 480w, /p/tensorrt%E8%A7%86%E9%A2%91%E6%B5%81%E6%8E%A8%E7%90%86%E9%80%9F%E5%BA%A6%E5%BC%82%E5%B8%B8%E8%AE%B0%E5%BD%95/20220628170236_hu5b635d502ec56feee62cd684edacef57_7148_1024x0_resize_q75_h2_box_2.webp 1024w" loading=lazy class=gallery-image data-flex-grow=158 data-flex-basis=379px></li><li>前后处理耗时在个别时候有剧烈变化</li></ul></li></ul><p>测试脚本，循环 100 次：</p><ul><li>FP16 耗时 20.5ms（模型 18.8ms），GPU 占用 90% 以上，模型推理耗时依然波动<ul><li><img src=/p/tensorrt%E8%A7%86%E9%A2%91%E6%B5%81%E6%8E%A8%E7%90%86%E9%80%9F%E5%BA%A6%E5%BC%82%E5%B8%B8%E8%AE%B0%E5%BD%95/20220628162953.webp width=376 height=249 srcset="/p/tensorrt%E8%A7%86%E9%A2%91%E6%B5%81%E6%8E%A8%E7%90%86%E9%80%9F%E5%BA%A6%E5%BC%82%E5%B8%B8%E8%AE%B0%E5%BD%95/20220628162953_hu787089145284c611b1aed997b16e9843_6684_480x0_resize_q75_h2_box_2.webp 480w, /p/tensorrt%E8%A7%86%E9%A2%91%E6%B5%81%E6%8E%A8%E7%90%86%E9%80%9F%E5%BA%A6%E5%BC%82%E5%B8%B8%E8%AE%B0%E5%BD%95/20220628162953_hu787089145284c611b1aed997b16e9843_6684_1024x0_resize_q75_h2_box_2.webp 1024w" loading=lazy class=gallery-image data-flex-grow=151 data-flex-basis=362px></li><li>耗时比测试线上环境高，怀疑是 GPU 占用原因</li><li>添加 20ms 延时，耗时降低回 16.8ms 水平（比线上稍低一些），波动更明显更规律</li><li><img src=/p/tensorrt%E8%A7%86%E9%A2%91%E6%B5%81%E6%8E%A8%E7%90%86%E9%80%9F%E5%BA%A6%E5%BC%82%E5%B8%B8%E8%AE%B0%E5%BD%95/20220628163236.webp width=373 height=248 srcset="/p/tensorrt%E8%A7%86%E9%A2%91%E6%B5%81%E6%8E%A8%E7%90%86%E9%80%9F%E5%BA%A6%E5%BC%82%E5%B8%B8%E8%AE%B0%E5%BD%95/20220628163236_hu5ed591b04cfbdd40682693e4e19c2838_8062_480x0_resize_q75_h2_box_2.webp 480w, /p/tensorrt%E8%A7%86%E9%A2%91%E6%B5%81%E6%8E%A8%E7%90%86%E9%80%9F%E5%BA%A6%E5%BC%82%E5%B8%B8%E8%AE%B0%E5%BD%95/20220628163236_hu5ed591b04cfbdd40682693e4e19c2838_8062_1024x0_resize_q75_h2_box_2.webp 1024w" loading=lazy class=gallery-image data-flex-grow=150 data-flex-basis=360px></li><li>前后处理耗时在个别时候有剧烈变化</li></ul></li><li>INT8 模型推理耗时有分层情况出现，稳定后和线上环境耗时一致，延长预热后分层消失<ul><li><img src=/p/tensorrt%E8%A7%86%E9%A2%91%E6%B5%81%E6%8E%A8%E7%90%86%E9%80%9F%E5%BA%A6%E5%BC%82%E5%B8%B8%E8%AE%B0%E5%BD%95/20220628170625.webp width=388 height=249 srcset="/p/tensorrt%E8%A7%86%E9%A2%91%E6%B5%81%E6%8E%A8%E7%90%86%E9%80%9F%E5%BA%A6%E5%BC%82%E5%B8%B8%E8%AE%B0%E5%BD%95/20220628170625_huc68952150e9055d5b0ef071f74d68e63_6454_480x0_resize_q75_h2_box_2.webp 480w, /p/tensorrt%E8%A7%86%E9%A2%91%E6%B5%81%E6%8E%A8%E7%90%86%E9%80%9F%E5%BA%A6%E5%BC%82%E5%B8%B8%E8%AE%B0%E5%BD%95/20220628170625_huc68952150e9055d5b0ef071f74d68e63_6454_1024x0_resize_q75_h2_box_2.webp 1024w" loading=lazy class=gallery-image data-flex-grow=155 data-flex-basis=373px></li></ul></li></ul><p>测试模型推理，预热 20 次，循环 100 次：
- FP16 耗时 20.4ms，GPU 跑满
- INT8 耗时 15.4ms，GPU 跑满</p><p>疑问：</p><ol><li>为什么推理时间会有规律性波动</li><li>为什么两次推理之间的间隔会影响 GPU 推理速度（为什么连续推理会更慢）</li><li>为什么前后处理耗时会偶现高峰值</li></ol><p>自答：</p><ul><li>上述几个问题都表明了一个现象，就是 TensorRT 推理时延和 GPU 利用率相关，这也是几种测试场景下出现不一致的主要原因</li><li>GPU 利用率是统计数据，真正的原因或许在于 GPU 内部管理程序有一定的耗时，两次推理之间必需要经过这一段时间，所以利用 GPU 的空闲时间可以掩盖掉这部分耗时，但当 GPU 相当繁忙时（常有的事）这部分耗时就会显露出来</li><li>还有的原因或许和 TensorRT 对连续推理的优化有关，这部分暂且不清楚</li><li>所以实际测试按照 GPU 利用率作为标准或许会更好点，但照目前观察，耗时基本可以代表 GPU 利用率</li><li>当然感觉也有可能是显卡服务器平台策略有修改了什么，不好说&mldr;</li></ul><h2 id=实验二>实验二</h2><p>localC 768x448 FP16 INT8，小模型推理</p><p>测试线上环境，8000 帧视频流：</p><ul><li>INT8 模型推理时间呈现诡异的分层（50 iter 跃变，100 iter 稳定），稳定后 GPU 占用 33% 左右<ul><li><img src=/p/tensorrt%E8%A7%86%E9%A2%91%E6%B5%81%E6%8E%A8%E7%90%86%E9%80%9F%E5%BA%A6%E5%BC%82%E5%B8%B8%E8%AE%B0%E5%BD%95/20220628195746.webp width=374 height=252 srcset="/p/tensorrt%E8%A7%86%E9%A2%91%E6%B5%81%E6%8E%A8%E7%90%86%E9%80%9F%E5%BA%A6%E5%BC%82%E5%B8%B8%E8%AE%B0%E5%BD%95/20220628195746_hue5550d4672a4aea17d49fbcade932959_3506_480x0_resize_q75_h2_box_2.webp 480w, /p/tensorrt%E8%A7%86%E9%A2%91%E6%B5%81%E6%8E%A8%E7%90%86%E9%80%9F%E5%BA%A6%E5%BC%82%E5%B8%B8%E8%AE%B0%E5%BD%95/20220628195746_hue5550d4672a4aea17d49fbcade932959_3506_1024x0_resize_q75_h2_box_2.webp 1024w" loading=lazy class=gallery-image data-flex-grow=148 data-flex-basis=356px></li><li>可多次复现，分层的时间节点大致相同只会略有偏移</li><li>可以确定和延时长短有关，延时越长，分层整体越往左移</li></ul></li></ul><p>测试脚本，循环 800 次：
- INT8 模型推理在添加 20ms 延时下呈现诡异分层，去掉延时则模型推理耗时正常（4.66ms）
- FP16 模型推理在添加 20ms 延时下也呈现诡异分层，去掉延时则模型推理耗时正常（5.3ms）</p><p>疑问：</p><ol><li>为什么推理耗时会表现出阶梯渐慢的样子</li><li>为什么最终耗时会稳定在成倍增长的地步</li></ol><p>自答：</p><ul><li>该表现可稳定复现且有明显规律，说明很可能是撞到了某内部机制的误区</li><li>延时越短，表现越正常；延时越长，越早出现耗时跃变，分层时间也会缩短。也就是可能和实验一是同一类问题</li><li>需要进一步验证</li></ul><h2 id=实验三>实验三</h2><p>寻找分层规律，测试所有模型</p><p>测试模型推理，预热 50 次，循环 200 次（值为：毫秒）：</p><div class=table-wrapper><table><thead><tr><th style=text-align:center>分辨率/模型</th><th style=text-align:center>localC FP16</th><th style=text-align:center>localC INT8</th><th style=text-align:center>localL FP16</th><th style=text-align:center>localL INT8</th></tr></thead><tbody><tr><td style=text-align:center>1024x576</td><td style=text-align:center>8.2</td><td style=text-align:center>7.2</td><td style=text-align:center>19.8</td><td style=text-align:center>15.2</td></tr><tr><td style=text-align:center>768x448</td><td style=text-align:center>5.4</td><td style=text-align:center>4.7</td><td style=text-align:center>12.6</td><td style=text-align:center>9.9</td></tr></tbody></table></div><p>测试脚本，循环 800 次，延时 20ms（值为：跃变时次数 / 稳定时次数，√ 为无分层）：</p><div class=table-wrapper><table><thead><tr><th style=text-align:center>分辨率/模型</th><th style=text-align:center>localC FP16</th><th style=text-align:center>localC INT8</th><th style=text-align:center>localL FP16</th><th style=text-align:center>localL INT8</th></tr></thead><tbody><tr><td style=text-align:center>1024x576</td><td style=text-align:center>50/300</td><td style=text-align:center>60/300</td><td style=text-align:center>√</td><td style=text-align:center>√</td></tr><tr><td style=text-align:center>768x448</td><td style=text-align:center>60/275</td><td style=text-align:center>60/175</td><td style=text-align:center>50/125</td><td style=text-align:center>60/300</td></tr></tbody></table></div><p>分层稳定后耗时分别为（值为：毫秒）：</p><div class=table-wrapper><table><thead><tr><th style=text-align:center>分辨率/模型</th><th style=text-align:center>localC FP16</th><th style=text-align:center>localC INT8</th><th style=text-align:center>localL FP16</th><th style=text-align:center>localL INT8</th></tr></thead><tbody><tr><td style=text-align:center>1024x576</td><td style=text-align:center>12.2</td><td style=text-align:center>12.1</td><td style=text-align:center>16.8</td><td style=text-align:center>13.2</td></tr><tr><td style=text-align:center>768x448</td><td style=text-align:center>11.4</td><td style=text-align:center>10.1</td><td style=text-align:center>12.4</td><td style=text-align:center>12.4</td></tr></tbody></table></div><p>后续实验：</p><ul><li>增长延时后全部模型都会分层，减小延时后全部模型不会分层</li><li>分层最终时间也与延时有关，延时越长，稳定后推理耗时越长</li><li>将延时函数加到 测试模型推理 场景（也就是步骤 3）中，分层现象依然可以复现，所以可以排除外设总线相关的原因，问题出现在显卡内部</li></ul><p>自答：</p><ul><li>众所周知 TensorRT 在连续推理中做了优化，或许“连续”是指在一定时间范围内，而且这个时间范围会随模型计算量改变。也就是小模型只能在更短时间内才算连续推理，而大模型则宽裕一些。然后破坏连续推理的模式就会使模型推理速度变慢，但这依然没能解释为什么会经历推理耗时正常-线性减速-耗时稳定的现象</li><li>所以只在这种程度上分析还是有所不足，如果能在本地采用 Nsight Compute 或许能有进一步了解</li></ul></section><footer class=article-footer><section class=article-tags><a href=/tags/tensorrt/>TensorRT</a>
<a href=/tags/%E6%A8%A1%E5%9E%8B%E5%8A%A0%E9%80%9F%E5%92%8C%E5%8E%8B%E7%BC%A9/>模型加速和压缩</a></section><section class=article-copyright><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>Licensed under CC BY-NC-SA 4.0</span></section></footer></article><aside class=related-content--wrapper><h2 class=section-title>相关文章</h2><div class=related-content><div class="flex article-list--tile"><article><a href=/p/caffe%E4%BD%BF%E7%94%A8%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95/><div class=article-details><h2 class=article-title>Caffe使用问题记录</h2></div></a></article><article><a href=/p/%E9%9B%B6%E6%A0%B8%E7%8E%B0%E8%B1%A1/><div class=article-details><h2 class=article-title>零核现象</h2></div></a></article></div></div></aside><script src=https://giscus.app/client.js data-repo=Lamply/hugo_stack_pages data-repo-id data-category data-category-id data-mapping=title data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=top data-theme=light data-lang=en crossorigin=anonymous async></script><script>function setGiscusTheme(e){let t=document.querySelector("iframe.giscus-frame");t&&t.contentWindow.postMessage({giscus:{setConfig:{theme:e}}},"https://giscus.app")}(function(){addEventListener("message",t=>{if(event.origin!=="https://giscus.app")return;e()}),window.addEventListener("onColorSchemeChange",e);function e(){setGiscusTheme(document.documentElement.dataset.scheme==="light"?"light":"dark_dimmed")}})()</script><footer class=site-footer><section class=copyright>&copy;
2020 -
2024 次二小栈</section><section class=powerby>使用 <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> 构建<br>主题 <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.23.0>Stack</a></b> 由 <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a> 设计</section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.js defer></script><script>(function(){const e=document.createElement("link");e.href="/local.google.fonts.css",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>
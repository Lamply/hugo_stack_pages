[{"content":"解释型语言，运行时通过特定解释器来解释并运行。一般官方使用的解释器是 CPython，也就是底层使用 C 实现，另外大部分第三方库（像 Numpy）也都是以 CPython 调用为目的来实现的。\n在 import 库时会生成带解释器信息后缀的字节码（常为 __pycache__/*.cpython-36.pyc）以避免太多重复编译，可以通过 export PYTHONDONTWRITEBYTECODE=1 来避免生成字节码，find . -type f -name \u0026quot;*.py[co]\u0026quot; -delete -or -type d -name \u0026quot;__pycache__\u0026quot; -delete 来删除这些文件。\n注意，Cython 和 CPython 不同，Cython 是一种 C/C++ 和 Python 互取所长混合使用的语言，最终输出 C 代码。\n安装 推荐使用 pyenv 等工具安装管理，使用 ppa 源安装可能会出现问题\n最佳实践 https://pythonguidecn.readthedocs.io/zh/latest/\n库安装和打包 项目中存在 setup.py 是 Python 包构建分发工具所使用的文件，如果需要从源码安装 Python 包一般会使用采用这样的方式\n1 2 python setup.py build python setup.py install 但由于 setuptools 在依赖上存在一些问题，现在已逐步弃用。建议采用 pyproject.toml 来替代 setup.py 来构建工程，具体可参照 https://godatadriven.com/blog/a-practical-guide-to-setuptools-and-pyproject-toml/\n更常见的安装库方法是直接使用 pip 安装\n1 2 3 pip install xxx pip install -e . # 不安装 wheel，而是引用源代码目录，用于开发时即时使用修改的代码 pip install -e .[full] # 安装 extras_require 版本 一般安装的路径是 /usr/local/lib/pythonx.x/dist-packages，但一些自己编译的 python 库可能会安装到 /usr/lib/pythonx.x/site-packages 下，只是为了区分库的来源而已，在普遍使用虚拟环境的现在似乎没什么意义，可以手动移动库统一放到 dist-packages 里\n总的来说，根目录的工程文件有如下这些：\nsetup.py：使用 setuptools 分发打包库的脚本，传统方法，较为固定，现逐步弃用 setup.cfg：为了避免 setup.py 的重复样本代码而使用的 ini 文件 https://setuptools.pypa.io/en/latest/userguide/declarative_config.html#declarative-config pyproject.toml：PEP 517 提出的新的打包分发 Python 模块的 ini 文件，可以指定使用什么工具来进行生成分发 关于 pip 安装缓存可以见 [[Ubuntu系统问题#pip 缓存问题|pip 缓存问题]]\n关于导出使用的 pip 库，在容器中可以直接用 pip freeze \u0026gt; requirements.txt 导出当前环境所有 python 库\n关于更多参见：\nhttps://ianhopkinson.org.uk/2022/02/understanding-setup-py-setup-cfg-and-pyproject-toml-in-python/ https://setuptools.pypa.io/en/latest/build_meta.html Linter 代码检查工具，可以使用 Ruff，集成了很多常见的功能，在 VSCode 上可以使用 auto-fix 以及 Organize import 来整理代码\n1 2 # 在行后面加上 # noqa 表示让 linter 忽略这一行的警告 xxxxxxx = xxxxx # noqa Formatter 自动格式化工具，使用 Black 可以不需要配置\n单元测试 测试的单元要尽可能小而且独立，函数命名采用尽可能长的功能描述性名字\n可以使用 pytest，基本流程就是 Arrange-Act-Assert，需要将文件命名为 test_xxx 格式\n1 2 3 4 5 6 from xxx import func def test_xxxxxx(): x = 10 y = func(x) assert(y == 10) 然后在 tests 文件夹目录终端输入 pytest 就会进行自动测试\n特性 REPL REPL（Read-Eval-Print Loop）是一种顶层交互的 shell，可以输入语言相关的命令打印结果，CLI（Command-line interface）也是这样的东西（见 [[Shell]]）。Python 也带有 REPL 工具，在命令行终端下输入 python 就会打开相应版本的 REPL shell，然后就能执行 Python 命令\n脚本 python 文件 .py 可视作脚本直接用命令行解析器 python xxx.py 解析执行，其执行顺序是由上而下逐行执行，也可以先 python 进入命令行交互模式再逐条输入命令执行或作为模块调用，区别在于不同的 内置变量（内置变量和内置方法格式为 __xxx__，可以通过 vars() 查看内置变量），如果执行脚本则会赋予内置变量 __name__ = '__main__'，所以可以通过如下方法来实现 main 函数（只在作为脚本解析时执行的函数）：\n1 2 3 4 5 6 7 8 9 # 无论是脚本解析还是模块调用都顺序执行 import xxxxxx # 一般提倡在此处定义函数和类而不是执行代码，特别是多进程时需要注意这点 xxxx # 若为脚本解析而非模块调用，则执行 if __name__ == \u0026#39;__main__\u0026#39;: xxxxx 如果要以模块方式运行可以使用 python -m xxx.py 以 import 的方式执行（允许某种类型的相对导入，__package__ 设为当前文件夹）\n此外，Python 还支持以 zipapp 的方式将 Python 目录打包成 zip 压缩文件以通过 python xxx.zip 的方式执行 xxx/__main__.py，方便进行程序分发\n对于有依赖库的工程，可以 pip 安装 pipreqs 库，然后使用 pipreqs ./ --encoding=utf-8 来生成 requirements.txt\n模块 Python 文件一般分为包和模块，编写脚本并且导入相应的包或模块的方法从而实现各种功能，相当于库文件（脚本本身也可视作模块，只是会进入 __main__，和其他语言差不多）。\n包就是文件夹本身，通过在文件夹内部新增 __init__.py 表示该文件夹为包，在该文件中添加代码来使得 import xxxpackage 时进行一些默认行为。在这个层面上，导入包和导入模块可以使用一样的方法达到同样的目的。\n模块就是定义对象方法的库文件，导入模块时一般有两种方式\n绝对导入：import xxx 或者 from xxx import xxx 相对导入：from .xxx import xxx，可以有多个前导点，但不提倡。通过模块属性 __name__ 或者 __package__ （若有）来定位位置 Python 的相对导入做得相对恶心，你无法在使用了相对导入的模块里加入 if __name__ == '__main__': 来保存模块测试代码，因为这样会被视作顶层文件（__name__ 被设置为 __main__，而 __package__ 被设置为 None），从而缺失路径无法完成相对导入 使用 python -m 运行可以让 __package__ 被设置为当前文件夹，运行一定程度的相对导入 对包的内部，推荐使用相对导入，以便对包进行维护。对于运行的脚本，则必须使用绝对导入\n还有在脚本内通过字符串方式来进行导入：\n1 2 3 import importlib mod = importlib.import_module(\u0026#39;xxx_module\u0026#39;) func = getattr(mod, \u0026#39;xxx_func\u0026#39;) # 使用字符串导入函数 对象 Python 一切皆对象，变量名和对象类似于指针和内存的关系，动态创建各种类型的对象并用变量名指向它，从而达到 动态类型 的目的。这意味着 Python 里大部分赋值运算其实都是新建了对象再返回指向对象的别名\nPython 提供可变类型和不可变类型\n1 2 3 4 5 6 7 8 my_list = [1, 2, 3] my_list[0] = 4 # my_list 本身 id 没有改变，但其的元素改变了，是可变类型 x = 6 x = x + 1 # x 变量是一个新的变量，id 发生了改变，是不可变类型 x = \u0026#39;asdx\u0026#39; x[2] = \u0026#39;2\u0026#39; # 报错，字符串也是不可变类型 常数： 参与浮点相关操作要在数字后尾加 . 表明是浮点常数，否则可能会出错 全局变量： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 a = 10 def asd(): a = 3 def asd_global(): global a a = 3 # 函数内 a 为局部变量 asd() # a = 10 # 函数内 a 为全局变量 asd_global() # a = 3 类： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 # python3 自动继承 Object 类 class Name: # 重写初始化函数 def __init__(self, arg1, arg2): # 父类初始化 super().__init__() # self 代表类对象自身，无所谓命名，用于调用类的方法 def func1(self, arg1): pass # 下划线注意：用双下划线开头命名方法时，默认为受保护的方法， # Python 会添加个类前缀以保护函数不被子类覆盖， # 如：这里需要用 a._Name__func2() 才能访问该方法 def __func2(self, arg1): pass # 双下划线包裹时，命名不变，作为内置变量或方法 def __func3__(self, arg1): pass # 单下划线开头，表示是内部使用的方法或函数，属于约定俗成 def _func4(self, arg1): pass https://pythonguidecn.readthedocs.io/zh/latest/writing/structure.html#id14\n函数输入 Python 的函数/方法允许两种传入参数：位置参数、键值参数。位置参数就是通常的入参方式，假设实参和形参是按顺序一一对应的；键值参数就是指明形参对应的是哪个实参\n1 2 3 4 5 6 7 8 9 10 11 12 # a 是位置参数 # b 是键值参数 def test(a, b=1): print(a, b) # 和函数定义不同，实际可以按自己喜欢使用那种输入方式（调用 C 底层库的除外） test(1, b=2) test(a=1, b=2) test(1, 2) # 顺序必须要先位置参数再键值参数，这种情况就不行 test(a=1, 2) *args 保存函数输入多出来的参数，**kwargs 保存使用键-值方式输入的多余参数，同时使用时 **kwargs 要在 *args 后面\n1 2 3 4 5 6 7 8 9 10 11 def test(*args,**kwargs): print(args) print(kwargs) test(1,\u0026#39;2\u0026#39;,None,c=8,d=12) # (1, \u0026#39;2\u0026#39;, None) # {\u0026#39;c\u0026#39;: 8, \u0026#39;d\u0026#39;: 12} # 输入也可以通过字典或元组输入 test(*(1, \u0026#39;2\u0026#39;, None), **{\u0026#39;c\u0026#39;: 8, \u0026#39;d\u0026#39;: 12}) 另外，通过加一个单独的 * 作为入参可以限定位置编码的范围，表示之后的入参只能靠关键词来输入\n1 2 3 4 5 6 7 8 9 10 11 def test1(a=1, b=2) print(a, b) def test2(a=1, *, b=2): print(a, b) # Works test1(1,2) # Not works test2(1,2) Duck Typing If it walks like a duck, swims like a duck, and quacks like a duck, then it probably is a duck.\n一种注重方便的编程方法，不去关注对象的类型，而是关注对象的行为。换句话说，在编程中使用对象提供的方法和特性，而不需要管对象是不是期望的类型\nhttps://devopedia.org/duck-typing 内置变量和属性方法 Python 中对象和实例的属性以字典形式保存，有两种方法查看：\nvars(module)：返回当前对象单个类的属性字典，其实就是返回它的 __dict__ 对象。不输入参数则返回当前环境的符号表 dir(module)：返回当前对象单个类及其基类的属性字典，会返回所有有效属性。不输入参数则返回当前环境的所有对象名字 eval(str)：把 str 内容当成代码执行 常见内置变量：\n__all__：用于控制模块公开的接口的变量，__all__=[\u0026quot;func1\u0026quot;, \u0026quot;func2\u0026quot;] 可以让模块导入时只导入列出来的成员 __base__：class 的基类，只有类有，实例没有，如果要看基类方法的实现，可以 print(inspect.getsource(type(model).__base__.__call__)) __code__：与函数代码层面相关的对象？可以显示当前函数的文件位置，以及形参 __code__.co_varnames 以双下划线包起来的内置方法也叫魔法方法：\n__init__ / __new__：Object 自带的初始化方法，__new__ 会被首先自动调用用于构造返回类的实例（也就是可以返回别的类的实例），__init__ 则是改变实例的状态返回 None。__new__ 可以决定是否后续调用 __init__，Object 的话默认调用 __del__：在类被自动删除之前执行的方法，类似于析构函数 继承 多类继承涉及重复调用（钻石/菱形继承）问题，也就是共同基类会被多次调用，python 通过查找顺序（MRO）来解决这个问题。具体来说，在类中通过 super() 来调用 MRO 的下一个类的方法，确保每一个类只初始化一次，如：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # 多类继承，这些类初始化首行均有 super().__init__()，共同基类为 base_parser class merge_parser(landmarks_parser, face_parser): def __init__(self): super().__init__() # 初始化调用顺序为： # start merge_parser # start landmarks_parser # start face_parser # start base_parser # end base_parser # end face_parser # end landmarks_parser # end merge_parser parser = merge_parser() import inspect inspect.getmro(merge_parser) # 获取 MRO 初始化顺序 # \u0026gt; (merge_parser, landmarks_parser, face_parser, base_parser) 不过多类继承还是存在无法很好传入参数的问题，而且也不好维护，所以实际推荐避开这种方法\nlambda 表达式 创建匿名函数。用表达式构建函数，只能访问自有参数，比 def 快速便捷\n1 2 sum = lambda x1, x2: x1 + x2 sum(2,3) 迭代器与生成器 iterator： 创建：a = iter(list) 遍历：next(a)，无法 index? 自定义：在类中实现 __iter__(self) 初始化和 __next__(self) 取值方法则可以构建迭代器 generator： yield 对象变动（Object Mutation） 可变对象（如列表）赋值只是增加别名，都会更改同一个空间。函数内部修改外部传进来的可变对象元素也一样是直接改内存。但当在函数内重新赋值时（如 in_dict = dict(xxx)），传进函数内部的对象就不再是别名，而是新的对象\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 \u0026gt;\u0026gt;\u0026gt; a = [] \u0026gt;\u0026gt;\u0026gt; b = [1,2] \u0026gt;\u0026gt;\u0026gt; a.append(b) \u0026gt;\u0026gt;\u0026gt; a.append(b) \u0026gt;\u0026gt;\u0026gt; a.append(b) \u0026gt;\u0026gt;\u0026gt; a [[1, 2], [1, 2], [1, 2]] \u0026gt;\u0026gt;\u0026gt; b[1] = 3 \u0026gt;\u0026gt;\u0026gt; b [1, 3] \u0026gt;\u0026gt;\u0026gt; a [[1, 3], [1, 3], [1, 3]] # 想要 a b 独立，应当 a.append(list(b)) 可变对象作为入参时，函数默认参数只会运算一次，不会每次调用都重新运算：\n1 2 3 4 5 6 7 8 9 10 11 12 def add_to(num, target=[]): target.append(num) return target add_to(1) # Output: [1] add_to(2) # Output: [1, 2] add_to(3) # Output: [1, 2, 3] 如果想要每次运行时重置，则需要将 target = None，然后在函数内创建列表\n有时候在函数内部需要缓存当前状态时利用这个特性可以很方便（相当于在函数内部维护了一个静态数组）\n不可变对象（Immutable Object） 也叫可哈希（hashable）对象，其在生命周期内不会改变（有 __hash__() 方法），能够通过 __eq__() 或 __cmp__() 进行比较。\n可哈希对象可以作为字典的键或集合的元素（在内部使用哈希值表示）。\n内置对象是可哈希的，自定义类的实例对象默认也是可哈希的（哈希值是 id()）\n列表维护 如果创建新列表，可以采用列表解析的方式。比普通循环方法要快，可嵌套，字典也能这样用\n1 2 3 4 a = [ i**2 for i in range(10) if i \u0026gt; 3 ] # 展开嵌套列表 a = [b for i in a for b in i] 如果只是要遍历列表，应该采用迭代器，可以避免过多的内容加载\n1 2 3 # python 3.x filtered_values = (i**2 for i in range(10) if i \u0026gt; 3) filtered_values = filter(lambda i: i \u0026gt; 3, range(10)) # 没有输出乘法处理 通用的讲，这三种内置函数结合 lambda 函数可以达到一定程度的函数式编程：\nmap(func, iterable)：对 iterable 逐个应用 func，输出等大小 list filter(func, iterable)：对 iterable 逐个应用 func，根据 func 结果是 true 或 false 选择是否保留该元素，返回小于等于原 list 大小的新 list functools.reduce(func, sequence[, initial])：func 输入上次结果和这次元素，计算累计结果传递给下个输入，遍历数组，可以实现累乘累加，返回单个值 协程 类似中断，它允许在函数执行中跳转执行其他函数，子程序是协程的一种特例\nyield：生成器就是协程的一种实现 async/await：python3.5 加入的关键字，通过 async def func(): 来表示定义一个异步函数，内部可以设置挂起条件。在异步函数中通过 res = await func() 来表示挂起当前异步函数，等待 func() 返回结果 参考：\nhttps://www.cnblogs.com/dhcn/p/9032461.html https://www.cnblogs.com/xinghun85/p/9937741.html 闭包 Python 函数内部允许嵌套函数，如果内部的函数使用了外部函数提供的临时变量（内部函数的非局部变量），然后返回外部函数返回值为内部函数，则形成了闭包，该外部变量被绑定成了值或在存储空间中，也就相当于内部函数被实例化创建了\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 def f(x): def g(y): return x + y return g # Return a closure. def h(x): return lambda y: x + y # Return a closure. # Assigning specific closures to variables. a = f(1) b = h(1) # Using the closures stored in variables. assert a(5) == 6 assert b(5) == 6 # Using closures without binding them to variables first. assert f(1)(5) == 6 # f(1) is the closure. assert h(1)(5) == 6 # h(1) is the closure. 需要注意，Python 的闭包是 late binding 的，也就是内部函数在被调用时才会去寻找内部变量的值，这意味着不能够一串 for 循环创建函数列表\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 cc = [lambda x : i * x for i in range(5)] cc[0](2) # return: 8 cc[1](2) # return: 8 def create_multipliers(): multipliers = [] for i in range(5): def multiplier(x): return i * x multipliers.append(multiplier) return multipliers cc = create_multipliers() cc[0](2) # return: 8 cc[1](2) # return: 8 参考：\nhttps://docs.python-guide.org/writing/gotchas/#late-binding-closures\n装饰器 如果有一些函数功能逻辑之外的事情要做，或者说需要控制函数的行为，比如日志、授权之类的，可以使用装饰器来实现以保持代码的简洁清晰\n大致意思就是使用一个新函数去包装该函数\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 from functools import wraps def cal_time(func): @wraps(func) # 让 wrap_func 使用 func 的函数信息的装饰器 def wrap_func(*args, **kwargs): start = time() res = func(*args, **kwargs) end = time() print(\u0026#39;time:\u0026#39;, end-start) return res return wrap_func @cal_time def process(a, b): return a + b process(1,2) # Output: time: 7.152557373046875e-07 # Output: 3 # @ 后面跟以 func 为参数的可调用对象，所以可以再加一层使装饰器带参数 def benchmark(cuda=False): def cal_time(func): @wraps(func) # 让 wrap_func 使用 func 的函数信息的装饰器 def wrap_func(*args, **kwargs): if cuda: torch.cuda.synchronize() start = time() res = func(*args, **kwargs) if cuda: torch.cuda.synchronize() end = time() print(\u0026#39;time:\u0026#39;, end-start) return res return wrap_func return cal_time @benchmark(True) def process(a, b): return a + b # 一些内建的装饰器 class X(): # 类方法，可以 X.func1() 直接调用，允许通过 cls 来获取调用类的一些属性或方法，避免硬编码 @classmethod def func1(cls, arg1, arg2): pass # 静态方法，可以 X.func1() 直接调用，无法访问类的属性状态 @staticmethod def func2(arg1, arg2): pass 类和对象也可以用作装饰器，会更加灵活\n1 2 3 4 5 6 7 8 9 10 11 12 13 class luminance_inference(): def __init__(self, f): self.f = f def __call__(self, x, *args): x, recov_uv = rgb_to_luminance(x) y = self.f(x, *args) y = luminance_to_rgb(y, recov_uv) return y @luminance_inference def forward(x): return x 一些有用的装饰器：\n@functools.lru_cache(maxsize=128)：函数缓存装饰器，会缓存 maxsize 个结果，函数的固定参数和关键字会用字典缓存下来，如果相同就会返回上次的结果。不过这个装饰器不能用在 numpy array 上，如需用于复杂返回值上可以考虑 [[Python#Python#Joblib|Joblib]] 问题：\n在类方法里使用装饰器有很多要注意事项，暂且没搞懂，总之很难用 参考：\nhttps://www.runoob.com/w3cnote/python-func-decorators.html 注册 https://applenob.github.io/python/register/\n垃圾回收 Python 主要采用引用计数，辅以 标记-清理 和分代回收策略。\n对象引用计数 +1 的情况：\n对象被创建 对象被引用 b = a 对象被传入函数中 对象被存进容器中 对象引用计数 -1 的情况：\n别名被显式销毁 del a 引用其他对象 b = a =\u0026gt; b = c，a 引用 -1 离开作用域 容器被销毁，或元素被删除 问题：\n耗资源（引用一次改一次）、无法释放循环引用（a, b = b, a）\n解决：\n使用 标记-清理 将容器内元素分为 reachable 和 unreachable（对象间构建有向图，从 root object （全局变量、调用栈、寄存器）出发，能抵达的就是 reachable，否则 unreachable），清理 unreachable。清理的时机使用分代回收，即根据存活时间给对象分代，按特定策略触发垃圾回收\n详细参考：\nhttps://blog.csdn.net/xiongchengluo1129/article/details/80462651 https://segmentfault.com/a/1190000016078708 内省（Introspection）和反射（Reflection） 大致意思都是程序在 运行时 去查看或修改一个 不确定是否存在 的对象，前者只能检查，后者允许修改。Python 中通过 getattr(obj, str)、setattr(obj, str, val) 等来实现\nThe ability to inspect the code in the system and see object types is not reflection, but rather Type Introspection. Reflection is then the ability to make modifications at runtime by making use of introspection. The distinction is necessary here as some languages support introspection, but do not support reflection. One such example is C++.\nsetattr(object, name, value)：功能上类似于 object.name=value，给对象设置属性值，该属性可以不存在（新建），不过这里 name 为字符串\ngetattr(object, name[, default])：同上，属于按属性名索引\n多线程与多进程 多线程 threading，多进程 multiprocessing。由于 Python 一般使用 CPython 作为解释器，而 CPython 有 GIL 的存在（一时间只能一核在跑），所以无法在 Python 层面实施真正的多线程。如果同步和通信问题不是很严重，使用多进程会不错，每个进程会有一个 GIL，但不会互相影响\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 # 多进程下 numpy 种子可能相同，随机函数或种子设置应采用 python 的 random import random # 使用 pickle 来序列化入参，改用 multiprocess 可以有更强的序列化能力 import multiprocessing def func(x): pass def func_wrapper(z): func(z[0], z[1], ...) if __name__ == \u0026#39;__main__\u0026#39;: # 单进程处理 for x in list_v: func(x) # 简易使用，8 进程处理 pool = multiprocessing.Pool(8) # 将 list_v 换为 [(arg1, arg2), (arg1, arg2), ...] 的二维数组， # 再套一层 wrapper 解析入参，再传入到 func 中即可处理多个参数 # （类似 Tensorflow 的数据预处理） pool.map(func, list_v) pool.close() pool.join() 需要注意的是，不能多个进程对同一全局变量做修改（在各个进程中似乎会拷贝一份全局变量进去，然后每个进程都有一个仅对自己进程可见的全局变量，最终修改的结果也只影响拷贝的变量，不会改变主进程的全局变量）\n如需共享资源使用 Multiprocessing.Queue、Multiprocessing.Array、Multiprocessing.Manager\n子进程的构建方式包含 fork 和 spawn，还有一种 forkserver 似乎是两者的混合管理：\nfork：子进程使用父进程共享的内存，只会对修改部分写时复制，速度快，不过如果长时间运行修改的小资源多会导致内存涨得厉害。Unix 类以及 MacOS（python3.8 以下）默认用这种方式，Windows 不支持。另外，该方式会复制 numpy 随机种子，会出现多进程使用同一个种子的情况 spawn：子进程从头构建，复制父进程数据，相当于再跑一个程序，速度慢。Windows 以及 MacOS（python3.8 及以上）默认这种方式 补充资料： https://bnikolic.co.uk/blog/python/parallelism/2019/11/13/python-forkserver-preload.html 在一些情况下（比如 VSCode 上调试）似乎只能用 spawn\njupyter 上多进程可见 [[Python#jupyter]]\nmultiprocessing.Pool 的使用说明：\napply(f, *args)：使用进程池的一个进程跑一个函数，阻塞。非阻塞版本 apply_async map(f, *args)：使用进程池并行跑一个函数，阻塞。非阻塞版本 map_async imap(f, *args)：类似 apply https://discuss.python.org/t/differences-between-pool-map-pool-apply-and-pool-apply-async/6575 is 和 is not 类似 ==、!=，不过 is 和 is not 比较地址\n1 2 3 4 5 6 7 a=\u0026#34;hello\u0026#34; b=\u0026#34;hello\u0026#34; print(a is b, a == b) # 输出为 True, True a=[\u0026#34;hello\u0026#34;] b=[\u0026#34;hello\u0026#34;] print(a is b, a == b) # 输出为 False, True 需要注意的是，== 可能会被重载（如 numpy 里会返回 keep_dims 的 bool 数组），这样就不能用 a == None 对 a 判别（重载后对元素进行逐个判别），而 a is None 可以，所以判别对象是否存在一般都用 is None\nNone 和 False if 语句里可以判断 空数据、0、False 和 None 都为假，使用 is 判断的话它们都是不同的\n1 2 3 4 5 6 7 8 9 10 11 12 a = [] b = 0 c = False d = None if a or b or c or d: # not enter pass 0 == False # True 1 == True # True 2 == True # False with\u0026hellip;as\u0026hellip; 可以用于打开文件（上下文管理器），自动分配和释放资源，保证操作结束后无论是否异常都能自动关闭打开的文件。可以通过类里实现 __enter__ 和 __exit__ 方法来自制上下文管理器，也可以使用 contextlib 来装饰生成器做成简易的上下文管理器\n1 2 with open(\u0026#39;xxx.txt\u0026#39;, \u0026#39;w+\u0026#39;) as f: f.write(str) https://pythonguidecn.readthedocs.io/zh/latest/writing/structure.html#id12\n类型注解 类型注解是 Python 3.5 后引入的功能，主要用于标注对象的类型，使用方法类似 a: int = 3，常用于标注函数方法的输入输出变量类型，以方便编辑器检查。具体类型标准可以在 typing 库中找到\n1 2 3 4 5 6 7 8 9 10 11 12 from typing import List # 复合类型注解 a: List[int] = [0, 3] # Python 3.9+ 可以不需要 typing 库 a: list[int] = [0, 3] # 也可以在内置构建函数时添加注解 a = dict[int, list[int]]() # 更高级的还可以使用泛型来处理复杂的多类型支持和多参数同步类型问题 通过 .pyi stub file 可以给函数指定类型，从而让 Pycharm 等环境进行类型检查而非运行时动态检查。名字要相同，且在 PATH 中 https://www.cnblogs.com/chester-cs/p/14000921.html\nhttps://zhuanlan.zhihu.com/p/419955374\n异常处理 遵循 Fail-Fast 原则，不要静默处理，不要用默认处理纠正，除非明确知道怎么处理。对于意想外情况的输入，不要捕获，Let it Crash，让上层解决\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # 直接抛出异常 if xxx: raise RuntimeError(\u0026#39;asdasdsa\u0026#39;) # 异常捕获以及打印信息 import traceback try: Image.open(\u0026#39;xxx.jpg\u0026#39;) except IOError: xxx except Exception as e: print(repr(e)) # 打印错误（无转义） print(traceback.print_exc()) # 打印完整 traceback else:\t# 无误的话 xxx finally: xxx\t# 无论正确错误一定会执行的部分 # 断言，用 python -O 执行可忽略 assert(xxx == \u0026#39;asd\u0026#39;) https://dzone.com/articles/fail-fast-principle-in-software-development\n性能分析 使用 dis 模块可以分析字节码\n1 2 import dis dis.dis(func) 正则表达式 使用 re 模块\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 import re # 因为 re 的输入 pattern 需要符合正则表达式规则，而正则表达式和 ASCII 的转义字符部分冲突 # 所以需要在 pattern string 前加上 r 表示 raw string 不处理转义字符得到原始字符，用于 # 正则匹配 res = re.match(r\u0026#39;model\\.stage\\d\\.\u0026#39;, key) # 与上面同结果的另一种写法，稍高效些 prog = re.compile(pattern) result = prog.match(key) # 将 key 中符合 prog pattern 的字符串替换成 xx prog.sub(\u0026#39;xx\u0026#39;, key) # 转义字符 re.escape(\u0026#39; vv/n \u0026#39;) # \u0026#39;\\ vv/n\\ \u0026#39; # 返回捕获组，0 表示匹配的字符，1 表示第一个捕获组，2 表示第二个捕获组，如此类推 res.group(0) 日志系统 Python 使用 logging 库作为标准日志记录工具，其格式化等处理通过 handler 实现，常用的 handler 有：\nStreamHandler：将输出流发送到标准输出或文件对象等支持 write() 和 flush() 的对象上 NullHandler：不会处理或输出，可以用来静默相关日志而不用大量改代码 开发库时，应当将 logger 设置成 NullHandler logging.getLogger(__name__).addHandler(NullHandler())，似乎可以将库的日志输出交由用户设置决定\nhttps://realpython.com/python-logging-source-code/#library-vs-application-logging-what-is-nullhandler __future__ 模块 可以将新版本 python 的特性导入到当前版本中使用\n关于 Unicode、str 和 bytes 标准 ASCII 码 0 ~ 127 表示字符，最高位留作奇偶校验（使字节的位累加和为奇/偶数），后 128 为扩展码。为表示更多语言，用两个字节来编码，如 GB2312。\nUnicode 则是统一多种语言的字符集，每个字符对应四字节内的 ID（或称 code point），其编码方式（出于传输使用和压缩常用字符等目的）有 UTF-8（以 8 位为单位的可变长编码）等。\n在 python2.x 中，a = 'asd' 和 a = u'asd' 分别是 str 和 unicode 类型，而在 python3.x 中两者都是 str 对象类型（似乎都是 unicode 的 code point 形式，都视为 unicode string）。而 UTF-8 表示则是字节形式，比如 UTF-8 字节 0x30 代表 unicode 字符 0 。\n若要保存则需要编码为 bytes 类型（相当于字节组成的数组），通过 a.encode('utf-8') 实现，此时它的长度将会是如实的字节数。\n对于 a.encode(\u0026quot;ascii\u0026quot;, \u0026quot;xmlcharrefreplace\u0026quot;) 还有第二个参数，可以提供无法被相应规则编码的情况下的对策。\n注：字符串标志 r'xxx' 表明用原始字符不转义（比如换行用”反斜杠+n“表示），f'xxx{a:.2f}' 表明 f-string，字符串内可以用大括号来引入外部变量，b'xxx' 表明是 bytes 对象\n更多资料：https://pycoders-weekly-chinese.readthedocs.io/en/latest/issue5/unipain.html\n转换方法汇总如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 str_a = \u0026#39;asd\u0026#39; int_a = -4 arr_a = np.array([4.2, 3.1]) byt_a = b\u0026#39;asd\u0026#39; # 转 bytes b = str_a.encode(\u0026#39;utf-8\u0026#39;) b = int_a.to_bytes(4, sys.byteorder, signed=True) b = arr_a.to_bytes() # 同 bytes(arr_a) b = bytearray(byt_a) # 可写的字节数组，用于字节运算 # bytes 转 str_a = b.decode(\u0026#39;utf-8\u0026#39;) int_a = struct.unpack(\u0026#39;1i\u0026#39;, b) arr_a = struct.unpack(\u0026#39;2d\u0026#39;, b) # 十六进制字符转 bytes bytes.fromhex(\u0026#39;E1B3\u0026#39;) 字符串对齐方法：\n1 2 print(\u0026#34;%-70s \\t %s\u0026#34; % (s1, s2)) # 将 s1 字符串左对齐填充到 70 字符 print(f\u0026#34;{s1:\u0026lt;15}\u0026#34;) # 左对齐占 15 字符，最好将不能对齐的变量打印放到最后面 字符转义：\n1 2 re.escape(\u0026#39;x.out.\u0026#39;) # \u0026gt; \u0026#39;x\\\\.out\\\\.\u0026#39; 编码错误问题：\n首先观察结构：如果是类似 \\123\\432\\142 的格式，说明是 Unicode 码，这里是 8 进制 Unicode，而且可能是一个汉字（占三个 unicode）；如果是乱码字符说明是被解码过了，需要先用 .encode('utf-8') 之类的编码回 bytes 然后转换成 16 进制的编码 bytes，也就是手动编码。如果是 \\123\\432\\142 的话可以 [bytes.fromhex(hex(int(code, 8))[2:]) for code in octal_code.split('\\\\')[1:]] 把逐个 Unicode 码转成十六进制 bytes 最后把 bytes 合起来，并且用正确的编码格式解码回正确字符， b''.join(hex_bytes).decode('utf-8') I/O 主要分两种：StringIO、BytesIO，前者操作 str，后者操作二进制\n1 2 3 4 5 6 7 8 import struct # 结构化读二进制文件 label_file = open(\u0026#34;Samples.bin\u0026#34;, \u0026#39;rb\u0026#39;) # 样本格式为215个浮点+1个整型，一共69个样本 label_raw = struct.unpack(69*\u0026#39;215fi\u0026#39;, label_file.read(69*216*4)) # 格式化读取相应字节的 buffer label_array = np.array(label_raw) label_array = np.reshape(label_array, [69,-1]) label_file.close() 想要通过文件 I/O 写数据进硬盘需要经过几个步骤，其中存在几种缓冲区：\n内部缓冲区。程序调用 I/O 接口 file.write() 写入时会先写到运行时/库/语言所创建的内部缓冲区中以加快程序写的速度，当缓冲区满了才会经过系统调用写入到实际文件（操作系统缓冲区），也可以通过调用 file.flush() 来显式进行这个过程，使用 with ... as 打开的文件会自动做 flush 操作系统缓冲区。当完成了内部缓冲区到操作系统缓冲区的复制后，其他进程打开文件就能访问到更新的内容，但是还没有真正写到硬盘中，可以通过调用 os.fsync(fd) 来强制写到硬盘中，不过现代硬盘内部通常也有多级缓存区，而且主动的 fsync 可能会被禁用之类的，所以实际会更复杂些 不过好像在 ext4 下必须要经过 fsync 才能同步文件内容，更多参考 https://stackoverflow.com/questions/7127075/what-exactly-is-file-flush-doing 版本控制 1 2 3 4 from packaging.version import Version, parse v1 = parse(\u0026#34;1.0a5\u0026#34;) # 支持更丰富的非标准格式 v2 = Version(\u0026#34;1.0\u0026#34;) https://packaging.pypa.io/en/latest/version.html 环境变量 PYTHONHOME：表示 python 标准库的前缀路径，默认系统库为 /usr/ Change the location of the standard Python libraries. By default, the libraries are searched in prefix/lib/pythonversion and exec_prefix/lib/pythonversion, where prefix and exec_prefix are installation-dependent directories, both defaulting to /usr/local.\nWhen PYTHONHOME is set to a single directory, its value replaces both prefix and exec_prefix. To specify different values for these, set PYTHONHOME to prefix:exec_prefix.\nPYTHONPATH：python 模块索引路径，可以在 sys.path 中查看和修改，添加索引时常用的环境变量 如果要在脚本中临时插入环境变量，可选择 os.environ['PYTHONPATH']='path'，如果不好使那就 sys.path.insert(0, 'path')\n计算 对性能要求较高的计算情况下可以考虑别的库，比如 Taichi 之类的\n内置函数 vars(object)：函数返回对象 object 的属性和属性值字典 dir(object)：返回对象 object 的所有属性列表 常用生成函数 np.linspace(start, end, num_points)：生成 [start, end] 的 num_points 大小的数组 传递 a = b：对于 numpy 数组，此操作只改变指针，a 和 b 共用一块内存 a = b[0]：注意，当 b 的元素是列表对象元素时（如 pytorch Tensor），内存不会拷贝，修改 a 会同时修改 b[0] a = b.copy()：内存拷贝 a = b[:,0:4]：切片会拷贝内存，不会修改原数组 矩阵计算 a - b：这类运算最好同维度，不然可能会出现意外的结果 a @ b，矩阵乘法，同 np.dot(a,b)，为 Python3.5 新添加的特性 np.einsum('ij,ik-\u0026gt;jk',a,b)：爱因斯坦求和，高级的矩阵乘法表示，\u0026ldquo;输入标记中重复字母表示沿这些轴的值将相乘\u0026rdquo;，“输出标记中省略字母表示沿该轴的值将被求和”。如例，输入相同的维度 i（a 和 b 的行维度各元素）做 element-wise 乘法，输出表示取不同的维度 j k（列维度）做遍历计算排列，而省略的维度表示对上述乘法结果求和，若 i 为 3，[1,2,3]x[4,5,6] -\u0026gt; [4+10+18] np.einsum('ij,ik-\u0026gt;ijk',a,b)：输出三维度，同上沿行相乘，若 i 为 3，[1,2,3]x[4,5,6] -\u0026gt; [[4],[10],[18]] 在 Numpy 下会加速，但在其他一些场合（PyTorch、TensorFlow）之类的可能会减速，所以在非通用支持的矩阵计算场合要谨慎使用 数组累乘积：np.prod(a) 返回三角斜边：np.hypot(x1, x2)，x1 x2 是两条斜角边，可以是多维的常数 矩阵求逆：np.linalg.inv(x)，注意，x 需要是 well-conditioned 的，也就是 np.linalg.cond(x) 需要是较小的数，不然的得来的值可能会有相当大的偏差，这时候需要用伪逆来替代 广播机制：不同维度的矩阵进行运算时的机制，也可以用显式方法完成，是 GPU 运算加速（向量化）的头等目标 shape(2,3)-shape(2,1)：会自动复制最后一维，变成 shape(2,3)-shape(2,3) shape(2,1)-shape(1,2)：会做自动切分遍历，得出 shape(2,2) 批量索引 a[c,:]：bool 值数组 c = np.array([False, True, ...]), c.shape=(12,) 可以作为 index 筛选其他数组，此处可筛选出 a 的第一维数据 np.where(c)：返回 bool 值数组中 True 的索引，常用如 np.where((a\u0026gt;4)\u0026amp;(a\u0026lt;5))，当 c 是二维数组时返回 (row_idxs, col_idxs) 的元组 np.isin(e1, e2)：检测 e1 中是否有含有 e2 中元素的值，若是则为 True，否则 False。输出 e1 大小的 array。 a\u0026gt;0.5：返回等大小 bool 值矩阵，若 a[a\u0026gt;0.5]，则筛选出相应值并组成一维向量，若 a*(a\u0026gt;0.5)，则可置 a 小等于 0.5 的值为 0 a[a==255] = 0：批量替换 255 值 img[vec1, vec2] = 255：可以向量化，vec1、vec2 为一维数组 a[:,[2,0]]：取 a 数组第二维度的 2 和 0 通道 切片 冒号:数组切片，不省略维度，如： 1 2 3 4 5 6 7 8 9 a = np.arange(10) b = a[2:7:2] # 从索引 2 开始到索引 7 停止，间隔为 2（numpy 间隔可为负数，torch 不可） # [start:stop:step] # step 可省略，遵循左闭右开 [start, stop) # start 和 stop 也可省略， # 表示之后或之前的全部索引 [start:] [:stop] a = np.arange(12).reshape(4,3) a = a[:,[2,1,0]] # 实现 BGR 转 RGB，同 a[:,::-1]，但 torch 只支持该操作 省略号...：数组切片，可省略多个维度，或者保持维度，如： 1 2 3 4 a = np.random.rand(2,8,5,5) print (a[...,1]) # axis 3 的第 2 个 [2,8,5] print (a[1,...]) # axis 0 的第 2 个 [8,5,5] print (a[...,1:]) # axis 3 的第 2 个及以后 [2,8,5,4] 冒号切片其实是通过传 slice(x1,x2,x3) 进 __getitem__ 来实现的，可以检查入参类型来在自定义类中实现切片\n特殊索引 np.searchsorted(array, value, side='right')：寻找值的插入区间 删除 list.remove(xxx) np.delete(a, [1], axis=1)：删除 index=1 列 逻辑 判空千万不要用 xxx is None，空数组也会返回 False，最好都用 len()、.size 等方法 交集并集等操作采用 [[Python#集合]] 批量逻辑判断可以用 all([True, False, True]) == False 或 any([True, False, True]) == True 来进行，类似全部与操作（空列表也返回 True） 比较 可以采用 DeepDiff，能够列出嵌套结构化数据之间的差异，支持常用结构和 numpy 数组作为元素，还有忽略大小写、忽略类型变化、指定浮点精度、正则表达式等功能 https://miguendes.me/the-best-way-to-compare-two-dictionaries-in-python 扩增 [1, 2, 3]*2 = [1, 2, 3, 1, 2, 3] 或 [2,3,4] + [23] = [2,3,4,23]。注意，乘法扩增不会在内存上进行拷贝，扩增的部分是共享的 元组扩增也一样，不过只有一个元素要写成 (x, ) 形式 字典扩增则直接赋值就好 a[new_key] = new_value np.insert(a, [2], 1, axis=-1)：在最后一维的 index=2 前面插入全 1 堆叠/展开 PyTorch 中有 tile/unfold，见此 [[PyTorch#^e3b3d9]]，numpy 不清楚 数组扩维度 np.insert(a, 0, np.ones(9), axis=1)：在 a 数组的第 0 列__之前__插入一列 1，a.shape=[9, 3]，相当于 matlab 的 [ones(9,1) a] np.append(a, 1)：将 a 展成一维，并在末尾插入 1，多维情况不明 np.reshape(a, [a.shape[0], a.shape[1], -1])：类似 torch.unsqueeze(-1) a[np.newaxis, :]：等同于 pytorch 的 a.unsqueeze(0)，tensorflow 中也是类似方法 np.repeat(a, 3, axis=-1)：将 a 的最后一维扩展三倍，也就是 [4,4,1] 扩展成 [4,4,3] 重排 np.transpose(a, (1,2,0))，另外有些库只交换两个维度，有些库会称作 permute() 矩阵内部设立子矩阵，同时不改变排列结构： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # 需要设置分组情况，假设内部子矩阵为 4x4 大小一组，即 block_shape = (4, 4) # 由 a:(1380, 778) -\u0026gt; b:(345, 194, 4, 4) # strides 为每个维度跨越到下一维度的字节大小，注意要加上舍弃的末尾余数 # 如 b[0] -\u0026gt; b[1] 需要经历 194x4x4+(778-194*4)*4，也就是 778*4 strides = 4 * np.array([a.shape[1] * block_shape[0], block_shape[1], a.shape[1], 1]) # 输出 shape a_block_shape = (a.shape[0] // block_shape[0], a.shape[1] // block_shape[1], block_shape[0], block_shape[1]) # 这里转成 4 字节的浮点矩阵再切分 b = np.lib.stride_tricks.as_strided(a.astype(np.float32), a_block_shape, strides) 合并 numpy：np.concatenate( (a,b), axis=1 )，将 a 和 b 沿 1 维合并，有些库称作 concat() 或 cat()，如果要 concat 到新维度的话就是 np.stack() list：clist = alist + blist 不要求容器类型：itertools.chain(a, b)，创建迭代器 插值 np.interp(np.linspace(1, 10, 50), np.linspace(1, 10, 10), a)：用于一维线性插值，a 是大小为 10 的一维数组 np.append()：展成一维，然后在末尾添加值 scipy.interpolate.interpn(grid_cord, grid_val, inter_p)：多维插值，其中： grid_cord：网格在各个维度的坐标的元组，比如 (np.array([1,2,3,4]), np.array([1,2]), np.array([1,2,3])) 可以代表从 x 维度上看各个网格的坐标刻度为 1,2,3,4（可以不等间隔，网格最终满足长方形状就行），另外两个维度同理 grid_val：网格内的值，如果按上述坐标的话，其 shape 应为 [4, 2, 3] inter_p：要插值的点，shape 为 [N, 3] https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.interpn.html 取模 a % b：a 或 b 可为负数，具体结果： 1 2 3 4 # 仅 python -17 % 10 = 3 17 % -10 = -3 -17 % -10 = -7 取整 a // b：注意，如果 a 是浮点数，b 是整型数，取整结果会是浮点数，需要转换类型 截取 np.clip(a, min, max)：有些库会叫 clamp 随机 random.shuffle(a)：对 a 进行随机打乱，inplace 操作 random.sample(range(0, len(x)), 10)：随机采样，用于产生不重复随机数 np.random.rand(d1,d2,d3)：注意，numpy 的随机在多进程中是各个进程得到相同值的随机，多进程下应当采用 r = np.random.RandomState(random.randint(0, (1 \u0026lt;\u0026lt; 32) - 1)) 来获得随机类，r.rand(d1,d2,d3) 最大最小索引 np.argmax(a) 和 np.argmin(a) np.argsort()：可以返回最小的多个元素索引 排序 Python 使用 Timsort 算法来实现稳定排序，空间复杂度 O(n)，时间复杂度最好、平均、最差分别为 O(n), O(nlogn), O(nlogn)\nsort()：应用于 list，是 inplace 操作 sorted()：可用于所有可迭代对象上，返回新 list 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 # 先将排列依据设为 key 建立字典 a = {int(filestr):filestr for filestr in files} # 然后排序，依次取出值 b = [a[k] for k in sorted(a)] # 也可以按值来排序 b = {k: v for k, v in sorted(a.items(), key=lambda item: item[1])} # 可以通过 key 传入排序的依据提取函数 def sort_func(x): dot_stuff = x.split(\u0026#39;.\u0026#39;) slash_stuff = dot_stuff[-2].split(\u0026#39;_\u0026#39;) res = 6 - len(slash_stuff[-1]) for i in range(res): slash_stuff[-1] = \u0026#39;0\u0026#39; + slash_stuff[-1] dot_stuff[-2] = \u0026#39;_\u0026#39;.join(slash_stuff) return \u0026#39;.\u0026#39;.join(dot_stuff) sorted(a, key=sort_func) # 将 xxx_1 xxx_10 xxx_2 排成 xxx_1 xxx_2 xxx_10 sorted(a, key=lambda x: int(get_base_name(x).split(\u0026#39;_\u0026#39;)[-1]) # 如果是字典排序，则可以通过 items() 分开键值 sorted(a.items(), key=lambda kv: kv[1]) 字符串匹配 比较两个字符串的相似性：Levenshtein.distance(str1, str2)，用于和单词表匹配去除 OCR 识别噪声 误差 np.allclose(actual, desire, rtol, atol)：比较 actual 和 desire 的各数值差异，系数用于缩放，比较的容忍度为：atol + rtol * abs(desire) 统计 np.var()：计算方差 角度中心点画旋转方向框 1 2 3 4 5 cv2.rectangle(img_square, (int(tlx), int(tly)), (int(brx), int(bry)), (0, 0, 255), 2) rot_mat = cv2.getRotationMatrix2D((int(x_center), int(y_center)), angle, 1.0) img_rsquare = cv2.warpAffine(img_square, rot_mat, (img_square.shape[1], img_square.shape[0])) img[np.sum(img_rsquare, 2)\u0026gt;0, :] = [0,0,0] img = img + img_rsquare 转换 numpy 转 opencv b = np.uint8(a)：要带 dtype 的 numpy 数组才能被 opencv 处理，此方法为向下取整（溢出不提示，按内存数据取值） b = cv2.copyTo(a, np.uint8(np.ones(a.shape)))： python 的 opencv 数组内存拷贝 numpy 转 list a = np.ones((12,12)) b = a.tolist() c = np.array(b) 输入转字符串：a = str('xxxxx')：a = 'xxxxx' 输入转列表：b = list('xxx')：['x', 'x', 'x'] 字符串转列表 b = list(a) b = a.split('\\n')：以换行符分割字符串 a，缺省时以空白符来分割（包括空格换行符等） 列表转字符串 a = ''.join(b)：在列表 b 中插入字符串''来构成字符串 a 类型转换 批量：list(map(int, a)) 类型检查 type(xxx)：返回对象类型 isinstance(xxx, TYPE)：检查对象类型，其类和基类被视为同一类，TYPE 可为 tuple 以检查是否符合多种类型之一 常用数据结构（类） 文件 判断是文件还是目录：os.path.isfile() os.path.isdir()，符号链接也一视同仁 判断是否存在：os.path.exists(file_path) 获取目录下文件： glob.glob(\u0026quot;xxx/*.png\u0026quot;)，结果带路径前缀，如 xxx/1.png，不包括隐藏文件 os.listdir(\u0026quot;xxx/\u0026quot;)，结果不带路径前缀，如 1.txt，包括隐藏文件 获取路径： 当前工作目录：os.getcwd()，相当于 Shell 里的 pwd，注意，即使是在其他路径下的模块内部使用这个返回的也是一样的，都是 python xxx.py 时的目录 当前文件目录：os.path.dirname(os.path.abspath(__file__))，同样的，获取某个模块路径时就以 module.__file__ 替代 获取文件的绝对路径：os.path.abspath(path) 枚举变量 for (index, char) in enumerate(list(['a','b','c'])):\n枚举返回的是计数值和 list 内容的二元组\n对于可迭代对象，可用 next(X) 来得到下一个二元组\nzip(a, b)：数组/元组合并为数对，a、b 为数组或元组，len 相同，返回枚举变量，将 a、b 中的值以元组对形式逐个输出 列表 判断为空：len(a) 或者直接 if a: 检测是否有重合： 1 2 3 from collections import Counter count_list = dict(Counter(a+b)) print ({key:value for key,value in count_list.items()if value \u0026gt; 1}) 判断是否包含：if 'xxx' in set(a): 列表字符串连接似乎是自动的？：['431' 'xzcvzv'] == ['431xzcvzv'] 元组 与列表不同，元组不能直接修改元素，属于不可变类型\n自定义命名元组结构，类似轻量化的类：\n1 2 3 4 from collections import namedtuple gaussian = namedtuple(\u0026#39;Gaussian\u0026#39;, [\u0026#39;mean\u0026#39;, \u0026#39;var\u0026#39;]) # 打印显示格式 gaussian.__repr__ = lambda s: \u0026#39;𝒩(μ={:.3f}, 𝜎²={:.3f})\u0026#39;.format(s[0], s[1]) 但是不能直接 gaussian.mean=1 修改属性，需要通过 gaussian._replace(mean=1) 来修改\n还可以将字典的下标索引转为这种指针索引：\n1 2 TrialOptions = namedtuple(\u0026#39;TrialOptions\u0026#39;, dict_a.keys()) opt = TrialOptions(*dict_a.values()) 集合 集合似乎是一种元素唯一的列表，不考虑顺序，可以用于各种取交集并集的场合\n1 2 set([1,2,3,3,4,4,4,4]) # output: {1,2,3,4} 字典 Python 的字典是哈希表的实现，字典的键需要是不可变类型（常量、元组或对象都行），而不能是可变类型（列表、数组等）。具体来说，其哈希函数的处理如下：\n整数和布尔值采用原值 元组使用各元素哈希值的相加 对象使用内存地址 冲突处理采用开放寻址的伪随机法。\n使用记录：\n判断字典是否含有该键值：dict.__contains__('name') 当存在时返回默认值可用：dict.get('name', None)，第二项为 default 项，不能用键值方式输入 default=xxx，因为这种内置函数多数是 C 层 API 只能用位置来确定参数 排序：sorted(a) 得到 key 的排序后的 list，见 [[Python#Python#计算#排序|排序]] 比较：字典 key 理论上是不讲顺序的，a == dict(sorted(a.items(), key=lambda x:x[0])) 返回键值：dict.values()，返回的是 dict_values 类型数组，使用 list(dict.values()) 转换为列表才可以索引使用 更新字典里对应的值：x2.update(x1)，有则替换，无则插入 删除：del dict['name'] 字典反转：{value : key for (key, value) in a.items()} 特殊字典 如果需要在遍历时按插入顺序遍历，则可以使用 collections.OrderedDict 代替（python 3.6 以上都是默认记录了插入顺序的） 如果需要让字典自动初始化值为 list，不会 append 空键值出错，可以使用 defaultdict(list) 来代替 对象 vars(xxx)：返回对象的属性和对应值的字典，若无入参则同 locals() 返回当前位置的属性和对应值。 时间 一般情况下使用平台默认的计时，返回秒 1 2 3 4 from timeit import default_timer as timer start = timer() end = timer() print(end-start, \u0026#39; s\u0026#39;) 存在多个设备（GPU 也算）同时运作时测时间，需要一些同步的方法和平台无关的时间测量，如 torch.cuda.synchronize() 获取文件修改时间： 1 2 3 timestamp = os.path.getmtime(f) struct_time = time.localtime(timestamp) time.strftime(\u0026#39;%Y-%m-%d %H:%M:%S\u0026#39;, struct_time) 设计模式 创建型模式 用于生成特定行为的对象的模式。主要是工厂，其他还包括单例模式\n结构型模式 用于特定使用场景的代码结构模式\n最著名的是 ZCA（Zope Component Architecture），主要提供用于定义、注册、查找组件的工具，包括两类组件：Adapters、Utilities\n适配器（Adapter）：封装一个类或对象 A，用于工作在一个类或对象 B 的上下文中\n行为型模式 对过程进行结构化的模式\n示例 并行 I/O 前面提到使用 multiprocessing 可以完成并行处理 [[Python#多线程与多进程]]，其中一个常见的应用场景就是并行 I/O。比如利用多核 CPU 进行快速文件移动：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 import os import shutil from multiprocessing import Pool from typing import List def copy_file(src_file: str, dst_directory: str): if not os.path.exists(dst_directory): os.makedirs(dst_directory, exist_ok=True) try: # copy2 会尝试保留元信息，follow_symlinks=False 会复制符号链接而不是复制原本 shutil.copy2(src_file, dst_directory, follow_symlinks=False) except: # 如果文件已存在则跳过 pass def copy_files_in_folder(src_folder: str, dst_folder: str, exclude: List[str]): # followlinks=True 会遍历符号链接文件夹 for root, dirs, files in os.walk(src_folder, followlinks=True): if any(ex in root for ex in exclude): continue relative_path = os.path.relpath(root, src_folder) dst_directory = os.path.join(dst_folder, relative_path) # 单独处理符号链接文件夹 if os.path.islink(root): dst_directory = \u0026#39;/\u0026#39;.join(dst_directory.split(\u0026#39;/\u0026#39;)[:-1]) copy_file(root, dst_directory) else: # 异步处理，不阻塞 _ = [pool.apply_async(copy_file, args=(os.path.join(root, f), dst_directory)) for f in files if f not in exclude] pool = Pool(N) exclude = [\u0026#39;.ipynb_checkpoints\u0026#39;, \u0026#39;__pycache__\u0026#39;] copy_files_in_folder(src_folder, dst_folder, exclude) pool.close() pool.join() 再比如，快速清理 ipy 缓存\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 exclude = set([\u0026#39;.ipynb_checkpoints\u0026#39;, \u0026#39;__pycache__\u0026#39;]) def worker(path): for root, dirs, files in os.walk(path, followlinks=True): if any(ex in root for ex in exclude): return root return None src_folder = opt.src dirs = [d for d in os.listdir(src_folder) if os.path.isdir(os.path.join(src_folder, d))] with mp.Pool(15) as pool: results = pool.map(worker, dirs) result = [r for r in results if r] print(result) [shutil.move(path, \u0026#39;/somwhere/ipy_check/\u0026#39;+str(i)+\u0026#39;_\u0026#39;+\u0026#39;_\u0026#39;.join(path.split(\u0026#39;/\u0026#39;)[1:-1])) for i, path in enumerate(result)] 关键在于使用进程池来异步处理每个操作，需要确保每个进程的操作是独立的\n库 collections 包含很多有用的数据结构\n1 2 3 4 5 6 ## Counter: 频率统计相关类，字典的子类 a = \u0026#39;eabcdabcdabcaba\u0026#39; # 类似字典 c = collections.Counter(a) # {\u0026#39;e\u0026#39;: 1, \u0026#39;a\u0026#39;: 5, \u0026#39;b\u0026#39;: 4, \u0026#39;c\u0026#39;: 3, \u0026#39;d\u0026#39;: 2} # 提供前 n 个高频出现的元素 c.most_common(3) # [(\u0026#39;a\u0026#39;, 5), (\u0026#39;b\u0026#39;, 4), (\u0026#39;c\u0026#39;, 3)] Numpy 全局精度设置：np.set_printoptions(precision=8) 缩略打印输出限制：np.set_printoptions(threshold=10000, linewidth=200) 不以科学计数法显示输出：np.set_printoptions(suppress=True)，pytorch 也是类似 torch.set_printoptions(sci_mode=False) 保存成 .mat 在 matlab 分析：scipy.io.savemat('PR_curve.mat', {'precision': precisionC, 'recall': recallC, 'threshold': thresholdC}) 不同类型数组间进行 \u0026ldquo;=\u0026rdquo; 赋值时，如 a[:,:,0] = b，会将 b 的值直接内存索引给 a，也就是说没有类型转换，可能会溢出。 Matplotlib 在 notebook 中使用时务必加上 %matplotlib notebook 来启用内联交互，不然会出现很多问题\n单通道图片显示需要设置为 plt.show(img, cmap='gray', vmin=0, vmax=255)，否则会显示一片黑 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 # 基本 xy import matplotlib.pyplot as plt # 设置中文字体 plt.rcParams[\u0026#39;font.family\u0026#39;] = [\u0026#39;WenQuanYi Micro Hei\u0026#39;] plt.rcParams[\u0026#39;font.size\u0026#39;] = 15 plt.plot(x, y, \u0026#39;-\u0026#39;) plt.show() # 多曲线添加标记 plt.legend([\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;]) # 打开网格 plt.grid(\u0026#39;on\u0026#39;) # 设置 y 轴间隔 px = plt.yticks(np.arange(0, np.max(a)+0.01, 0.01)) # 直方图 plt.hist(data, bins=40, facecolor=\u0026#34;blue\u0026#34;, edgecolor=\u0026#34;black\u0026#34;, alpha=0.7) # 饼状图 for i in range(4): data_count.append(num) labels.append(lab) colors = sns.color_palette(\u0026#39;bright\u0026#39;) plt.pie(data_count, labels=labels, colors=colors, autopct=\u0026#39;%0.0f%%\u0026#39;) # 图像 plt.imshow(array) # 去掉空白间隔 plt.subplots_adjust(hspace=0, wspace=0) # plt.gca().set_axis_off() # plt.subplots_adjust(top = 1, bottom = 0, right = 1, left = 0, # hspace = 0, wspace = 0) # plt.margins(0,0) # plt.gca().xaxis.set_major_locator(plt.NullLocator()) # plt.gca().yaxis.set_major_locator(plt.NullLocator()) # 不显示 x 坐标轴刻度 plt.xticks([]) # 例 plt.figure(figsize=(8, 8)) plt.subplot(1, 2, 1) plt.imshow(img) plt.subplot(1, 2, 2) # 自动缩放值域 plt.imshow(tf.keras.preprocessing.image.array_to_img(out[0])) plt.axis(\u0026#39;off\u0026#39;)\t# 保存图像，不 pad 周围 plt.savefig(\u0026#34;res.png\u0026#34;, bbox_inches=\u0026#39;tight\u0026#39;, pad_inches=0.0) plt.show() # 交互显示 import matplotlib.style as mplstyle mplstyle.use(\u0026#39;fast\u0026#39;) fig = plt.figure(figsize=[9,9], tight_layout={\u0026#39;pad\u0026#39;:0.0}) ax = fig.add_subplot(1,1,1) im = ax.imshow(np.random.randn(10,10)) ax.axis(\u0026#39;off\u0026#39;) def disp(img): im.set_data(img) fig.canvas.draw_idle() # 三维曲面 img = np.zeros((256,256)) xx = np.arange(0,256,1) yy = np.arange(0,256,1) X, Y = np.meshgrid(xx, yy) fig = plt.figure() #定义新的三维坐标轴 ax3 = plt.axes(projection=\u0026#39;3d\u0026#39;) ax3.plot_surface(X,Y,img,cmap=\u0026#39;rainbow\u0026#39;) # ax3.contour(X,Y,img,offset=1, cmap=\u0026#39;rainbow\u0026#39;) #等高线图，要设置offset，为Z的最小值 plt.show() # 带类别颜色的散点图 plt.scatter(data[:,0], data[:,1], c=colors) # color 为 0-1, RGB [n,3] 或 [1,3] 问题：\nUserWarning: Glyph 25151 (\\N{CJK UNIFIED IDEOGRAPH-623F}) missing from current font：中文不能正确显示，通过找中文字体 fc-list :lang=zh family，然后设置 plt.rcParams['font.family'] = ['中文字体', 'sans-serif'] 替换就行 Font family ['Noto Sans CJK'] not found： 接上文，则 sudo apt install msttcorefonts -qq + rm ~/.cache/matplotlib -rf，然后使用如 plt.rcParams['font.family'] = ['WenQuanYi Micro Hei'] seaborn matplotlib 上层库\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 # 设置主题以及中文字体、大小 sns.set_theme(style=\u0026#34;whitegrid\u0026#34;, font_scale=1.4, rc={\u0026#34;font.family\u0026#34;: \u0026#34;WenQuanYi Micro Hei\u0026#34;}) # 取两个属性进行画图，x 为离散属性，y 为连续属性 # hue 为离散属性（用于成对显示） # 改为 boxenplot 可以画出增强版箱线图 ax = sns.boxplot(x=\u0026#34;method\u0026#34;, y=\u0026#34;lpips\u0026#34;, hue=\u0026#34;param\u0026#34;, data=scores_df) # 半透明化 plt.figure(figsize=(10,8)) ax = sns.boxplot(x=\u0026#34;tuned\u0026#34;, y=\u0026#34;lpips\u0026#34;, data=scores_df, saturation=.9) for patch in ax.artists: r, g, b, a = patch.get_facecolor() patch.set_facecolor((r, g, b, .5)) # 调整 y 轴 px = plt.yticks(np.arange(0, scores_df[\u0026#39;lpips\u0026#39;].max()+0.01, 0.01)) # 画散点图 ax = sns.stripplot(x=\u0026#34;epoch\u0026#34;, y=\u0026#34;lpips\u0026#34;, data=scores_df, jitter=0.1) # 画类型图，通过 height 和 aspect 来调整画布大小 # attr_data 为拥有 attr value bk 三个维度属性的 DataFrame g = sns.catplot( data=attr_data, kind=\u0026#34;bar\u0026#34;, x=\u0026#34;attr\u0026#34;, y=\u0026#34;value\u0026#34;, hue=\u0026#34;bk\u0026#34;, palette=\u0026#34;dark\u0026#34;, alpha=.6, height=10, aspect=20/10 ) # 去掉左边轴线 g.despine(left=True) g.set_axis_labels(\u0026#34;\u0026#34;, \u0026#34;value\u0026#34;) g.legend.set_title(\u0026#34;\u0026#34;) 问题：\nmissing from current font：问题同 matplotlib 需要 sns.set_theme(rc={\u0026quot;font.family\u0026quot;: \u0026quot;WenQuanYi Micro Hei\u0026quot;}) jupyter notebook 中多进程示范（但似乎不太可靠）：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from multiprocessing.pool import ThreadPool as Pool sites = [ \u0026#39;https://github.com/veit/jupyter-tutorial/\u0026#39;, \u0026#39;https://jupyter-tutorial.readthedocs.io/en/latest/\u0026#39;, \u0026#39;https://github.com/veit/pyviz-tutorial/\u0026#39;, \u0026#39;https://pyviz-tutorial.readthedocs.io/de/latest/\u0026#39;, \u0026#39;https://cusy.io/en\u0026#39;, ] def func(url): return url pool = Pool(4) for result in pool.imap_unordered(func, sites): print(result) 使用 virtualenv：\n安装 pip3 install ipykernel\npython3 -m ipykernel install --user --name=xxx 删除 kernel：\njupyter kernelspec remove 查看 kernel：\njupyter kernelspec list 使用 parser 默认配置：\nparser.parse_args(args=[]) Undo \u0026amp; Redo：\nEsc 下 z \u0026amp; Shift+z 跨文件复制：command mode 下复制 cell，然后另一个文件连按两次 ctrl+v 修改字体：在 ~/.jupyter/custom/custom.css 内添加 1 2 3 4 .CodeMirror { font-family: Dejavu Sans Mono; font-size: 14px !important; } 增加 notebook 宽度： 1 2 from IPython.core.display import display, HTML display(HTML(\u0026#34;\u0026lt;style\u0026gt;.container { width:100% !important; }\u0026lt;/style\u0026gt;\u0026#34;)) 自动重新加载模块（不重启就能调试模块，除改变类结构（如继承）外）： 1 2 %load_ext autoreload %autoreload 2 不输出 Output： %%capture 更多指令见： https://ipython.readthedocs.io/en/stable/interactive/magics.html 问题： Tab 失效，而 Shift + Tab 有效：pip install jedi==0.17.2 自动补全很慢：Jedi 的问题：%config IPCompleter.use_jedi = False 禁用 Jedi https://github.com/ipython/ipython/issues/10493 pip install -e . 安装的库 jupyter 用不了：安装到了 /usr/lib/python3.9/site-packages/ 中导致，移到 /usr/local/lib/python3.9/dist-packages/ 下就行 LD_LIBRARY_PATH 和终端不同？ 见 [[Python#坑]] 13. logging 1 2 3 4 5 6 7 8 9 10 import logging # 输出到文件 # logging.basicConfig(filename=\u0026#39;logger.log\u0026#39;, level=logging.INFO) # 输出到终端 logger = logging.getLogger(\u0026#39;fasion_mnist_AutoML\u0026#39;) logger.setLevel(logging.INFO) logger.info(\u0026#39;\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\u0026#39;.format( test_loss, correct, len(test_loader.dataset), accuracy)) argparse 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 parser = argparse.ArgumentParser(description=\u0026#39;GAN Research\u0026#39;) parser.add_argument( \u0026#34;--config\u0026#34;, default=\u0026#34;\u0026#34;, metavar=\u0026#34;FILE\u0026#34;, help=\u0026#34;path to config file\u0026#34;, type=str, ) parser.add_argument(\u0026#39;--log_step\u0026#39;, default=10, type=int, help=\u0026#39;Print logs every log_step\u0026#39;) parser.add_argument(\u0026#39;--use_tensorboard\u0026#39;, default=True, type=str2bool) parser.add_argument( \u0026#34;opts\u0026#34;, # positional argument help=\u0026#34;Modify config options using the command-line\u0026#34;, default=None, nargs=argparse.REMAINDER, ) args = parser.parse_args() 最后一个参数，接受未明确指明的任意类型值，用空格分隔 如果是在 notebook 中使用，则可以传入空参数使用默认值，parser.parse_args(args=[])，如果要解析命令行字符串则可以 parser.parse_known_args(args_string.split(' ')) 使用 nargs 参数可以调整选项所需的参数个数： parser.add_argument(, nargs=\u0026quot;?\u0026quot;,)：? 表明指定选项时可以附带一个参数，也可以只指定选项不带参数（使用 const 值），或者都不指定（使用 default 值） parser.add_argument(, nargs=\u0026quot;+\u0026quot;,)：+ 和 * 类似，都类似于正则表达式的匹配规则，在指定选项需要多个参数时可以用，指定其他选项时终止参数匹配。其解析后类型变为 list，元素是指定的 type，所以不需要 type=list 使用 argparse.ArgumentParser(prog='PROG') 允许只输入选项的前缀（需唯一） 对于可能有额外未识别的选项和参数输入时，使用 parser.parse_known_args() 来代替 parser.parse_args() 可以将它们存到额外的一个 list 里 args, unkown_list = parser.parse_known_args() 对于想输入 bool 类型的情况，需要手写 string 转 bool：\n1 2 3 4 5 6 7 8 9 10 # bool 类型输入需要做字符转换处理 def str2bool(v): if v.lower() in (\u0026#39;yes\u0026#39;, \u0026#39;true\u0026#39;, \u0026#39;t\u0026#39;, \u0026#39;y\u0026#39;, \u0026#39;1\u0026#39;): return True elif v.lower() in (\u0026#39;no\u0026#39;, \u0026#39;false\u0026#39;, \u0026#39;f\u0026#39;, \u0026#39;n\u0026#39;, \u0026#39;0\u0026#39;): return False else: raise argparse.ArgumentTypeError(\u0026#39;Unsupported value encountered.\u0026#39;) parser.add_argument(\u0026#39;--use_dropout\u0026#39;, type=str2bool, nargs=\u0026#39;?\u0026#39;, const=True, default=False, help=\u0026#39;use dropout for the generator\u0026#39;) 这样就能使用 --use_dropout、--use_dropout True 来表示 Ture，--use_dropout False 或者干脆不指明来表示 False\nipywidgets 若无法显示则加 %matplotlib inline，还不行则 pip install --upgrade --force-reinstall ipywidgets、pip install --upgrade --force-reinstall widgetsnbextension、jupyter nbextension enable --py widgetsnbextension --sys-prefix\ntqdm 进度条插件\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from tqdm import tqdm for idx, image in tqdm(enumerate(input_iter), total=num_files): pass # 训练显示 for epoch in range(1, 2): with tqdm(range(100), unit=\u0026#34;batch\u0026#34;) as tepoch: for iters in tepoch: tepoch.set_description(f\u0026#34;Epoch {epoch}\u0026#34;) optimizer_D.zero_grad() y = model(x) loss = loss_func(y, x) loss.backward() optimizer_D.step() tepoch.set_postfix(loss=loss.item()) Requests 1 2 3 4 # 请求图片 res = requests.get(url) with open(\u0026#39;./xx.png\u0026#39;, \u0026#39;wb\u0026#39;) as f: f.write(res.content) JSON 以字典形式存储，如 {\u0026quot;id\u0026quot;: \u0026quot;str\u0026quot;}，如果要保存数组（只支持一维）的话则如 {\u0026quot;id\u0026quot;: [num1, num2, num3]}\njson.dumps(a, indent=2)：将字典转成 JSON 字符串，缩进两格 json.dump(a, f, indent=2)：在 I/O 上下文中将字典输出 如果含有中文字体，则需要取消默认的 ASCII 编码，指定 ensure_ascii=False Joblib 针对大数据 numpy 数组进行优化的工具库，可以本地缓存函数的结果，从而避免重复计算\n1 2 3 4 5 6 7 8 9 10 11 # 在定义函数的文件里 from joblib import Memory location = \u0026#39;./cache\u0026#39; # 如果要缓存于内存则 /dev/shm memory = Memory(location, verbose=0) # 直接包装 process_func = memory.cache(process_func) # 装饰器 @memory.cache def process_func 注意：\nmemory.cache 最好只包装一个函数，级联包装多个会显著降低速度，可能是某些策略或内存问题导致 OpenCV 更多见 [[OpenCV]]\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 import cv2 # Read cv2.imread(\u0026#39;xxx.png\u0026#39;, flags=cv2.IMREAD_UNCHANGED) # 读单通道时要加上 flags # Show cv2.imshow(\u0026#39;WINDOW_NAME\u0026#39;, IMG) # Video flow key_value = (cv2.waitKey(1) \u0026amp; 0xFF) if key_value == ord(\u0026#39;q\u0026#39;): break # Image flow cv2.waitKey(0) # Release cv2.destroyAllWindows() # Display Text cv2.putText(disp_img, str, (30, 30), cv2.FONT_HERSHEY_COMPLEX_SMALL, 0.8, (255, 0, 0)) PIL 读取 jpg 图像速度比 OpenCV 稍慢，但是如果只读图片高宽等属性会快非常多\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 from PIL import Image # 打开图像文件（返回 File 类耗时间短），读图失败需要处理异常 imgfile = Image.open(label_path) # 转换为 Image 或 numpy 数组（耗时长） a = imgfile.convert(\u0026#34;RGB\u0026#34;) # 若 imgfile 本身为 RGB，则 # np.array(a, dtype=np.uint8) 和 np.array(imgfile, dtype=np.uint8) 一样， # 但前一种多余操作会增加更多耗时 # PIL -\u0026gt; numpy，可通过调色板的模式得到原始图像数据 np.array(a, dtype=np.uint8) # numpy -\u0026gt; PIL Image.fromarray(a.astype(\u0026#39;uint8\u0026#39;)) # 从文件流读取图片 lab = open(\u0026#34;xxx.png\u0026#34;, \u0026#39;rb\u0026#39;).read() image = Image.open(io.BytesIO(lab)) # 保存 image.save(\u0026#34;xx.png\u0026#34;) # 显示 image.show() mat73 用于读取 matlab 格式 .mat 数据的库\n1 2 import mat73 data_dict = mat73.loadmat(\u0026#39;xxx.mat\u0026#39;) Pandas Series：可用 python list, dict 或者 numpy array 等数据结构来初始化 DataFrame：Series列表，一行为一条数据（包含多个属性） .dtypes：看各列数据的类型 .memory_usage(deep=True)：看列数据内存总占用 .info()：包含上述两种的总览 .infer_objects()：对数据类型不统一的样本进行切片后可以采用该方法修正为合适数据类型 .json_normalize()：对嵌套字典进行展开填充成 DataFrame，较为复杂 如果需要节省内存可以改变一下列元素的类型，比如采用 category 替代字符串类型：\n1 2 3 4 5 6 # Category 替代字符串 cat_cols = dataset.select_dtypes(include=object).columns dataset[cat_cols] = dataset[cat_cols].fillna(\u0026#39;none\u0026#39;).astype(\u0026#39;category\u0026#39;) # 更小的精度类型替代 dataset[\u0026#34;id\u0026#34;] = pd.to_numeric(dataset[\u0026#34;id\u0026#34;], downcast=\u0026#34;unsigned\u0026#34;) 一些常用的处理：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 from pandas import Series s1 = Series([1,\u0026#39;asd\u0026#39;,231]) # 类似 numpy object array s2 = Series({\u0026#39;a\u0026#39;:3, \u0026#39;b\u0026#39;:\u0026#39;asfwq\u0026#39;, \u0026#39;c\u0026#39;:48}) # 自动取索引 s3 = Series({\u0026#39;a\u0026#39;:3, \u0026#39;b\u0026#39;:\u0026#39;asfwq\u0026#39;, \u0026#39;c\u0026#39;:48}, index=[\u0026#39;id\u0026#39;,\u0026#39;b\u0026#39;,\u0026#39;c\u0026#39;]) # 自定义索引，\u0026#39;id\u0026#39; 索引没有值，故为 NAN s1.notnull() # 判断非 nan 值 s1.isnull() # 判断 nan 值 # 从 csv 中创建 data = pd.read_csv(file_path) # 从 python 或 numpy 数组创建，columns 描述属性信息 data = pd.DataFrame(array, columns=[\u0026#39;name\u0026#39;, \u0026#39;score\u0026#39;]) # 从 Series 中创建 data = pd.DataFrame([s1,s2,s3]) data.iloc[0] # 整数下标索引 data.describe() # 得到基本描述，第一行 count 为非缺省值个数 # 25% (50%, 75%同): # 大于所有数据的25%的数，小于75%的数 的数值 data.head() # 取头几个数据 data.xxx==5 # 取出属性 xxx 值为 5 的所有数据 data[\u0026#39;xxx\u0026#39;]==5 # 同上 data[data[\u0026#39;xxx\u0026#39;].isin(xxx_list)] # 通过元素是否在列表中来筛选 data.sort_values(by=[\u0026#39;xxx\u0026#39;,\u0026#39;yyy\u0026#39;]) # 根据 xxx 和 yyy 的值排序，有先后 data[(data[\u0026#39;xxx\u0026#39;]-data[\u0026#39;yyy\u0026#39;]).abs() \u0026lt; 2] # 可以做一些基本运算 ## 增加一维 size 用于统计去除重复的行 data.groupby(data.columns.tolist(),as_index=False).size() ## concat MxJ 和 MxK，得到 Mx(J+K) pd.concat([df1, df2], axis=1) ## 按给定的列表排序 https://www.cnblogs.com/lemonbit/p/7004505.html data[\u0026#39;xxx\u0026#39;] = all_ratings[\u0026#39;xxx\u0026#39;].astype(\u0026#39;category\u0026#39;) data[\u0026#39;xxx\u0026#39;].cat.reorder_categories(sort_name, inplace=True) # 似乎 deprecated 了 data.sort_values(\u0026#39;xxx\u0026#39;, inplace=True) ## 列排序 data = data.reindex(sorted(data.columns), axis=1) ## 切片 data[12:33] # row index 会从 12 开始 data[12:33].reset_index(drop=True) # row index 会从 0 开始 ## 转换 data[\u0026#39;params\u0026#39;].apply(pd.Series) # 逐个处理 params 列，转为新的 DataFrame 处理 excel 表格，首先要安装 openpyxl，然后 xxx = pd.read_excel('xxx.xlsx')\n问题：\nArrowInvalid: (\u0026quot;Could not convert '2.71' with type str: tried to convert to int64\u0026quot;, 'Conversion failed for column params with type object')： to_feather() 时出现的错误，原因是数据里有 list 元素类型不统一，也就是说混合数据的 object 是不允许的，每个元素都必须是同类型。迷惑设计，我都用 object 了，还会在意这个？ https://github.com/wesm/feather/issues/349 https://github.com/pandas-dev/pandas/issues/21228 ^3f4b47 采用 .astype({'b': str}) 将混合类型的 b 列改为 str 类型似乎可以解决？ sklearn 常用的机器学习库\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler, PolynomialFeatures # 特征每个维度缩放到 0 均值 1 方差，预处理基本用法 fit_transform scaler = StandardScaler() norm_x = scaler.fit_transform(x) norm_x = (x - scaler.mean_) / scaler.scale_ # 使用参数归一化 x = norm_x * scaler.scale_ + scaler.mean_ # 使用参数复原 # 多项式特征生成，[1, a, b, c, a^2, ab, ac, b^2, bc, c^2] PolynomialFeatures() # 可以合在一起组成一个 pipeline pipe = Pipeline([(\u0026#39;poly\u0026#39;, PolynomialFeatures()), (\u0026#39;scaler\u0026#39;, StandardScaler())]) pipe.fit_transform(x) absl Google 的库，从 C++ 移到 Python，包含标志系统、日志系统等\nGradio 专供机器学习的 UI 库，含有一些基本的组件\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 import gradio as gr def classify_image(image): input_image, input_data = preprocess(image) input_name = session.get_inputs()[0].name output_name = session.get_outputs()[0].name score = session.run([output_name], {input_name: input_data})[0] return input_image, f\u0026#34;评分(1-5): {str(score[0].squeeze())}\u0026#34; def run(): imageBox = gr.inputs.Image().style(width=750, height=768) textBox = gr.outputs.Textbox() outputBox = gr.outputs.Image(type=\u0026#39;numpy\u0026#39;).style(width=224, height=224) gr.Interface( classify_image, imageBox, # set server name to local machine IP to share with another [outputBox, textBox]).launch(server_name=\u0026#39;x.x.x.x\u0026#39;, server_port=7860) 坑 cv2.error: /home/sad/opencv/modules/imgproc/src/imgwarp.cpp:3361: error: (-215) func != 0 in function resize debug 了一下似乎是 OpenCV 不支持 32 位及以上的 int 或 uint 型矩阵的 resize, 然后翻了下源码, 果然线性插值的 ResizeFunc tabel 不支持. 而 python caffe 返回的值是 int64 的, 所以拿到了空的 func, 报错 Python 数据结构的 = 赋值全为指针赋值 ( 即浅拷贝 ), 共享内存, 对其修改会影响原数据结构数值 TypeError: ellipse() takes at most 5 arguments (8 given)：center 和 axes 要是整数 Could not build wheels for bottleneck which use PEP 517 ：装这个 https://github.com/pybind/pybind11 fatal error: Python.h: No such file or directory：安装相应版本的开发包 sudo apt-get install pythonxx-dev ImportError: No module named pkg_resources，可能是 setuptools 的问题，pip install 一下可能会好 shutil.copy(src, dst) 可能会出现 dst 不存在的奇怪错误，若 src 文件和 dst 目录都存在则可能是相对地址问题，把地址改成 os.path.abspath() 的就行 OSError: [Errno 5] Input/output error：广州腾讯云服务器读写 cpfs 云盘出现的问题，可能是文件 copy 到同目录偶尔会有问题，copy 到子目录或许会好点？https://research.google.com/colaboratory/faq.html#drive-timeout TypeError: a bytes-like object is required, not 'str'： 读取 pickle 时出错，需要读入 byte 格式 open(xxx, 'rb') 如果进一步出现 UnicodeDecodeError: 'ascii' codec can't decode byte 0xad in position 0: ordinal not in range(128) 则需要修改 encode 格式 pickle.load(f, encoding='latin1') pip list 发现安装的库不在：可能是安装的库路径有误，详见 [[#库安装]] pip install 出现 No space left 说明默认的临时缓存空间不足，可以通过设置环境变量 export TMPDIR='/work/tmp_other' 来改变临时缓存目录 pip install 出现 ERROR: Can not execute setup.py since setuptools is not available in the build environment. 错误：需要更新 pip 和 setuptools ModuleNotFoundError: No module named \u0026quot;xxx\u0026quot;：可能是因为当前目录下有个 xxx 库，所以 import 了错误的库导致 jupyter-notebook 环境变量不同步：通过 jupyter kernelspec list 找到 kernel 配置目录（/usr/local/share/jupyter/kernels/python3/kernel.json），修改 kernel.json，添加一行 \u0026quot;env\u0026quot;: {\u0026quot;LD_LIBRARY_PATH\u0026quot;:\u0026quot;\u0026quot;} 来设置环境变量 ModuleNotFoundError: No module named '_ctypes'：经典错误，原因未明。如果装 libffi-dev 无法解决则需要重装 python super(type, obj): obj must be an instance or subtype of type：Jupyter Notebook 重载模块的神奇错误，重启 kernel 就行 Error: Failed to find a python interpreter in the .data section：py-spy 问题，似乎 py-spy record -o pro.svg -- python xxx.py 就没问题，原因不明 pip 安装用了很长时间，可以通过加 \u0026ndash;verbose 来看看在做什么 pip install xxx --verbose RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr：cuDNN 版本错误，删掉旧的 dpkg -P 以及可能的 pip uninstall nvidia-cudnn-cu11，要看 cudnn 用的什么方式安装 wandb: ERROR api_key not configured (no-tty). call wandb.login(key=[your_api_key])，wandb 库是联网的 dashboard，命令行输入 wandb offline 切换成本地版本免去 api_key ERROR: Could not find a version that satisfies the requirement tb-nightly：pip 库安装问题，可能是镜像源没有该库，更换镜像源或使用官方源 gradio 502 bad gateway 错误：本地部署需要设置为 0.0.0.0 或设置域名来监听网络请求，而不是 127.0.0.1 本地还回 notebook 极度卡顿，py-spy 显示卡在 autoreload 的 update_instances 中： ","date":"2024-03-09T00:00:00Z","permalink":"/p/python/","title":"Python"},{"content":"正则表达式，意思是用来表示符合某种规则的式子，是用于匹配符合特定模式的字符串，也就是描述一类有特定规则的字符串。比如要在一篇文章中找到关于“xx大学”相关信息，如果既想找到“广州大学”的部分，也想找到“北京大学”的部分，直接搜索“大学”又可能会出现“青年大学习”这种无关信息，这时候通过正则表达式 [北|广][京|州]大学 就能查找“广州大学”和“北京大学”，当然副产品还有“广京大学”和“北州大学”。\n正则表达式一般含有三种字符： 打印字符、非打印字符、特殊字符。\n其中，一般打印字符和非打印字符只匹配一个字符位，如匹配 广州什么大学 的正则表达式 广州\\S\\S大学 中每个字符都只匹配对应的一个字符位。而特殊字符可以改变匹配的范围，从而允许匹配更多的字符位，如 广州什么什么大学 和 广州大学 都能够使用 广州(.*)大学 来匹配，这是格式化匹配的关键。\n在不同系统环境中，采用的正则表达规范或许会有所不同（BREs, EREs, PREs），这里不做细究。\n打印/非打印字符 通俗来说，能看到的就是打印字符（包含空格在内），不能看到的用于控制（制表符、回车符等）或筛选某一类字符的就是非打印字符，参见 ASCII 码。\n\\n：换行符 \\r：回车符 \\s：空白符（大写 \\S 为非空白） \\w：字母、数字或下划线（大写 \\W 为非字） \\d：数字（大写 \\D 为非数字）\n\u0026hellip; 特殊字符（元字符） 用于限定范围、限定匹配数量、限定位置等的特殊用途字符。匹配特殊字符需要转义，比如要匹配 * 则输入 \\*。\n关于捕获元和非捕获元，个人猜测联想：用于匹配的子表达式就是捕获元，匹配到了就是捕获组，捕获组会保存起来供后续使用（如反向引用），而非捕获元匹配到的非捕获组不会保存起来，可能类似于宏定义和变量的区别。\n限定范围（匹配一个字符位或子表达式）:\n( )：括起来表示为子正则表达式，该子表达式可以用于其他特殊字符做进一步限定 [ ]：括起来表示限定范围在\u0026hellip;之内，比如 [0-9]、[abcd]、[^abcd] 分别表示 0 到 9、abcd 之一、非 abcd 的任意字符。字符间默认用 | 分隔，如果想用“与”可以加入 \u0026amp;\u0026amp;，“非”则表示为 ^，范围表示似乎是以 ASCII 值做范围，似乎又字母大小写无关？？？（注意，匹配 [ ] 时需要转义，而匹配 - 需要非常谨慎对待，因为会随语言不同有不同约定，最好就写在最后面 [.\u0026amp;#_-]，并经过测试 ） .：匹配除换行符 \\n 外的任意单字符 |：二选一，匹配符号左边的字符串或符号右边的字符串 限定匹配数量（对象为前面的字符或子表达式）：\n+：匹配一次或多次（贪婪） *：匹配零次或多次（贪婪） ?：匹配零次或一次 {n}：匹配 n 次 {n,}：至少匹配 n 次 {n,m}：匹配 [n, m] 次 限定位置（表示匹配字符串出现的位置）：\n^：限定在一行的开头，用在字符串开头 $：限定在一行的结尾，用在字符串结尾 \\b：限定在单词边界，即字和空格之间，作为词首或尾用 \\B：限定在非单词边界 非捕获元（接在表示子表达式的左括号后）：\n?:：表示该子表达式不会作为捕获组保存起来 ?=：正向预查，在 ?: 的基础上仅做预查，即匹配完后该子表达式不会被“消耗”，如 \u0026ldquo;ababa\u0026rdquo;，查 ab(?:a) 得到 \u0026ldquo;aba\u0026rdquo; 一个结果，而查 ab(?=a) 得到 \u0026ldquo;aba\u0026rdquo;、\u0026ldquo;aba\u0026rdquo; 两个结果 ?!：反向预查，就是正向预查的子表达式匹配取反，不匹配指定子表达式，同时是非捕获元 如：搜查开头不为 xxxx 的任意行 (^(?!xxxx).*\\n)+ 如：挑出两个 [Trial complete] 及其有用信息 units_4 ( \\|-units_4: .*\\n)(?:^(?!\\[Trial complete\\]).*\\n)+(.*\\n)\\[Trial complete\\] ?\u0026lt;!：往前的反向预查，代表 \u0026ldquo;What\u0026rsquo;s before this is not\u0026hellip;\u0026rdquo; 反向引用（匹配子表达式）：\n\\1 到 \\99：引用捕获组保存的结果，为动态引用，匹配的结果左到右顺序保存到 1-99 缓冲区内。如 \u0026ldquo;frisk is frisk\u0026rdquo; 可用 (\\w{3}) is \\1 匹配 （VSCode 中反向引用为 $1） 参考资料 基础：\nhttps://www.runoob.com/regexp/regexp-syntax.html\n非捕获元：\nhttps://segmentfault.com/a/1190000010514763\nhttps://www.jb51.net/article/79309.htm\n测试：\nhttps://regexr.com/\n","date":"2024-03-09T00:00:00Z","permalink":"/p/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/","title":"正则表达式"},{"content":"links: https://github.com/TencentARC/GFPGAN\n一种人脸高清化的方法，在 stable-diffusion 中被广泛作为插件使用。\nGFPGAN 人脸裁剪是 FFHQ，人脸和背景分开来放大，背景使用 RealESRGAN， 调查 v1.4 clean 版本 。\n架构上采用 resblock 做下采样得出 stylecode，然后 StyleGAN2 上采样恢复。在每层卷积之间对一半通道使用 SFT (Spatial Feature Transform)，别的好像没什么特别的 。\nSFT 是由 stylecode 上采样时每一级都会嵌入的变换，每层变换参数由 resblock 下采样层+上采样层得出。理论上是利用一组共享的语义概率特征（condition）给图像做分类来提供超分辨率细节，但实际实现似乎并不是共享，而是直接 encode-decode 多级输出，每层都会用一个新的上采样层解码再生成新的 condition。\n换而言之就是共享 encoder，然后分两个 decoder，一个组成类似 UNet 生成多分辨率特征输出，另一个是 stylegan2，把多分辨率特征按照对半通道 SFT 的方式进行生成。\n尝试 减少 condition 嵌入数量可以看到各种边缘越来越简单。\n类 UNet 能生成多级 RGB 输出，看起来每层各司其职（最后一层负责色彩，前面各层对应各层细节），看来是不用缩小图做监督的好处。\n也说明总体结构和色彩是由类 UNet 提供，而非 StyleGAN。\n直接采用高斯分布的随机数作为 latent 输入到 StyleGAN 中，也可以得到合理的结果，虽然细节差很多：\n说明 StyleGAN 在 GFPGAN 中作用非常小，主要负责超分里的细节。而类 UNet 的特征不太好整合，可能难以用于其他任务的特征提取。\n总结 效果很好，尝试了几个退化图片都非常好，缺点是真得有点假的皮肤，以及高度依赖类 UNet 跳跃链接导致不好做特征提取。\n","date":"2023-09-15T00:00:00Z","image":"/p/gfpgan%E5%B0%9D%E8%AF%95/GFPGAN1_hua19e7be9ff06dc72a46a8fa9c415a192_43292_120x120_fill_q75_h2_box_smart1_2.webp","permalink":"/p/gfpgan%E5%B0%9D%E8%AF%95/","title":"GFPGAN尝试"},{"content":"links: [[2023-07-03-Week]]\n要点 Rotary Positional Encoding，通过旋转来去掉绝对位置信息保持相对位置信息的位置编码技术\n自注意力的点积不保留绝对位置信息，而保留相对位置信息\n通过将 token embedding 表示为复数以实现旋转的位置编码\n传统 Positional Encoding 主要包括 Fixed 或 Learned 的 postitional encoding matrix\n比如 Fixed： $$ PE(pos, 2i) = sin(\\frac{pos}{10000^{\\frac{2i}{d}}}) $$ $$ PE(pos, 2i + 1) = cos(\\frac{pos}{10000^{\\frac{2i}{d}}})$$ 其中，pos 是 word 对应于 sequence 中的位置 idx，i 则是输出 embeddings 序列中的位置（因为输入一个 word 输出是一串 embedding），d 是总 embedding 维数，10000 可以看成基波频率\nRoPE 实现 可以看到传统的位置编码是绝对位置编码，实现较为简单，虽然可能推导出具有一定的相对位置编码能力，但还有所不足，为此需要做一些改进。具体原理演进可以参考资料 2-4。总之，RoPE 可以通过绝对位置编码的方式实现相对位置编码，兼得简单与相对编码的好处，而且还能够通过扩展基波频率来无痛加大 context 大小\n在代码中表示为：\n1 2 3 4 5 6 7 8 9 10 11 12 base = 10000 # 基波频率 max_position_embeddings = 2048 # 2k context dim = 512 # 输入维度，也就是 2i 范围 t = torch.arange(max_position_embeddings) inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim)) freqs = torch.einsum(\u0026#34;i,j-\u0026gt;ij\u0026#34;, t, inv_freq) # torch.outer # freqs_cis = torch.polar(torch.ones_like(freqs), freqs) emb = torch.cat((freqs, freqs), dim=-1) sin_cached = emb.sin() cos_cached = emb.cos() 应用时似乎是乘法的方式，看作是复平面上的旋转操作（和上面的代码不同的实现）：\n1 2 3 4 5 6 7 8 # 对 attention 的 q k 做旋转 xq_ = torch.view_as_complex(xq_) xk_ = torch.view_as_complex(xk_) # xq_out.shape = [batch_size, seq_len, dim] xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(2) xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(2) return xq_out.type_as(xq), xk_out.type_as(xk) 扩展 context 的方式包括频率分量上的扩展（次优）\n1 2 3 extend_scale = 4 # max_position_embeddings = 8192 scale = 1 / extend_scale t *= scale 以及基波频率的扩展（NTK-Aware Scaled）\n1 2 extend_scale = 8 # max_position_embeddings = 16384 base = base * extend_scale ** (dim / (dim-2)) 参考资料 https://zhuanlan.zhihu.com/p/642884818 https://kexue.fm/archives/8130 https://kexue.fm/archives/8265 https://kexue.fm/archives/9675 https://blog.eleuther.ai/rotary-embeddings/ ","date":"2023-07-07T00:00:00Z","permalink":"/p/%E6%97%8B%E8%BD%AC%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81rope/","title":"旋转位置编码(RoPE)"},{"content":"迭代更新双线性插值 做光流图像时容易出现很多噪点，将重采样后变化较大的部分 mask 掉，就可以得到相对可靠但是有缺失值（nan）的图像：\n突发奇想的一种解决方法就是采集每个以缺失点为中心的 32x32、16x16、8x8、4x4、2x2 的区域（还可以更高）的非 nan 均值，这些均值可以看作一个点的运动（由大范围模糊准确值向小范围精确噪声值运动），再直接取均值就能得到考虑了图像连续性的一个较好的估计。得出结果如下：\n这种方法好处在于比较好实现，可以考虑带掩码的全局滤波器组的实现方法，效果应该也不错。而且还能扩展，比如用其他先验方法来取区域和值来代表，以及点的运动也可以引入先验知识来估计。不知道一般是怎么处理的，反正直接平滑就够用。\n朴素的实现方法：\n用 pytorch 向量化一下可以到 3ms 这样\n","date":"2023-04-06T00:00:00Z","permalink":"/p/%E5%A6%82%E4%BD%95%E5%A4%84%E7%90%86%E5%90%ABnan%E5%80%BC%E7%9A%84%E5%85%89%E6%B5%81%E5%9B%BE%E5%83%8F/","title":"如何处理含nan值的光流图像"},{"content":"信息量为 I(x) = log(1/p(x))，其中 log 通常是 2 底，表示为 bit 单位。其中直觉在于，当信号 x 出现的概率 p(x) 越高，则含有的信息量应当越少，而 p(x) 越低，其含有的信息量应当越大。\n熵为信息量 H(x) = E(I(x)) 的期望，可表示成：\n$$H(x)=\\sum_ip(x_i)*log\\frac{1}{p(x_i)}$$\n关于一个样本集, 一般有两种分布, 预测的概率分布 q 和 真实的概率分布 p 。\n交叉熵 为了编码样本集, 需要的平均编码长度为信息量的期望 $$H(X)=\\sum_ip(x_i)*I(x_i)=\\sum_ip(x_i)*log\\frac{1}{p(x_i)}$$ 因为不清楚真实的概率分布, 所以用模型预测的概率分布 q 对比采样的真实分布 p 来降低泛化误差。 $$H(p,q)=\\sum_xp(x)*log\\frac{1}{q(x)}$$ $H(p,q)$即为_交叉熵_, $H(p,q) \u0026gt;= H(p)$。它表示了用 q 来表示 p 时所需的信息量均值（？）。此处 x 代表 p、q 的支撑集，也就是非零的部分。\n相对熵 用非真实分布 q 编码所需长度多出来的 bit 就是 相对熵, 也叫 KL散度 (Kullback–Leibler divergence) $$D(p||q)=H(p,q)-H(p)=\\sum_ip(i)*log\\frac{p(i)}{q(i)}$$\n真实分布不改变的情况下，即 H(p) = Constant，最小化相对熵相当于最小化交叉熵。因为交叉熵计算更简单，而且训练集代表的 p 分布一般不变，所以在机器学习中通常使用交叉熵来作为代价函数。需要注意的是，KL 散度是不可互换的，在一些情况下这种性质会带来学习的不平衡（GAN）。\nJS 散度 $$JS(p||q)=\\frac{1}{2}KL(p||\\frac{p+q}{2}) + \\frac{1}{2}KL(q||\\frac{p+q}{2})$$\n使得 p、q 可以互换。但是如果支撑集不重叠，则 JS 散度为常量。\n","date":"2023-02-24T00:00:00Z","permalink":"/p/%E7%86%B5/","title":"熵"},{"content":"links: [[2022-10-17-Week]], [[2022-10-24-Week]]\n将 sub-pixel convolution 用作图像生成的输出层时，依然有可能出现棋盘效应。如图是经过 “Checkerboard artifact free sub-pixel convolution” 改进训练过后的效果（左边为原图，右边为处理后的图片）。在不同的模型、不同的数据、不同的训练方法下该问题严重程度不一，但或多或少都会存在。\n解决方法之一是不采用 sub-pixel、transpose convolution 之类的操作，而改用线性或邻近插值来替代，但这样一来计算量过大，而且效果也不太好。\n解决方案 从信号系统的角度，将解决传统线性时域系统的 checkerboard 问题扩展到卷积上。可以得到一种简单的后处理方法来完全解决这个问题（ https://arxiv.org/pdf/1806.02658.pdf ）\n据称：\nIt is known that linear interpolators which consist of up-samplers and linear time-invariant systems cause checkerboard artifacts due to the periodic time-variant property.\n通过令输出满足一系列约束使得这种因为时变（图像上是空间变化）导致的 checkerboard 得以完全抑制。具体而言就是输出的特征经过上采样零阶保持器，也就是遍历特征把当前点之前（左上方邻域）的值累加起来求平均作为该点输出。效果如下：\n代码层面如下：\n1 2 3 4 5 6 7 8 9 10 ## 原文中的方法 A 或 B output = moduel(x) output = torch.pixel_shuffle(output, out_scale) # 先填充左上角用于模拟零阶保持器的滞后值输入，其实不用也可以，但是输出图像会往左上移一格 m_pad = nn.ReplicationPad2d((out_scale-1,0,out_scale-1,0)) # ZeroPad2d 也可以 m_blur = nn.AvgPool2d(out_scale, stride=1) output = m_blur(m_pad(output)) ## 原文中的方法 C ## 对于 nn.ConvTranspose2d() 的情况似乎还可以对 weight 进行卷积来做到，不太清楚什么意思 看起来效果不错，算法上相当于后处理增加了一次滤波，有不少优化空间。\n实际使用会有一点模糊，毕竟仅采用 sub-pixel 能达到的实际分辨率提升很有限。\n","date":"2022-10-24T00:00:00Z","image":"/p/%E5%A6%82%E4%BD%95%E6%B6%88%E9%99%A4%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90%E6%97%B6%E7%9A%84%E6%A3%8B%E7%9B%98%E6%95%88%E5%BA%94/shot_hua5252ff81ce67ad7cdcf3f8b04f4040e_26936_120x120_fill_box_smart1_3.png","permalink":"/p/%E5%A6%82%E4%BD%95%E6%B6%88%E9%99%A4%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90%E6%97%B6%E7%9A%84%E6%A3%8B%E7%9B%98%E6%95%88%E5%BA%94/","title":"如何消除图像生成时的棋盘效应"},{"content":"links: [[2022-06-27-Week]], [[103-问题汇总#^7d1ab9]]\n这个问题是在优化云端模型时出现的，设计了个更小的模型准备替换大的模型，结果发现模拟视频推理时耗时会出现异常波动，最终达不到优化预期。经过分析发现只在两次推理之间存在一点时间间隔时才会出现这个问题，很是诡异。因为是在云端容器上跑的，没法进行 Nsight 分析，而且我也不懂 GPU，只能默默记录下来。\n环境：\nUbuntu 16.04 TensorRT 7.2.1 CUDA 10.2 cuDNN 8.0 Python 3.6 PyTorch 1.7.1 torch2trt 0.1.0 Tesla T4 Intel(R) Xeon(R) Platinum 8255C CPU @ 2.50GHz（up to 10-core） 模型构成：\nlocalL（优化 nnt 版本） 和 localC 模型转换成 TensorRT 引擎 FP16 和 INT8 1024x576 和 768x448 推理分辨率 初始化预热 20 次 分解测试流程为几个步骤：\nOpenCV 从 nas 盘读取视频（I/O），转换成 YUV 字节流，以及其余模拟线上环境的配置和 log（I/O） 脚本前处理，涉及 to device 数据传输和少量 CUDA 计算 模型推理，里面是黑箱，涉及显卡内部计算和传输，不涉及 to host 数据传输 脚本后处理，涉及 to host 数据传输和少量 CUDA 计算 后续 log（I/O），以及调用 time.sleep 系统睡眠模拟流式处理保持帧率 存在三种场景，未能达成可预见的统一：\n测试模型推理，采用相同输入的连续推理的形式 测试脚本，不涉及线上环境模拟部分 测试线上环境 测试方法只记录脚本内部（步骤 2，3，4）耗时，采用 torch.cuda.synchronize()+timeit.default_timer() 的方式严格记录每一步耗时\n实验一 localL 1024x576 FP16 和 INT8 做 baseline\n测试线上环境，52 帧视频流：\nFP16 耗时 19ms（模型 17ms），GPU 占用 40% 左右，模型推理在一定范围内波动 前后处理耗时在个别时候有剧烈变化 INT8 耗时 15ms（模型 13.2ms），模型推理波动很小 前后处理耗时在个别时候有剧烈变化 测试脚本，循环 100 次：\nFP16 耗时 20.5ms（模型 18.8ms），GPU 占用 90% 以上，模型推理耗时依然波动 耗时比测试线上环境高，怀疑是 GPU 占用原因 添加 20ms 延时，耗时降低回 16.8ms 水平（比线上稍低一些），波动更明显更规律 前后处理耗时在个别时候有剧烈变化 INT8 模型推理耗时有分层情况出现，稳定后和线上环境耗时一致，延长预热后分层消失 测试模型推理，预热 20 次，循环 100 次： - FP16 耗时 20.4ms，GPU 跑满 - INT8 耗时 15.4ms，GPU 跑满\n疑问：\n为什么推理时间会有规律性波动 为什么两次推理之间的间隔会影响 GPU 推理速度（为什么连续推理会更慢） 为什么前后处理耗时会偶现高峰值 自答：\n上述几个问题都表明了一个现象，就是 TensorRT 推理时延和 GPU 利用率相关，这也是几种测试场景下出现不一致的主要原因 GPU 利用率是统计数据，真正的原因或许在于 GPU 内部管理程序有一定的耗时，两次推理之间必需要经过这一段时间，所以利用 GPU 的空闲时间可以掩盖掉这部分耗时，但当 GPU 相当繁忙时（常有的事）这部分耗时就会显露出来 还有的原因或许和 TensorRT 对连续推理的优化有关，这部分暂且不清楚 所以实际测试按照 GPU 利用率作为标准或许会更好点，但照目前观察，耗时基本可以代表 GPU 利用率 当然感觉也有可能是显卡服务器平台策略有修改了什么，不好说\u0026hellip; 实验二 localC 768x448 FP16 INT8，小模型推理\n测试线上环境，8000 帧视频流：\nINT8 模型推理时间呈现诡异的分层（50 iter 跃变，100 iter 稳定），稳定后 GPU 占用 33% 左右 可多次复现，分层的时间节点大致相同只会略有偏移 可以确定和延时长短有关，延时越长，分层整体越往左移 测试脚本，循环 800 次： - INT8 模型推理在添加 20ms 延时下呈现诡异分层，去掉延时则模型推理耗时正常（4.66ms） - FP16 模型推理在添加 20ms 延时下也呈现诡异分层，去掉延时则模型推理耗时正常（5.3ms）\n疑问：\n为什么推理耗时会表现出阶梯渐慢的样子 为什么最终耗时会稳定在成倍增长的地步 自答：\n该表现可稳定复现且有明显规律，说明很可能是撞到了某内部机制的误区 延时越短，表现越正常；延时越长，越早出现耗时跃变，分层时间也会缩短。也就是可能和实验一是同一类问题 需要进一步验证 实验三 寻找分层规律，测试所有模型\n测试模型推理，预热 50 次，循环 200 次（值为：毫秒）：\n分辨率/模型 localC FP16 localC INT8 localL FP16 localL INT8 1024x576 8.2 7.2 19.8 15.2 768x448 5.4 4.7 12.6 9.9 测试脚本，循环 800 次，延时 20ms（值为：跃变时次数 / 稳定时次数，√ 为无分层）：\n分辨率/模型 localC FP16 localC INT8 localL FP16 localL INT8 1024x576 50/300 60/300 √ √ 768x448 60/275 60/175 50/125 60/300 分层稳定后耗时分别为（值为：毫秒）：\n分辨率/模型 localC FP16 localC INT8 localL FP16 localL INT8 1024x576 12.2 12.1 16.8 13.2 768x448 11.4 10.1 12.4 12.4 后续实验：\n增长延时后全部模型都会分层，减小延时后全部模型不会分层 分层最终时间也与延时有关，延时越长，稳定后推理耗时越长 将延时函数加到 测试模型推理 场景（也就是步骤 3）中，分层现象依然可以复现，所以可以排除外设总线相关的原因，问题出现在显卡内部 自答：\n众所周知 TensorRT 在连续推理中做了优化，或许“连续”是指在一定时间范围内，而且这个时间范围会随模型计算量改变。也就是小模型只能在更短时间内才算连续推理，而大模型则宽裕一些。然后破坏连续推理的模式就会使模型推理速度变慢，但这依然没能解释为什么会经历推理耗时正常-线性减速-耗时稳定的现象 所以只在这种程度上分析还是有所不足，如果能在本地采用 Nsight Compute 或许能有进一步了解 ","date":"2022-06-28T00:00:00Z","permalink":"/p/tensorrt%E8%A7%86%E9%A2%91%E6%B5%81%E6%8E%A8%E7%90%86%E9%80%9F%E5%BA%A6%E5%BC%82%E5%B8%B8%E8%AE%B0%E5%BD%95/","title":"TensorRT视频流推理速度异常记录"},{"content":"A Style-Based Generator Architecture for Generative Adversarial Networks https://arxiv.org/pdf/1812.04948.pdf\nMain Contribution 提出了一种能够控制图像生成过程的生成器架构。通过注入噪声来自动、无监督地分离随机变化（人的各种不同属性，比如发型、雀斑等）以及高层特征（比如人的身份、姿态等特征），同时还能输出特定尺度的风格混合插值图像。 将输入 latent code 嵌入到中间 latent space，对 variation 因子是怎么在网络中表示的有深远影响。作者认为输入 latent space 必须遵循训练数据的概率密度会导致某种程度上必然的纠缠，而中间 latent space 不受限制从而可以免于纠缠（实验在原文 Table 4）。同时文章还提出了 perceptual path length 和 linear separability 两种新的自动化指标来衡量生成器的解缠度。 提出了新的高质量人脸数据集 FFHQ 以及配套代码和预训练网络。 Architecture 和传统生成器相比，使用常数作为输入，并将输入 latent code 经过一个网络映射到中间 latent space，输出 w 经过转换得到用于控制 AdaIN 的两组仿射变换参数 A：(ys, yb)：\nAdaIN 调节各通道的权重来为控制风格混合程度提供基础。最后显示引入噪声 B 来引导生成随机细节。\n总的来说，基于 Progressive GAN baseline 的改进思路：B. 双线性上下采样（《Making Convolutional Networks Shift-Invariant Again》）C. latent space 映射网络和 AdaIN。D. 发现常规的输入 latent code 已经 no longer benefits，所以使用学习到的一组常数作为替代以简化架构，这时仅对 AdaIN 输入就已经有很好的结果。E. 引入噪声。F. 加上 mixing regularization 来解相关相邻的风格，允许更细粒度地操控。\nProperties 关于为什么网络能学习到各种风格，并通过调节特定某种层的 latent code 就能控制特定某种风格来生成图像，作者的解释：\nWe can view the mapping network and affine trans-formations as a way to draw samples for each style from a learned distribution, and the synthesis network as a way to generate a novel image based on a collection of styles. The effects of each style are localized in the network, i.e., modifying a specific subset of the styles can be expected to affect only certain aspects of the image.\nTo see the reason for this localization, let us consider how the AdaIN operation (Eq. 1) first normalizes each channel to zero mean and unit variance, and only then applies scales and biases based on the style. The new per-channel statistics, as dictated by the style, modify the relative importance of features for the subsequent convolution operation,but they do not depend on the original statistics because of the normalization. Thus each style controls only one convolution before being overridden by the next AdaIN operation.\nDetail Mixing regularization\n在训练时，一部分图片（论文中用 90% 的训练集）使用两种 latent code (z1, z2) 来生成（在某层前后分别使用两种不同的 z1, z2 生成的 w1, w2 ），防止网络假设前后层风格相关导致依赖。加入这个后，在测试时使用多种 latent code 来混合输出 FID 也不会降低太多。 Stochastic variation\n传统生成器从输入层送入随机因素，网络需要消耗容量去转换成所需的伪随机基图像，同时也很难避免周期性。文中采用卷积后直接加入噪声 B 来回避这个问题，同时发现噪声的影响被网络很紧密地限定在了局部。作者假设对于生成器的任一点都有压力取生成新的内容，而最简单的方法就是依赖这些噪声。因为每层都有这些噪声的存在，所以不必从更早的激活层去得到这种随机性，这就导致了这种局部效应。（ 这里噪声在不同层的大小对生成图像的纹理、局部细节和整体风格都有些微不足道的有趣的结果，可以看看论文里的图表 ）\n由于 latent code 是以同等参数改变所有层的（而且是以仿射变换形式，而非噪声所做的像素级增减），所以风格的全局影响会被剥离出来。这和以往的风格迁移方法（Gram matrix, channel-wise mean, variance 等编码图像风格，然后随空间变化的特征编码具体特例）是一致的。尝试改变噪声来控制风格会使空间上的生成有所偏差从而被判别器惩罚，所以最终不需要明确指导网络也会学习区分全局和局部的控制。 Disentanglement studies 解缠的一般目的是：latent space 由线性子空间组成，每个子空间控制一种风格变化因素。然而 latent code 的抽样概率（一般是各维度等长来抽，也就是固定的球状分布）需要和训练数据的密度（比如分布是缺了块角的，只有部分人才有的风格）相匹配。这就是 intermediate latent space 存在的目的，能通过学习将抽样空间扭曲到和训练数据相当的程度，从而能更加的线性。\n作者觉得解缠的表示比起纠缠的表示在生成现实图像上会更加容易，所以假设有种压力让生成器去这么做。不幸的是，最近一些研究提出量化解缠度需要一个编码网络来将输入图像编码到 latent code（ 这应该是更加接近现实的合理情况，pixel2style2pixel 算是一个 ），作者提出了两个替代方法来衡量解缠度。\nPerceptual path length\n直觉上来说，一个不那么曲折的 latent space 应该比曲折的 latent space 在感知上输出的图像变化更加平滑，而且在曲折的 latent space 上做插值就会出现明显的纠缠。于是作者提出一种方法，通过对随机两个 latent code 进行随机插值（可以是 z space 上球面插值，也可以是 w space 上线性插值），同时再增加微小增量（1e-4）取插值的邻域，计算两种插值 latent 生成的图像的 VGG 特征的差值平方和（lpips 距离）再除以微小增量的平方，随机采 100,000 次得到期望值作为衡量 latent space 到生成图像的平滑程度的方法。\n这里对 z 的插值和对 w 的插值会稍有不同，因为 z space 一般是训练时用的随机正态分布（代表训练集的范围，上图 b），而 w space 我们期望是解缠且 flatten 的（代表学习到的特征的范围，上图 c），那对 w 插值可能含有从 z space 映射不来的区域（如一些男性特性在女性中不存在，上图 c 的右下角）。所以区分为两种 PPL 方法，一种是 full-path，就是不管三七二十一取两个随机 z1,z2 生成的 w1,w2 做正常的线性插值得到 w3 及其微小增量的邻域 w4，另一种是限定 endpoints，插值 w 时只取对 z 没影响的小 w，也就是只取随机 z1 生成的 w1 及其微小增量的邻域 w2，这就能确保 w 在期望范围内（原话：It is therefore to be expected that if we restrict our measure to path endpoints, i.e.,t∈{0,1},we should obtain a smaller lW while lZ is not affected.） Linear separability\n作者提出一种通过超平面切分 latent space 将 latent 分到两个不同集的方法来衡量解缠度。首先要得到预训练的特定属性的分类器，用来检测 latent 对应生成图片是否存在对应属性。然后每属性采 100,000 个置信度大于一半的样本经过分类器，得到结果作为标签针对每个属性训练线性 SVM 分类器，计算条件熵 H(Y|X)，X 是 SVM 预测结果，Y 是预训练分类器的结果，最后综合 40 种属性求期望作为分类程度指标。 Analyzing and Improving the Image Quality of StyleGAN https://arxiv.org/abs/1912.04958\nMain Contribution 重新设计归一化，解决 StyleGAN 生成图中存在的 artifacts，更换 Progressive GAN 为不需要改变拓扑也能完成由低到高分辨率生成的架构。 使用 perceptual path length 做 metric，并基于此对生成网络做正则化以达到平滑映射，最终发现从图像到 latent space 映射的效果比原始版本明显好得多。 Detail Architecture\n原始 StyleGAN 生成图中会出现水滴状 artifacts，而且从 64x64 分辨率特征图开始就被发现存在。文章指出这是 AdaIN 的问题，因为对每通道 feature map 做独立的 normalization 会毁坏掉根据各通道特征图间相对大小发现的任何信息。作者假设生成器故意将信号的强度信息偷偷传到 IN 来垄断统计特性，这样生成器就能像其他地方一样高效缩放信号了（照这么说所有 Instance Normalization 都会有这个问题，虽然我也没试过 IN 比 BN 要好的情况，即使 batch size 很小）？实际上去掉 AdaIN 后 artifacts 就消失了。\n作者将 AdaIN 分解为 normalization 和 modulation 两部分，并认为 modulation 的 bias 和 noise 会受到 normalization 中幅度操控的负面影响，而且发现把这两个操作移到 normalization 后会得到更加可预见的结果，而且 mean 也是可以去掉的，最后把常数输入的变换去掉也没有可以观察到的缺点，于是得出架构 (c)\n去掉 normalization 可以让 artifacts 消失，但会失去现有的 style mixing 能力（因为前面层的 mixing 会导致幅度变化，没有及时通过 normalization 抵消这种变化就会影响后续层的 mixing）。作者提出一种通过对输入特征图的期望统计量做非显式强制性的 normalization 来维持这两种优点。具体而言就是对卷积 weights 做 modulation 和 demodulation（类似 BN 计算图融合，不过引入了动态风格调制的向量）。modulation 就是每卷积核通道乘上对应调制系数，demodulation 的实现则需要一定弱化，假设输入 activations 是独立同分布的单位标准差的随机变量，那调制后输出 activations 的标准差就是调制后卷积参数的 L2 范数，所以除上这个数就当做 demodulation 了。作者指出这样统计分析在 xavier 和 kaiming 这些网络参数初始化技巧里有广泛应用，但很少用于替代目前这些依赖于数据的 normalization。之前也有工作指出对权重的单位归一化在 GAN 训练中是有益的（ WGAN 和 Spectral Normalization 应该也是这么回事 ）\nQuantitative analysis\n对生成模型的生成图像质量的量化评价依然是充满挑战性的主题。文章采用了三种方法衡量。一种是 FID，衡量在分类模型的高维特征空间中两分布的密度差异，另一种是 Precision 和 Recall，似乎是一种明确量化 生成图像和训练图像达到相似的百分比 和 能生成的训练数据的百分比？这两种都基于分类器网络，而近期研究表明它们在 ImageNet 上训练后更专注于纹理而不是形状（ [1] ），这和人类的认知方式相反（人更注重物体的形状），这会导致一些 FID 和 P\u0026amp;R 指标一致但质量却很不相同的图像出现。作者观察了第三种方法 percetual path length（PPL）与感知图像质量的关联，这是一种通过计算 latent space 上小扰动下的生成图像间的 LPIPS 距离来量化 latent space 映射到输出图像的平滑度的方法。作者发现平滑的映射空间似乎对应着更好的生成质量。作者假设生成器训练为了减轻判别器的惩罚，最高效方便的方式就是将生成高质量的 latent space 区域扩展，挤压低质量的 latent space 区域，这会在短期增加平均的输出质量，但累计的失真会影响训练动态以及最终图像生成质量。\n话虽如此，也不能直接以 PPL 最小化作为目标（会导致 recall 降低），于是作者提出新的正则化方法。\nLazy regularization：首先为了降低正则化计算代价，作者发现正则项的计算频率可以低于 loss 的计算频率，比如 R1 正则化每 16 个 mini-batches 才计算一次也不会有损害。\nPath length regularization：鼓励 W 变化固定步长会导致非零固定幅度的图像变化。这通过在图像空间随机方向步进，观察对应的 w 梯度？梯度应该和与 w 或图像空间方向无关的相同长度接近？这表明从 latent space 到图像空间是良定义的。\nR1/R2 正则 为了收敛还用了一种正则化技巧 [2]，也就是 R1 正则化（和 R2 效果相当），大意是以判别器的梯度构建一种惩罚项，让判别器在原始数据分布和生成数据分布上尽可能梯度要小，从而接近收敛的条件（雅克比矩阵特征值有负实部无虚部，且两个分布绝对连续）\nReferences http://arxiv.org/abs/1811.12231 https://blog.csdn.net/w55100/article/details/88091704 StyleGAN2-ADA https://arxiv.org/abs/2006.06676\n通过对判别器进行多策略数据扩增，大大减少了训练需要的样本量。为了保证对判别器的扩增不会影响到生成数据分布，需要动态调整扩增的 p。（不过实际尝试发现还是会有可能影响的，而且用几 k 数据集训练效果也只能差强人意）\nReferences https://blog.csdn.net/WinerChopin/article/details/113666346 StyleGAN3 https://nvlabs-fi-cdn.nvidia.com/stylegan3/stylegan3-paper.pdf\nStyleGAN2 存在像素粘连的情况，高层特征（纹理等）出现的位置不是由低层特征出现的位置所决定。作者阐明这是由于生成器卷积上采样非线性等结构不当导致的混叠现象（[1] 其实一直都违反采样定理，但因为精度至上所以一直不怎么在意），可以改进让其拥有等变性（平移旋转时纹理跟着生成目标）\n看代码核心在于设计低通滤波器在生成器各级上采样时抑制掉高频，只在最高分辨率输出时才允许高频特征，也就是将上采样模块魔改了下\n平移等变性则是增加了一些手工设计模块以及输入 latent 处理模块来加强。旋转等变性则是将所有 3x3 卷积换成 1x1 卷积，然后加大通道，并且修改下采样滤波器为径向对称型\nReferences https://arxiv.org/pdf/1904.11486.pdf https://blog.csdn.net/weixin_38443388/article/details/121050462 https://zhuanlan.zhihu.com/p/425791703 Application StyleGAN 是一种很有“搞头”的生成器框架，因为它提供了能够生成和提取（增加 encoder）图像的高层、中层、低层语义信息的框架。通过迁移学习或其他方法，就能使用它来生成想要的数据集以及一些语义操控生成。\nModel interpolation 拿到两个 StyleGAN 生成模型，我们可以通过插值两者的参数来获得其“中间状态”。\nLayer Swapping 拿到两个 StyleGAN 生成模型，交换某些层的参数，由于低层低分率的参数会主导姿态、身份等信息，而高层高分辨率参数会主导颜色、纹理等信息，这样交换后也能得到混合的“中间状态”模型。\nLatent Space Embedding 实际更常见的情况，我们需要根据现实图像来引导 StyleGAN 生成图像，这就需要考虑怎么得到现实图像对应 StyleGAN 的 latent code，目前主要有两种方法：\n从随机 latent code 开始，然后用 VGG 之类的网络衡量现实和生成图像的差距，再根据 gradient descent 的方法来训练出最接近的 latent code。在 StyleGAN 的官方实现中有相应代码。 训练一个 encoder 将现实图像映射成 latent code。 两种方法各有优劣，(1) 主要优势在于稳定而且泛用，可以最大限度利用生成器的潜能，缺点是每张图片都要训练。（2）主要优势在于灵活且快速，可以利用 encoder 来进一步实现高级的语义操控，而且只需要一次训练，推理速度快，缺点是不能泛用。\nRobust 为了测试 StyleGAN 的 robust，采用上述 (1) 的 latent space embedding 方法来探索它的极限，可知道一些关于 latent code 和生成质量的重要细节，其中比较有趣的地方有几点：\n在一个域中训练好的 StyleGAN 也可以相当程度的通过 latent code 还原其他域的图片。不过该还原的程度和预训练的 StyleGAN 的 latent space 质量有显著关联（FFHQ 训练出来的生成器质量要比 LSUN 的好，实际尝试发现迁移后会变差，大概就是和样本质量有关吧） FFHQ 训练出来的 StyleGAN 对仿射变换很敏感，特别是 shift_（以前也试过 shift 会导致生成质量差，大概是生成时默认了中心点的原因）（StyleGAN3 解决了这个问题）_ 尽管可以还原其他域的图片，但对两张其他域的图片的 latent code 做线性插值然后生成得到的 Morphing 图会十分混乱无意义，而且隐约能看到有类似人脸的图样出现，说明即使是在生成其他域的 latent space 中人脸依然是主要部分 还原其他域图片时，从随机初始化的 latent 开始要比从平均的 latent 开始要好，还原预训练域结果相当（不过需要在 W+ space 下） 更多细节参考 [2]\nPixel2Style2Pixel 设计 encoder 将图像转换成 StyleGAN 的 latent code，再通过该 latent code 调节 StyleGAN 输出以达成各种任务，输出的 latent code 可以是 W space（512），也可以是 W+ space（18x512）\nEncoder4Editing 论文：https://arxiv.org/abs/2102.02766\n和 pSp 的 Inversion 任务只注重还原质量比起来增加了可编辑性，完成训练后编辑的方法分为三种：\n沿着一些语义方向（1x512）增加偏置 针对 GAN 空间做 PCA 结果，得出其和原 latent 的协相关偏置？加到原 latent 中 sefa 方法，也就是求生成器调制核的因式分解项（矩阵归一化内积求出特征向量），沿特征向量方向增加偏置 HyperStyle 论文： https://arxiv.org/pdf/2111.15666.pdf\n摘要：当前 StyleGAN 的 latent space 研究存在重建的精准度和可编辑性之间的 tradeoff 问题。与最近业界使用目标图像微调方法将生成器迁移到可编辑 latent 区域的方法不同，作者提出了用超网络调制 StyleGAN 参数来达到在可编辑 latent 区域重现给定图像的方法，并且设计达到了较低的参数量和较高的推理速度。\n兴趣点：在域适应领域上，相比于 pSp、e4e、ReStyle 直接将源域 latent code 应用到目标域生成器上导致各种不匹配的特征增减，该方法通过给源域生成器训练超网络，然后将调制参数的 offset 应用到目标域生成器上来，再迁移 latent code，似乎会还原得更好。\n锐评：加上 interfacegan 和 ganspace 修改，纠缠还是有不少的，反转效果还行，域外表现不怎么样，主要和 direction 在域外不好使相关。一次插值生成在 V100 上大约 25ms。总之，没大用。\nReferences https://www.justinpinkney.com/ukiyoe-yourself/ https://arxiv.org/pdf/1904.03189.pdf ","date":"2022-04-25T00:00:00Z","image":"/p/stylegan/arch1_hub988efc80824a295e46270c7ce0704e1_32816_120x120_fill_q75_h2_box_smart1_2.webp","permalink":"/p/stylegan/","title":"StyleGAN"},{"content":"week1 人的思考分为两种模式，无法同时存在\nFocused mode：小范围固定的线路，用于深入分析某个问题（以既定的思考模式） Diffuse mode：大范围发散的思路，用于广泛的思考新事物，不会深入 大脑工作的认知\n学习困难的东西需要在发散和集中思考间来回切换锻炼，就像锻炼肌肉一样锻炼思维 大脑在默认的情况下的活跃区（潜意识思维）和思考时的活跃区是不同的，很多事情在静默下由大脑自动完成 大脑认知是动态的，在学习新知识时大脑会形成新的突触，睡觉时也会如此 对于不同概念学习的难度会有所不同，但都需要反复锻炼\n具体来说和抽象层次有关，比如通过将动物图片和物种名放在一起可以 直观 类比地学习到该物种名文字代表的概念，更抽象的 “爱、热情和希望“ 些文字的含义也是我们在人生中反复感受过才能理解的 对于数学和科学的概念，由于没有现实能直观感受的类比物，所以会较难学习理解，更需要重复练习（形成神经连接会在此增强） 理想的练习：一开始解决问题建立了微弱的神经模式，在不看答案前提下再次从头解决问题加强该神经模式，最终对问题熟练到能不看解法完整精确的在脑海中重温每个步骤 练习的方式：专注模式学习一段时间，然后休息让大脑处于发散模式巩固理解，番茄钟是个好东西 人的记忆分很多种，主要介绍两种：长期记忆（long-term memory）和 工作记忆（working memory）\n工作记忆在前额叶皮质区，有四个组块“插槽”，就像一个低效的精神黑板，需要经常重复来记忆以免被自然或代谢遗忘，比如记电话号码 当你学习时，思路就会顺着四个插槽蔓延向大脑的各个区域去读取相关组块记忆，刚开始学习时四个插槽得到的信息会互相纠缠，当你尝试组块化时，这些纠缠的信息会逐渐理清，最终只得到占用一个插槽的组块，多余的细节会被清除，而这个单插槽组块思路会成为成熟的思考回路，这时工作记忆的插槽就像是链接向巨大记忆网页的超链接 长期记忆很大，不过记忆间可能会有重叠，学到的东西都在这里 为了将工作记忆里的新东西放到长期记忆里，需要间隔重复的练习，可以根据遗忘曲线来进行 睡眠对学习记忆也非常重要\n非睡眠状态会在大脑内累积有毒物质，睡眠时脑细胞缩小从而让这些物质能够被液体？冲洗掉 睡眠时大脑会重复排练你认为困难的内容，强化重要记忆，删除不重要记忆 睡眠对 “解决困难问题的能力” 和 “理解所学内容的能力” 有显著意义。因为你的意识从大脑前额皮质中完全解放了，能够更轻松和其他部位沟通。当然要先在 focused mode 下工作才能在睡眠中启用 diffuse mode 睡前复习+给自己想要梦见所学内容的暗示 可以加大梦见所学内容的几率，梦见的话能增强你对该内容的理解能力（记忆整合） Sejnowski 访谈\n学新东西实践第一，看很多书不好使 听讲座时准备问题”伏击“演讲者，主动参与会比被动听好（还有一个技巧就是听完后用 30 秒记下核心观点，一定要在结束后马上进行） 户外运动进入 diffuse mode，随身记录想法不然易忘 大的活动空间 和 运动 可以让海马体的神经增加 活跃的创造性的人际氛围非常重要 你需要的是 热情 和 坚持 week2 组块（chunk）：通过意义连接在一起的结构化信息碎片，比如 P、O、P 连在一起组成单词 pop 就是一个组块。在神经层面上属于复杂的神经环路，该环路可以通过专注训练重复来增强，并且可以建立出新的更大的组块。组块化后就能更高效的记忆。\n组块的构建\n学习的过程：将小组块构建成大组块 ==容易过于关注某一单独步骤而忽略步骤间的练习== 要记住例题解法应当是旅行到陌生地方的地图，使用地图时需要时刻关注周围的变化 组块构建需要三点 专注，为了让新知识和其他知识有全面的连接，你需要专注，让四个组块插槽都为当前知识的组块化服务 理解，只有在理解了知识的基础上才能将当前记忆痕迹和其他痕迹联系起来，是组块构建的充分条件。学习专业知识时对需要的背景知识和基础概念有充分了解是很有必要的。 练习，练习可以加强神经网络，更重要的是在独自练习下你会更清楚通过什么途径能够访问组块，也就是一题多解，而在缺乏练习下很有可能会出现以为懂了其实不懂的情况，就是因为没有独自走过整个解题思路（大背景信息） 学习的过程 学习由自上而下和自下而上组成 自下而上就是组块构建的过程，使你能够在需要时轻松获取所需知识 自上而下就是从全景图出发认清大纲、概念图以及背景知识原由等，能让你清楚认识在学习什么以及适用于哪里 两者交汇的部分就是上下文（Context），重复的练习相关或不相关的问题，让你知道什么场景什么时刻用什么组块，对问题能够看得更远，这样就能将组块融入框架中 回顾（Recall）\n读一遍然后试图在脑海里回忆概念会比多次阅读效果好。而最好的学习过程就是阅读和回顾的相结合之后的成果\u0026ndash;思维导图，画出相关概念之间的联系是最佳途径，但这需要你先打牢根基（大概就是先阅读-回顾切换，熟悉后才开始画思维导图） ==自己明白才是真== 能力错觉：读笔记和课本会给人一种掌握知识的错觉，因为这个过程依然是被动的，而不是主动的回顾，你需要==自测==来确保自己是真的掌握了。所以别随便画线记笔记（没有真正理解思路的话盲目摘抄会形成误导） 自测过程中出现错误是纠正思考的好机会，纠正你的思考方向来避免下次错误会让你做得更好 在不同场景下回顾知识可以让你避免潜意识中对当前场景的依赖（考试时傻眼的人有必要） 误区\n概念映射（比喻）没有简单回忆（回顾）好使，毕竟要先把概念真正给理解了才有用 动力的神经层面（optional）\n乙酰胆碱：影响专注学习、集中注意力的神经递质 多巴胺：得到意料之外的惊喜时出现的神经递质，还会驱动你去完成对未来有益的事情（未来决策） 血清素：影响社交行为，以及对风险行为的控制程度。社交地位高的具有高水平的血清素，反之则低，低水平的血清素分泌与冒险行为有关 情绪：在人脑底部的杏仁核，与海马体共同参与记忆和决策 组块库\n即使是完全不同领域的组块之间也可能存在共通点，从一个领域的组块来帮助另一个领域的组块形成过程叫“迁移”（Transfer），建立更多更大的组块会让你更好理解知识以及找到问题的解决方法 最终可以发展出两种方式来理解知识和解决问题：1. 专注模式下顺着思路推理前进。2. 发散模式下通过直觉来找到跨度较大的答案（但是还需要经过专注模式来验证是否正确） 学习的细节层面\n过度学习：重复练习以掌握内容以达到自动性（Automaticity），对于 演讲、运动、演奏 之类的事情来说很有用，但会容易浪费时间以及产生能力错觉 刻意训练（deliberate practice）：专注于学习困难的部分，而不是已经掌握的那些 思维定势（Einstellung）：一开始出现的思路或已经加强过的神经模式会阻碍你发现更好的想法，当学习新事物时必须摒弃旧的思想和方法 交替学习：承接思维定势，理解如何得到问题的真正解决方法是非常重要的。为什么要用这一种方法而不能用另一种方法，这种发问会促使你独立思考，并且学会去找到更加合适的方法来解决问题。交替学习多种学科、提前看章节末不同类型的问题或者在不同章节间切换学习会让你的思维更加灵活而且富有创造力 week3 拖延（procrastination）\n拖延是感觉到痛苦于是转移注意力的过程，类似于成瘾。长期记忆需要一定时间来完成，而拖延是其大敌 人们会很经常进入“僵尸状态”，也就是某种习惯，在行为高度组块化后，只要脑子里产生了相关念头就能自动的完成，比如骑自行车 这种习惯可以分为四个阶段 cue（信号）：进入僵尸状态的触发器，比如微信响了，或者该到做作业的时候了。无所谓好坏 routine（惯性行为）：根据信号做出的惯性反应，行为可能好也可能坏 reward（奖励）：惯性行为是为了能够带来奖励 belief（信念）：也就是如何看待这个习惯，这会决定你是否能改变习惯。如果是消极的则容易放弃好的习惯 当我们开始学习或者做不愿意做的事时，容易因为产生负面情绪而开始进入某种习惯，这也就是拖延的本质 解决拖延\n之所以会痛苦，很可能是因为对 结果 本身感到痛苦，所以需要将注意力转移到 过程 上面，开启一个简单的、无负面影响、专注于过程的习惯才是解决拖延之道，不要过于依赖意志力 根据习惯的四个阶段可以对习惯进行改进： 注意信号，确定是什么插曲导致了拖延习惯的发生，比如隔绝手机、互联网之类的信号会是很有用的 改变惯性行为，制定计划养成新的惯性。番茄钟会很有效帮助你进入到过程中（用番茄钟时不要专注于结果） 要给好习惯的完成以奖励，调查拖延带来的暂时奖励，寻找更好的替代品，用以奖励新习惯。只有让大脑神经期待新的奖励才能替换掉旧的习惯（如果不知道想要什么奖励的话，可能这是最难的一步） 相信你能做到，有时候养成新习惯会遇到棘手的情况，容易打回原形，好的信念则能让你坚持下去。可以通过寻找伙伴打造良好氛围来达到 一些好的方法\n在前一天晚上写下任务清单（Dailynote 的 Plan 应该可以），有助于放下心思并在睡觉时让大脑在潜意识中设法解决任务。还有一点，定下下班时间，计划好休息时间和计划工作时间一样重要（大概是为了合理奖励） 同时要记录下有用的或没用的技巧以及分析没有完成任务的原因 早上先完成最困难最讨厌的任务（没有早上的人感觉不行） 记忆\n人类有强大的视觉和空间记忆能力，用视觉联系知识有助于记忆（虽然我觉得不好用），当然要转为长期记忆还是需要==间隔重复练习== ==索引卡==是个好方法，在纸正面写下索引缩写（或者知识概念名词？），然后在背面写下具体需要记忆的内容，这样就能测试通过看正面来回忆背面内容，而且书写的过程也会帮助你更深入转化信息，Anki 是重复练习的好工具（Obisidian 也有间隔重复的插件，但索引卡功能） 长期记忆\n海马体负责将短期记忆放入长期记忆 记忆是动态变化的，回忆的时候会发生再巩固（reconsolidation），睡眠时也会，所以间隔学习会更有效，想象力丰富的孩子甚至可以通过想象植入错误的记忆。 星形胶质参与提供神经元营养、维持细胞外离子平衡以及受伤后修复，可能和学习能力有至关重要的联系 意义联系，记忆宫殿\n通过一些助记方法可以有效加强记忆，比如首字母缩写连在一起什么的 记忆宫殿：在想象中将要记的概念视觉化放在你最熟悉的一些地方的特定位置。其中重点在于利用视觉、听觉甚至嗅觉来帮助记忆，还能锻炼创造力。（感觉可以将大框架构建成一个机器或者什么，然后将知识概念通过机器运转的过程记下来） week4 脑的发育\n海马体似乎能不断产生新的神经元，不用的话会死掉，而学习和运动可以让神经元存活 0-2 岁是立体视觉成熟期 前额皮质（prefrontal cortex）是最后成熟的（成年初期成熟），与分析复杂事情、社交、决策、计划的能力有关 视觉隐喻和类比（Metaphor，将事物过程用视觉化想象的方式在脑海中重构）是帮助理解记忆的最佳方式之一，而且能帮助解决思维定势，总之非常有效\n右脑为全局思考，左脑为细节思考。学习结束后需要歇息一下从专注模式（左脑）解放，并回顾整体从全局出发去思考（右脑）\n交流讨论以避开思维定势\n测试是非常重要的，最能集中精神的也是测试\n备考可以通过测试清单来检查自己有没有做好：\n你是否认真努力地去理解过课文？仅仅是找出课文里有解答过程的例题不算 你是否跟同学讨论过作业中的问题或者 至少和其他人对过答案？ 你是否尝试过在和同学讨论之前 先列出每道作业的解题大纲？ 你是否积极参与作业小组中的讨论，贡献自己的观点 并提出问题？ 当你遇到问题的时候是否会去咨询讲师或助教？ 交作业的时候你是否已经弄清了所有问题的答案？ 对于作业中不明白的问题你是否在课上提出疑问 寻求解答？ 如果你有辅导书 在考试前 你是否已经认真通读它 并且相信自己弄明白了书上所有的问题？ 你是否尝试略过具体计算直接快速写出一些问题的解题思路？ 你是否和同学一起复习过辅导书上的内容和其他问题并相互提问？ 如果考前有复习课 你是否参加过 并对自己不确定的部分提出疑问？ 最后 考试前睡眠时间安排是否合理？ 先难后易，想不出来的话一两分钟就跳过做简单题，之后再回来看看难题，可以让大脑在发散下更好处理难题 （没觉得会有用，几乎从来遇到过没有一开始做不出来但过一会就能做出来的难题）（对测验准备得很好时才有效，而且需要事先练习以及确保方法适合自己）\n在想法上把恐惧焦虑转变为兴奋\n","date":"2021-12-22T00:00:00Z","permalink":"/p/%E5%AD%A6%E4%B9%A0%E5%A6%82%E4%BD%95%E5%AD%A6%E4%B9%A0/","title":"学习如何学习"},{"content":"源自 https://github.com/fxmeng/filter-grafting\n一种通过使用不同超参训练多个模型，并在训练过程中相互融合模型参数以达到减少无用卷积核数的方法，原理比较玄学，原论文发表在 CVPR 2020 上\n融合方法 根据每层模型参数的熵来自动调节融合权重 alpha\n首先：\n$$H(x)=\\sum_i^np(x_i, x_{i+1})*log\\frac{1}{p(x_i, x_{i+1})}$$\n将参数 x 根据值大小分为 n 等分，统计在 [i, i+1) 区间的参数数量，得到概率进而计算熵 H(x)\n权重则为：\n$$ alpha = \\frac{A}{\\pi}\\cdot arctan(c\\cdot (E(W_i^{M2})-E(W_i^{M1})))+0.5 $$ 其中 A 和 c 为超参数，得出的 alpha 范围应该在 [0.5-A/2, 0.5+A/2]，arctan 大概是为了将输出值域限定，并用 A、c 来调节不同熵差下的输出斜率（c 越小在零点附近就越平滑，c 越大零点附近越陡峭）\n实际效果 [[模型改进实验-202110221125]]\n在小模型上部分时候指标会变好，效果看起来差不多，可能会产生行为有所不同的模型。总之在图生成领域不算多有效的方法，实验的模型和任务下零核数少也可能是原因，但有时候有好过没有\n","date":"2021-06-23T00:00:00Z","permalink":"/p/%E6%A8%A1%E5%9E%8B%E5%AB%81%E6%8E%A5/","title":"模型嫁接"},{"content":"Pix2PixHD 《High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs》： https://arxiv.org/pdf/1711.11585.pdf\nMain Contribution 增加一个 loss：真假样本的 Discriminator 的中间特征距离（Feature matching loss），再配合 VGG 之类 Perceptual Loss。算是最可看的一点。 高分辨率生成。Generator 使用 Coarse-to-Fine 逐步生成高分辨率，也就是论文里的先训练低分辨率 Global，然后在外面套一层 Local Enhancer 把中间特征跳过去加起来以求得到高清高分辨率。Discriminator 则送进两种尺度的图像。经过实际验证多尺度输出+多尺度监督会更好，这些都算是落地标配了。 Architecture pix2pix：使用 UNet generator + patch-based discriminator，输出 256x256，直接输出高分辨率的话不稳定且质量低，于是进行改良。\nCoarse-to-fine generator G1 为 global generator 处理 1024x512 图像，G2 为 local enhancer 处理 2048x1024 图像，同样的可以扩增出 G3 等等\nG1 采用架构为：https://arxiv.org/abs/1603.08155 ，其中提到的两个 Perceptual Loss Functions\nMulti-scale discriminators\n高分辨率需要大的感受野，而更大的卷积核或者更深的网络会有潜在过拟合的可能，且内存增长也很大。于是采用图像金字塔，对输入图缩小几次，得出几种尺度的图像，再根据缩放次数建立多个相同架构的 discriminators 来分别处理各自尺度（Patched Base），最后综合起来得出结果\nFeature matching loss\n使用各个 discriminator 各层特征的 L1 loss 来让生成图像和真实图像在判别器特征层面上相似，这可以说是类似 Perceptual Loss（在超分辨率和风格迁移很有用的方法）。实验进一步表明，一起使用会有更多提升 结合 Feature matching loss 和 GAN loss，得到最终的 loss： Instance maps\n为了解决 semantic label 无法区分物体的缺点，需要引入 instance maps，但是由于事先不确定 instance 个数，所以不好实现。基于此，作者指出 boundary 才是其中最重要的信息，先计算出 instance boundary map（四邻域里有不同的 label 则为 1，否则为 0），再 concat 一起送入 generator 和 discriminator\nImage manipulation\n为了使 manipulation 结果多样化且合理，加入 instance-level feature embedding，和 semantic label 一起作为 generator 输入。具体来说，需要额外训练一个 encoder-decoder，最后一层按 instance 进行平均池化，再将池化结果 broadcast 到 instance 每个像素。这样处理完整个训练集后，对各类别使用 K-means 就可以得出多种 instance feature，推理时随机选取一种 concat 输入进行 generate 就可以完成目的。encoder-decoder 的具体训练方法见论文 3.4。\nImplementation Notes 先训练 G1 再训练 G2，最后合在一起 fine-tune，作者提到此多分辨率 pipeline 易于建立，而且一般两种尺度就足够了 为了配合 Instance maps，semantic label 采用的 one-hot 形式 训练方法使用 LSGAN，Feature matching loss 的系数为 10 Experiments 使用了语义分割网络对真实图像和生成图像进行分割，比较两者的 mIoU 等差异，结果 pix2pixHD 的方法得到的分割指标接近使用真实图像的指标：0.6389 : 0.6857 Human A/B tests 非限制时间，500 张 Cityscapes 图像比较 10 次，产生 5000 个结果，统计在两种方法中选取其中一个的概率。结果：未使用 instance map 下 pix2pixHD 和 pix2pix 是 94% : 6%，pix2pixHD 和 CRN 是 85% : 15%，VGG Perceptual Loss 似乎没有起到明显的正负面倾向，对结果影响在 1% 内 限制时间，随机在 1/8s - 8s 间选取时间来展示给受试者，判断那张图片更好，据称可以看出需要多长时间才能意识到两者的差异，大概就是某种程度上能比较差异的粗略显著性 - 非限制 loss 比较，GAN + Feature matching + VGG Perceptual loss 比上单独 GAN loss、GAN + Feature matching loss 的 preference rate 分别为 68.55%，58.90%，稍微有一些提升，但不是很明显 还剩下了几个实验，但这些实际效果都存疑，其实作用都不是特别明显，实验有些偏颇，generator 和 loss 方面的提升应该是最明显的\n参考：\nhttps://zhuanlan.zhihu.com/p/56808180 https://zhuanlan.zhihu.com/p/68779906?from_voters_page=true Vid2Vid Pix2PixHD 的视频生成改进，主要是针对 temporally incoherent，我没有看论文，只看了一下代码和博客，Generator 大致的修改就是：\nPix2PixHD Global 的本体不变，输入改成前后多帧 concat 起来的图像，并且增加一支将过去几帧生成图像提取出特征加到主支上的分支，大概就是为了利用过去生成的图像来生成现在的图像。这样的话头几帧理论上就需要单独另外提供。 分出一支用来生成光流图和表示模糊程度？的掩码，用光流图 warp 最近一帧生成图像，根据掩码在 warp 的前一帧生成图和当前生成图中做加权，得出最终结果。 如果有前背景的掩码的话，再加一支类似 2 的分支，生成前景图和前景图掩码然后加权。 也就是说如果不使用前背景的话，实际需要两个 Pix2PixHD 的网络，输入增加前几帧，以及中间提取前几次生成图的特征后的耦合，还有一个光流 warp 的后处理。下面的图非常直观： https://img-blog.csdnimg.cn/2019030516345690.gif\n参考：\nhttps://blog.csdn.net/maqunfi/article/details/88186935 ","date":"2020-11-09T00:00:00Z","image":"/p/pix2pixhd/pix2pix_arch_hu6199731767d3a35c48a188e67800c4b9_22146_120x120_fill_q75_h2_box_smart1_2.webp","permalink":"/p/pix2pixhd/","title":"Pix2PixHD"},{"content":"整理一下常见的算法加速方法。一般来说，对于高计算量的算法，加速是必然要考虑的事情。理想情况下，在编写算法时就应该考虑让算法便于优化，下面就针对性谈谈。\n1. 多层循环 图像处理算法大多需要遍历图像，如果遍历操作较为复杂，这部分的速度就会很慢。所以最好就是能保证最里层循环只有 连续内存 的 简单运算 。如果能够展开来不用 for 循环有时候也会些优化。\n2. SIMD 意思就是单指令多数据流，允许一次处理多个数据，比如有 128bit 寄存器使用 SIMD 指令则可以一次处理 4 个 32bit 数据。前面提到的多层循环内连续内存的简单运算就可以通过 SIMD 进行大幅优化。SIMD 的使用需要特定的数据结构，函数可以通过查 Intrinsics 或者直接翻头文件找到。因为是处理器相关，需要看具体处理器是否支持，以及具体实现，比如 Intel 的 SSE，ARM 的 Neon。知道了用法替换起来就很方便了：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 // neon 下的矩阵乘法实现 void gemm_neon(const float *lhs, const float *rhs, float *out_mat, int rhs_rows, int rhs_cols) { for (int j = 0; j \u0026lt; rhs_rows; ++j) { float *data1 = (float*)lhs; float *data2 = (float*)(rhs + j*rhs_cols); float32x4_t production = vdupq_n_f32(0.f); int i = 0; for (i = 0; i \u0026lt; rhs_cols-3; i+=4) { float32x4_t lfactor = vld1q_f32(data1); float32x4_t rfactor = vld1q_f32(data2); data1 += 4; data2 += 4; production = vmlaq_f32(production, lfactor, rfactor); } float sum = 0.0f; for (; i\u0026lt; rhs_cols; ++i) { sum += (*data1)*(*data2); data1++; data2++; } float temp[4]; vst1q_f32(temp, production); sum += (temp[0]+temp[1]+temp[2]+temp[3]); out_mat[j] = sum; } } 3. 复杂算法重组 有时候算法的各步骤执行次序会对算法效率有很大影响，特别对于内存管理而言，最好集中在循环外处理。算法本身也可能会有冗余的计算，通过一些方法合并计算会带来一些性能优化。\n这其中对于密集计算的特例就是向量化（Vectorization）。将 for 循环控制的计算转化为矩阵运算，这在使用 Python、Matlab 等来设计算法时可以说是必备的。\n4. 牺牲空间换取速度 最为典型的就是查找表，通过预先计算一部分结果，并保存在一个大数组里，从而使后续算法能够加速运算。\n5. 二维数组优化 我们习惯将图像表示为二维数组，这样处理起来也更方便，但是二维数组运算的方便却会导致运算效率的降低，根本原因在于内存的不连续。虽然对于优秀的程序员来说不是什么问题，但我比较想提一提这种方法：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 float **init_matrix(int rows, int cols) { int i; float **m; m = (float **)malloc((size_t)(rows * sizeof(float *))); m[0] = (float *)malloc((size_t)(rows * cols * sizeof(float))); for(i = 1; i \u0026lt; rows; ++i) { m[i] = m[i-1] + cols; } return m; } void delete_matrix(float **m) { free(m[0]); free(m); } 6. 矩阵分解 将空间滤波分解成行列滤波，可以提升速度以及降低容量，这部分在 Dlib 中有具体的实现，了解得不是很多，不过除了这种简单分解外估计还有不少门道。\n7. L1 cache / 汇编 / 硬件 L1 cache 也就是利用缓存来优化密集存取的代码。大体来说减少变量的使用，尽量处理连续的空间。而汇编的话直接操作寄存器，可以对算法做非常底层的优化，针对具体芯片架构还会有更多可以优化的地方，甚至使用 FPGA 或 DSP 来并行运算、高速信号处理，或者合起来异构计算。这些我也不太了解，大概是 HPC 的内容了。\n8. 多线程/多进程优化 也就是 pthread 之类的，在系统层面上做优化，开发的时候注意好 I/O 阻塞、共享资源加锁、进程间通信等问题就好了。\n这里 Python 的 默认解释器 CPython 由于 全局解释锁（GIL） 的存在所以多线程不太灵光，可以采用多进程 + 协程的方式利用好多核 CPU。\n9. 等价运算 对于一些算法，有时候可以在数学上找到等价的运算形式，相当于走捷径，比如矩阵运算的一些等价性质，DFT 处理实信号的一些性质；或者转换后能够结合具体软硬件环境进行加速，像 Winograd 用中间变量和加法运算来减少乘法运算。这些转换是可以直接替换原有算法或者视条件替代使用的。\n还有一种比较邪门的方法就是寻找近似的计算，这方面是机器学习的特长。特别如果拥有大量的输入输出结果，但苦于设计映射的算法，这时候直接用机器学习去学习映射的矩阵会是简单又高效的做法。还有一种情况就是处理的数据自身存在大量冗余，传统方法计算量又很大，这时候可以通过在缩小冗余的数据上进行传统方法处理，然后用机器学习之类的方法对数据尺度进行扩大。NVIDIA 的 DLSS2.0 就是这种情况的优秀实现，在低分辨率下快速渲染，然后超采样获得高分辨率输出，利用了多帧信息以及各种奇技淫巧来使得最终输出就像直接处理高分辨率图像一样，这是相当有难度的，同时也是相当有价值的。\n","date":"2019-10-16T00:00:00Z","permalink":"/p/%E7%AE%97%E6%B3%95%E5%8A%A0%E9%80%9F%E7%9A%84%E5%87%A0%E7%A7%8D%E6%96%B9%E6%B3%95/","title":"算法加速的几种方法"},{"content":"这部分是关于在低计算量下完成人像分割的工作，因为时间充裕，所以调查尝试得比较多，最终完成的效果还不错。\n引言 人像分割（portrait segmentation）属于语义分割的子集，某种程度上类似于只专注人的前景分割，可以看成是二分类的语义分割。不过这里的应用场景是半身肖像，对于效果的评价更加专注于分割边缘和细节的质量，现在看来这次的工作在这方面其实做得比较一般，只是就通常的 IoU 意义上来看还不错。\n总体来说，人像分割方面的论文比较少，典型的就是 Xiaoyong Shen 等人的工作 [1] [2]。不过实际尝试过程中发现，其中的很多技巧无法有效的应用于低计算量的场景中，固定先验的引入也会导致分割效果在某些情况下变差。\n实现 数据集 数据集方面因为我采取了逐步迁移训练，多次 fine-tuning 的方式，所以需要从 ImageNet 到 COCO 人像子集再到标准半身人像分割的数据集。COCO 人像子集是从 COCO 中筛选出含人类（且占面积较大）的部分，大约 4 万张；半身人像分割的数据集训练集一千多张，验证集 50 张，测试集 300 张；还有一些私有的测试集。\n之所以这样做主要是因为最终用于应用场景的数据集过少，而且从 OSVOS[3] 等文中得到了更多的启发。\nOSVOS 的做法 设计 我主要尝试了 ENet [4]、FCN [5]、FCN dilated convolution 版本 [6]、UNet [7] 的改进版 以及 类 DeepLab V3+ [8] 中的方法。为了保持计算量，除了 [4] 外，我都采用了轻量化网络 ShuffleNet [9] 作为 backbone 来进行复现，大部分网络的输入限制为 224x224。\n对于 UNet 的改进版，因为是冲着实时而设计的，所以在 decoder 的设计上，尤其是底层特征的引入上显著减少了滤波器数量。当 output stride 为 4 时，可以得到的模型性能为：参数量 79.54K, FLOPS 24.44M, 访存量 46.98MB，可以达到实时。\n对于类 DeepLab V3+ 的版本，为了尽可能的压缩计算量，我最大限度的利用了 Depthwise Convolution、Dilation 以及 Channel Shuffle 的特性，同时为平衡精度将 output stride 限制为 2（损失非常少），decoder 也改成了更为节省计算量的简单版本，模型性能为：参数量 965.49K, FLOPS 743.37M, 访存量 392.87MB。\n来自底层特征的 skip-connection 是有很明显的边缘细化效果的，但同时也十分容易过拟合，这是值得注意的问题。 尽管我把 ASPP 写的很省，但尝试后发现如果那个结构不加 ASPP 的话性能会很差。 关于深度，尝试的结果似乎说明了 Encoder 部分需要足够的深（至少 8 个 res block 是不够的），而 Decoder 则不需要太复杂（没有足够的证据，不过增加 2 层能带来的好处不大） 训练 训练方面，UNet（实时） 和 类 DeepLab V3+ 方法因为是从头搭起，所以就像上面说的一样，需要两次 fine-tuning，其余方法并没有经过 COCO 人像预训练。优化方法我用的是 SGD+Momentum，事实上这是因为用其他的一些优化方法会导致“零核现象”的发生，我在以前的一篇文章中分析过这点，不知道这个现象和性能的下降有多大关系，但如果用 SGD 的话就不会有这种事情发生，而且性能会更好。\nCOCO 人像部分的预训练的加入很有效，但是必须使用和应用场景十分相似的部分作为预训练才有效，不筛选的话很可能没有效果上的影响。 使用大 Momentum（我用的 0.99）似乎不错，训练速度会很快而且效果也不会差（实验尝试似乎比 0.9 还要好）。 实际训练时还发现有挺严重的过拟合，所以做了些数据集扩增，结果挺有效的。 在 output stride 为 8 时增加一个辅助损失也是有效的。 测试 在 [1] 中给出的数据集中测试结果如下（其中我加入了 Std 度量用于观察分割的稳定性，意为 IoU 的标准差）\nMethod mIoU Std ENet 94.04% 6.25% FCN 95.73% 3.10% FCN + dilated 96.04% 3.27% UNet（实时） 95.32% 4.03% 类 DeepLab V3+ 96.4% 3.25% PortraitFCN+[1] 95.91% - 另外我还自己标了个五十多张明暗、运动、背景复杂度变化都比较大的测试图片，在此数据集上测试结果为\nMethod mIoU Std ENet 61.42% 20.04% FCN 87.71% 12.01% FCN + dilated 89.43% 10.62% UNet（实时） 91.96% 3.95% 类 DeepLab V3+ 93.7% 2.3% 可以看到类 DeepLab V3+ 方法测试结果还是挺不错的，其余的多个私有测试集上表现也不错，它在 MIX2 上前向一张图片大概需要 120ms。除此之外，达到实时性能的 UNet 改进版也不错，两者在 P-R 曲线上差异不是很大。\n非实时与实时网络的 P-R 曲线比较 最终的效果 可以看到边缘的跟踪还是不错的，除此之外，头发的细节处理也可以在顶部图中看到。\n因为 output stride 为 2 导致分割边缘在放大后加大了间隔，看上去和实际边缘有些距离。在下面的图中可以明显看到这种在人和物件之间的边界、甚至透明物件覆盖影响下算法的处理情况。\n虽然训练集全都是单人肖像，但多人的情况也能够分割，具体效果还是要看人物所占的尺度等。\n[参考文献]:\n[1] 《Automatic Portrait Segmentation for Image Stylization》\n[2] 《Automatic Real-time Background Cut for Portrait Videos》\n[3] 《One-Shot Video Object Segmentation》\n[4] 《ENet: A Deep Neural Network Architecture forReal-Time Semantic Segmentation》\n[5] 《Fully Convolutional Networks for Semantic Segmentation》\n[6] 《Understanding Convolution for Semantic Segmentation》\n[7] 《U-Net: Convolutional Networks for BiomedicalImage Segmentation》\n[8] 《Encoder-Decoder with Atrous SeparableConvolution for Semantic Image Segmentation》\n[9] 《ShuffleNet: An Extremely Efficient Convolutional Neural Network for MobileDevices》\n","date":"2019-04-19T00:00:00Z","image":"/p/%E4%BA%BA%E5%83%8F%E5%88%86%E5%89%B2/Einstein_hu425a277444e3247e832d80bb1c186570_489948_120x120_fill_box_smart1_3.png","permalink":"/p/%E4%BA%BA%E5%83%8F%E5%88%86%E5%89%B2/","title":"人像分割"},{"content":"以往在使用 caffe 中遇到的部分问题记录。\n使用教程 Caffe 一般通过编译生成的可执行文件 caffe（一般路径为 $CAFFE_PATH/build/tools/caffe）来进行网络训练和测试。\nTL;DR Python 调用（pycaffe 路径 caffe/python/caffe） 1 2 3 4 import caffe net = caffe.Net(prototxt, caffemodel, TEST) # 设置网络 net.blobs[\u0026#39;对应层的name\u0026#39;].data[...] = input # 操作输入输出 pred = net.foward() # 前向取出最终层结果, 也可以通过 pred = net.blobs[\u0026#39;name\u0026#39;].data 来拿到 C++ 前向追踪：\nnet.cpp::Forward -\u0026gt; layer.hpp::Forward -\u0026gt; 各layer的Forward (src/caffe/layers/*.cpp \u0026amp; *.cu) 自定义修改：\n自定义网络结构 ( 根据 caffe.proto ): 改 train.prototxt、solver.prototxt、test.prototxt 自定义层: 在 src/caffe/layers/* 下新增 .cpp、.cu 在 include/caffe/layers* 下新增 .hpp 改 caffe.proto，增加参数条目 部分复杂的子函数实现会放在 src/caffe/util 内 训练所需文件 *.prototxt：用于定义网络结构的文件，一般在网络本身的基础上加入了训练和测试过程所需的网络模块，以及模块相应的训练和测试用参数。 *_deploy.prototxt：同样是定义网络结构的文件，但只包含了前向推理部分，没有训练部分的模块和参数。 *_solver.prototxt：用于训练和测试的配置文件，类似学习率、学习策略、惩罚项和输出信息等，可在 $CAFFE_PATH/src/caffe/proto/caffe.proto 中的 SolverParameter 找到具体配置项信息。 上述文件的名字可随意更改，不过一般会加上这些后缀用以区分。\n路径相关 训练时需要指定 *_solver.prototxt 的路径，并在 *_solver.prototxt 指明网络 *.prototxt 的路径（也可以分开指定训练和测试用的网络），还需要定义输出模型以及状态的路径前缀 snapshot_prefix。\n一般 *.prototxt 里还需要在输入数据层指明输入数据集的路径。\n指令相关 训练：（可以加上预训练模型 -weights /路径/*.caffemodel 或 恢复训练状态 -snapshot /路径/*.solverstate，*.solverstate 是在 *.caffemodel 基础上加上了训练的状态信息）\n$CAFFE_PATH/build/tools/caffe train -solver *_solver.prototxt 测试：（需要指定训练测试用的网络和训练好的模型，测试的样本数为 TEST_PHASE 的 batch_size x iterations）\n$CAFFE_PATH/build/tools/caffe test -model *.prototxt -weights *.caffemodel -iterations 100 -gpu 0 前向：用 python 接口调用 caffe 完成，基本流程形如\n1 2 3 4 5 import caffe net = caffe.Net(prototxt_path, weights_path, caffe.TEST) net.blobs[\u0026#39;data\u0026#39;].data[...] = input_image net.forward() label = net.blobs[\u0026#39;label\u0026#39;].data Caffe 网络相关 Caffe 的网络 Net 是由各种层 Layer 组成的有向无环图，网络层通过 Blob 来存储 feature maps，具体层的实现在 $CAFFE_PATH/src/caffe/layers/ 和 $CAFFE_PATH/include/caffe/layers/ 里，其传递参数定义于 $CAFFE_PATH/src/caffe/proto/caffe.proto。网络结构文件中有什么看不懂或者需要深入了解的，找这三个地方就对了。\n训练网络一般包含训练过程所需要的所有部分，比如数据输入层（如 Data Layer），Loss 层（如 SoftmaxWithLoss Layer），定义好后 Caffe 会自己输入数据然后计算输出 loss 并更新参数。前向网络同理。\n如果需要用到 Caffe 没有的自定义网络层，需要自己编写相应 C++/CUDA 代码，放置于上述两个文件夹中，如果有传入参数还需要在 caffe.proto 中添加相应需要传入的参数配置。对于不好实现而且不需要 gpu 加速的自定义层，可以通过 python layer 来实现。\n对于有复杂操作的网络，比如 loss 需要在外部计算，则可以通过 Caffe 的 python 接口实现，在 python 环境中做训练更新。\n关于分割网络的训练说明 数据输入层 DenseImageData 是自定义层，在 dense_image_data_param 的 source 中指明了输入图片集的路径，txt 文件内的格式是：\n\u0026ldquo;样本图路径 标签路径\u0026rdquo;\n标签与样本图同等大小（224x224），单通道，其中像素值 0 为前景，1 为背景。\nloss_s8、loss 的 class_weighting 为对应标签的 loss 权重，用于解决样本不平衡。class_weighting 计算方法：对所有数据集标签统计各类别个数，比如 0 的个数，1 的个数。class_weighting_0 = num(1)/(num(1)+num(0))、class_weighting_1 = num(0)/(num(1)+num(0))。\n缺陷记录 Xavier初始化没有乘上增益 (ReLU应乘根号2, 等等) 在matlab上训练得出的模型是col-major,需要将所有矩阵参数转置才能在其他地方用 老版本caffe在初次前向时会比较慢, 新版未知 caffe 初始化数据层时启动线程是 TEST 和 TRAIN 并行进行的, 即使将test_initialization设置为false也会进行一次__TEST__的数据 prefetch, 同样会进行Transform, 所以要注意相关的共享变量. BatchNorm 的 eps 默认为 1e-5, 这个数值是 切实 会对结果产生一定影响的, 在 absorb 参数时也要注意 过程记录 后向根据top_diff和前向结果算出各blob参数的diff, 以及bottom的diff, 所以分别对blob和bottom求导 传播时记得不同微分层乘上前面的梯度值top_diff,后传多个梯度值的话全部加起来 setup是在加载网络时调用的, 加载完后不再调用 错误记录 Check failed: data_ 为 blob shape 错误, 一般是 reshape 函数出错, 也可能是网络设计错误导致 shape 传过来时负值错误 问题记录 caffe模型测试时batch_norm层的use_global_stats设为false居然没影响???? 错觉 训练过程开始良好, 中途出现后方部分卷积开始死亡(参数值非常低), 然后向前传染, 大部分卷积死亡, 表现为验证集上非常不稳定 推测是ReLU死亡 caffe 和 opencv 一起 import 会出错 added -Wl,-Bstatic -lprotobuf -Wl,-Bdynamic to LDFLAGS and removed protobuf from LIBRARIES ( 参照 https://github.com/BVLC/caffe/issues/1917 ) 犯2记录 resize层或者叫upsample upscale 层, 若训练时使用的缩放算法不同, 在卷积到比较小的时候(4x4)之类的, 会由于策略差异导致缩放前后误差非差大 test 或 upgrade 时 model 和 prototxt 写反 [libprotobuf ERROR google/protobuf/text_format.cc:274] Error parsing text-format caffe.NetParameter: 2:1: Invalid control characters encountered in text.\n\u0026hellip;..\n*** Check failure stack trace: ***\n已放弃 (核心已转储)\n二分类问题 SoftmaxWithLoss 层不要设 ignore_label, ignore_label 是会忽略该 label 的 loss 和 diff 传递, 导致结果会完全倒向另一个 label , 因为 SoftmaxWithLoss 是计算准确率来算 loss 的 常见安装问题 一般常见 protobuf 问题, 因为 Tensorflow 也用 protobuf, 不仅用, 还会自动升级 protobuf, 而 caffe 不怎么支持新版本的 protobuf, 所以如果配置了其他开源库的开发环境之后 caffe 报错了, 基本可以从几个方面检查 protobuf 有没问题.\npip list, 查看 protobuf 版本, 一般 2.6.1 比较通用, 如果是 3.5 那就换吧. 如果同时使用了 python2.7 和 python 3.5 的话那还要注意 pip 也分 pip2 和 pip3, 安装的库也分别独立. 可以在 /usr/bin, /usr/local/bin, /$HOME/.local/bin 下找到 pip 脚本, 打开就能看到它用的是 python2.7 还是 python3.5. ( 然后出现了下一个问题 ) protoc --version, protobuf 依赖的东西, 查看它的版本和 protobuf 的是否一样, 不一样的话可以通过下载相应版本 release, 或者从源码安装 protobuf. 然后在 /etc/ld.so.conf 里面添加上一行 /usr/local/lib, 然后 sudo ldconfig 更新下链接库就行了. ( 然后出现了下一个问题 ) apt list | grep \u0026quot;protobuf\u0026quot;, 有时候会有用 apt-get install 和 pip install 装了两种不同版本的 protobuf 的情况, 这时候可以 apt 删除并重新安装 protobuf ( 然后出现了下一个问题 ) File already exists in database: caffe.proto , 库链接问题或者版本问题 ( 2.6.1 不好用 ), pip uninstall protobuf 删掉 protobuf, 重启, 加 -fPIC 到 configure, 然后 ./configure --disable-shared, 然后在 protobuf 3.3 版本下 cd $PROTOBUF_BUILD_DIR/python, python setup.py build, python setup.py test, python setup.py install ( 然而出现了下一个问题 ) 还可能是 caffe 玄学问题, 总之最简单的就是直接把能用的 caffe 替换过来 make all 时出现一堆 protobuf 未定义的引用问题. ( 未解, 回溯 2.6.1 ) 2.6.1: caffe_pb2.py: syntax error, 注释掉默认 caffe 的 python/caffe/proto/caffe_pb2.py, 至于为什么项目 caffe 没有用自己的 caffe_pb2.py 而用到默认 caffe, 是因为没有成功 make pycaffe ??? 总之应该是版本问题.\nFile already exists in database: caffe.proto 依旧存在这个问题, 在 import caffe 后 import cv2 会发生, 还是需要静态链接 protobuf, 这样可以解决:\nlinking caffe against libprotobuf.a instead of libprotobuf.so could solve this issue\nI changed caffe\u0026rsquo;s Makefile. Specifically, I added -Wl,-Bstatic -lprotobuf -Wl,-Bdynamic to LDFLAGS and removed protobuf from LIBRARIES. I have uploaded my Makefile to gist (https://gist.github.com/tianzhi0549/773c8dbc383c0cb80e7b). You could check it out to see what changes I made (Line 172 and 369).\nFile \u0026quot;/usr/lib/python2.7/dist-packages/caffe/pycaffe.py\u0026quot;, line 13, in \u0026lt;module\u0026gt; from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, libcaffe.so.1.0.0: cannot open shared object file: No such file or directory. 这是 python 又喵了咪了用了默认 release 版 caffe, 删掉 /usr/lib/python2.7/dist-packages/caffe, 然后在工程头处 import sys 加sys.path.insert('/home/sad/ENet/caffe-enet/python') 和 sys.path.insert('/home/sad/ENet/caffe-enet/python/caffe') 再 import caffe , 问题终于解决!\nlibcudnn.so.5: cannot open shared object file: No such file or directory, ld 抽风, 需要专门刷新下 cuda 链接路径 :\n1 sudo ldconfig /usr/local/cuda-8.0/lib64 *** SIGSEGV (@0x100000049) received by PID 703 (TID 0x7f52cbb1c9c0) from PID 73; stack trace: *** 或者 Segmentation fault (core dumped), 可能是 python 层的使用出了问题 段错误, import caffe 退出后错误, 有可能是用了 opencv contrib 的 LIBRARY, 在 Makefile 里删掉 opencv_videoc 什么的\u0026hellip; 推荐安装方法 使用 CMake 来安装，推荐 ubuntu16.04 + gcc5.4 + python2.7 + CUDA8.0 + opencv3.4 + protobuf2.6\n实测 Ubuntu18.04 + gcc7 + python2.7 + CUDA10.2 + opencv3.4 + protobuf 3.11? 可以运行，但不支持 cudnn7.6.5。CUDA10.0 + cudnn7.3.1可以正常运作。\n","date":"2018-08-07T00:00:00Z","permalink":"/p/caffe%E4%BD%BF%E7%94%A8%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95/","title":"Caffe使用问题记录"},{"content":"这部分是将《Coordinating Filters for Faster Deep Neural Networks》中提到的 Force Regularization 和 LRA 用于实际项目的效果，虽然现在看来不是很严谨，不过算是一次很好的尝试。\n设定 为了探索 Low-Rank Approximations ( LRA ) 和 Force Regularization ( 参考 Wen. \u0026ldquo;Coordinating Filters for Faster Deep Neural Networks\u0026rdquo; ICCV 2017 ) 在我的工程上的实际效果, 进行了一些实际测试. 由于时间限制, 主要进行了两次探索, 分别为:\nLRA 单次大降秩(rank ratio 0.48) 在大降秩的基础上小降秩(rank ratio 0.8) LRA + Force Regularization 多次迭代 LRA (每次 rank ratio 0.9), FR (0.003 Degradation) 此外还有在 MNIST 上对 LeNet 的对照测试, 结果与文中叙述结论基本一致, Force Regularization 后 LRA 带来的压缩率有进一步的提高, 但主要在全连接层体现 (实验时卷积层个数完全没变), 尚未使用其他网络进行测试, 也没有观察出 Force Regularization 后卷积核的变化, 可能需要进一步实验 (调整 Force Regularization 参数, 用更好的可视化方法 t-SNE 等)\n结果 原模型结果 测试 分数 原准确率 0.8993 原召回率 0.9017 F1-Score 0.8919 LRA\n单次 0.48 大降秩后 finetune 17300 iters, 接着 0.8 小降秩 finetune 40000 iters: 测试 0.48分数 0.48+0.8分数 准确率 0.8947 0.9181 召回率 0.8868 0.8157 F1-Score 0.8813 0.8489 LRA + Force Regularization\n此版本由于原工程正在改进, 所以使用了新的方法 (修改了网络输出层的卷积个数以及输入的通道数, 准确率略微提高, 召回率变化不大)\n在新方法训练的模型下进行 0.003 Degradation 的 Force Regularization, 400 iters ( 50 iters/epoch ) 后 0.9 rank ratio 降秩, finetune 1000 iters后 继续 0.9 rank ratio 降秩, finetune 1300 iters 后得到收敛结果 测试 源模型 FR 第一次降秩 第二次降秩 准确率 0.9252 0.9207 0.9228 召回率 0.7729 0.8520 0.8443 F1-Score 0.8298 0.8731 0.8698 分析 从 LRA 可以看出, 单次大降秩也能恢复到接近源模型的效果(召回率下降大约2%), 模型大小压缩明显(12.7M =\u0026gt; 4.8M), 但是再度降秩模型效果开始较大幅度下降(召回率再度下降7%), 且模型大小变化不大(4.8M =\u0026gt; 3.7M) FR 后模型的召回率迅速降低, 但理论上在此基础上再进行多次降秩并最终 finetune 应该是能恢复效果的, 问题 loss 已经几乎收敛, 无法看出有明显下降, 召回率仍然有较大损失, 所以怀疑可能需要降低训练 force regularization, 和 learning rate, 或者有可能是 FR 未足够 finetune 的问题?? 后续 进行了FR再测试, 对原模型进行了更久的 finetune, 得到\n测试 分数 准确率 0.9194 召回率 0.9018 F1-Score 0.9030 对于速度, 进行了几个小实验, 似乎该方法在小网络上会由于增加卷积层所以减慢前向速度, 而且比起低秩增速, 卷积层的增加带来的负面影响似乎更大. 至少以本项目来说是有些许降速的.\n","date":"2018-08-07T00:00:00Z","permalink":"/p/%E5%85%B3%E4%BA%8E-lra-%E5%92%8C-force-regularization-%E7%9A%84%E6%8E%A2%E7%B4%A2/","title":"关于 LRA 和 Force Regularization 的探索"},{"content":"这部分是去年 9 月份开始的工作，算是第一次真正踏入深度学习的领域。具体工作也还算简单，就是复现一篇深度学习方法做的人脸对齐，当练练手。\n引言 因为深度学习的发展，很多传统的计算机视觉技术有了突破性进展，市面上也涌现了不少以前技术无法做到的产品，传统的像人脸检测、人脸对齐方面也有很大进步。这里就谈谈其中的一个，Deep Alignment Network [1]（下面简写 DAN）。\nDAN 是用卷积神经网络做人脸对齐的工作，大致思想就是级联卷积神经网络，每阶段都包含前一阶段的输出作为输入，输出 bias，加上 bias 并摆正人脸关键点和输入图，用 输出点生成的 heatmap + 最后一层卷积输出的特征 reshape 图 + 摆正后的原图 作为下一阶段的输入。这样就能不断修正，以达到 robust 的结果。\n实现 作者在 GitHub 上开源了代码 [2]，用的是 Theano 实现。除了验证集设置、initshape 部分冗余 和 测试的部分代码 外，其他部分应该都是没问题的，直接训练得到的结果除了 Challenging subset 稍微要差一些外，其他都和论文一致，算是比较好复现的一个了。\n我用的是 caffe 做复现，一方面是方便部署到安卓，另一方面是简单好用，改起来也容易。当时还没有 TensorFlow Lite，从各个方面来说 TensorFlow 都不太方便。当然，现在 TensorFlow 就更厉害了。\n大体上要做的事就是先实现一阶段，写好方便训练、测试用的 python 代码，把数据集封装成 hdf5。因为一阶段没用到自定义层，所以直接写出网络结构的 prototxt 和 solver 就能训练了，训练好后就能作为二阶的 pre-trained model。当然一阶相当于直接 VGG + 回归输出，所以也可以直接看到效果了，我训练出来测试结果如下（测试方法对应代码里的 centers，也就是 inter-pupil normalization error）:\nFull set (%) Common subset (%) Challenging subset (%) 6.09 5.29 9.37 因为训练过程稍有不同，参数也没怎么调，而且后面发现 heatmap 有一点小问题，这个结果和原代码一阶训练的结果有些差异（AUC 差大约 3%），不过无妨，这个结果已经比传统的方法要强得多了，我们继续二阶训练。\n二阶大部分代码可以和一阶共用，主要要做的部分就是把论文提到的几个自定义层实现，对应这四个地方：\n根据第一阶预测的结果和 mean shape 对比求出仿射变换参数 根据仿射变换参数对输入图做仿射变换，也就是对正原图啦 根据仿射变换参数对第一阶预测的结果做仿射变换，当然还要包括反变换的实现 根据对正的一阶预测结果产生 heatmap 然后还有一些 caffe 不支持的又比较常用的层，也就是 resize 层（也有叫 Interp 层或者 upsample 层，都是做插值，我个人认为最好用和部署框架相同的算法）。还有 loss 层，这个会影响到测试的结果和实际效果，我用的是和测试方法一致的度量来做 loss。\n写好这些层的代码后还有两件事要做。一是单独测试每一层的输出，确保每层前向都各自没问题；二是要做 gradient check，保证反向传播的梯度数值正确。\n完成一切之后，用一阶段模型作 pre-trained model，进行训练：\n训练过程\n结果 最终结果：\nFull set (%) Common subset (%) Challenging subset (%) 5.02 4.30 7.95 可以看到和论文结果已经很接近了，这个任务也就大致完成了。比较遗憾的是这个网络不太好替换，后来我尝试把 backbone 从 VGG 更换成其他的轻量型网络，效果都不太理想，而且一到二阶段时由于三张原尺寸图 concat 做输入导致网络参数和运算量剧增也是一个很大的问题。另外，训练过程也可以看到存在非常大的过拟合。虽然有很多地方可以改进，不过毕竟不是首要的研发项目，所以后面就没有做下去了。\n整个网络的结构框图如下：\n[参考文献]:\n[1] 《Deep Alignment Network: A convolutional neural network for robust face alignment》\n[2] github: MarekKowalski/DeepAlignmentNetwork ","date":"2018-07-19T00:00:00Z","image":"/p/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E7%9A%84%E4%BA%BA%E8%84%B8%E5%AF%B9%E9%BD%90/1_hu5c289e348364c1d3f31c9f2b61ccce7c_99390_120x120_fill_box_smart1_3.png","permalink":"/p/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E7%9A%84%E4%BA%BA%E8%84%B8%E5%AF%B9%E9%BD%90/","title":"深度学习方法的人脸对齐"},{"content":"这里是关于应用传统方法做人脸对齐的经验总结，是在去年5月到7月的工作，也是我入职后的第一个正式项目，用的是 SDM (Supervised Descent Method) [1] 的方法，具体细节可能不太记得，所以会慢慢补完。\n引言 在深度学习杀到这领域前，有两种主流的人脸对齐方法，一个是14年的号称能达到 3000FPS 的 LBF，还有一种就是13年的 SDM。由于 3000fps 复现效果不理想，实际上 SDM 比起 3000fps 精度要高一些，而且还有不错的现成代码，当时也只是想把静态的人脸对齐做了，所以就选用了 SDM。\n关于 SDM，其实是作者提出的一种非线性最小二乘优化的方法，类似牛顿步等方法，只不过回避了需要大计算量的 Hessian 矩阵和 Jacobian 矩阵的计算，人脸对齐算是它的一种应用。论文中的大致意思就是从牛顿步出发想办法把那两矩阵裹起来改为用迭代回归来学习，得到下降方向和大小，关于它更多的理论理解部分在官网上有很简单直观的介绍，这里暂时先放着。\n实现过程 我用的是 github 上 patrikhuber[2] 的开源代码，这个版本比论文的 SDM 稍微改进了一些，不过当然无论是模型大小、运算速度还是算法效果，都远不足以应用。所以我花了不少时间看论文，并在源代码基础上实现了一大堆算法模块，然后不停训练。当然很多想法实际上并没有奏效。\n那段时间现在看起来是挺盲目的，想到什么加什么，甚至还纠结用 HOG 还是 DSIFT 做特征提取，还是两者混合 SDM 迭代时切换特征提取器。在诸如此类想法上花了不少工程上的功夫以达到能够随时切换配置做训练，更重要的是浪费了训练时间。实际上这些都不是制约最终效果的瓶颈，因为当时的特征提取的范围是限定的，瓶颈并不在于特征提取器本身，况且 HOG 和 DSIFT 本身差别并不大，在瓶颈时如果不找出瓶颈下功夫而着眼于其他不确定的想法往往是不明智的。\n其实那时候很多的想法可能并不是没有奏效，而是被瓶颈掩盖了，造成了想法无效的错觉。换句话说，我们做算法的常常要评判什么想法是有效的，什么想法是无效的，而我觉得这种评判是要有所保留的，可能之所以想法无效是因为被什么我们没看到的因素所制约了，暂时行不通罢了。\n扯远了，总之当时的瞎蒙乱撞最终幸运的还是找到了几个有用的改进方法。类似《Extended Supervised Descent Method for Robust Face Alignment》[3] 里提到的，一个是关于特征提取范围以及cell数量的改进，改成了顺应 SDM 迭代过程从大到小的范围、从粗到细的计算；第二个就是分开全局和局部来进行回归。这一些改进其实算是人脸对齐中比较常见的改进了，没有太多新意，不过对于全局和局部区分回归来说，比起对最终效果的改进，这个方法对模型大小和运算速度的改进更为明显。最后为了工程上的应用，这些方法都需要大量调参。\n完成上面的这些其实也提升不了太多，关于这个项目最终提升最大的还是加入了人脸检测时得到的 5 个点做先验。也就是把 5 点扩展到 68 点，再加点 tricks 使这 68 点极其接近 ground truth，最后作为 init shape 输入到 SDM。这个方法让人脸对齐准确率和成功率大大增加，因为先验降低了回归的难度，把瓶颈推到了人脸检测时的五点回归成功率。虽然某些特殊情况可能会因为先验产生一些误导，不过也让通常情况下的对齐准确度达到非常高的地步，这是值得的。（后来我记得找到了一篇2017年7月的论文也有提到用类似我这个方法的改进版 SDM 和 LBF 来和他的深度学习方法对比的论文，只不过他用的是 JDA，我用的是 MTCNN，还有就是生成的 init shape 方法略有不同。）\n最终效果 至此，我的 SDM 人脸对齐的效果已经和当时竞品水平相当了，最终在 300w 上测试的结果为（用的 inter-pupil normalization error）\nFull set (%) Common subset (%) Challenging subset (%) 4.75 4.09 7.47 当然了，因为这个测试数字是用了五点 ground truth 做抖动的前提下得到的，所以没严格的参考价值，无法和其他公开方法做比较，不过也能反映这种方法的有效性，因为除了一些较为困难的情况，大多数时候通过 MTCNN 得到的五点都不会有太大偏差。\n最终做了三个模型，分别是有五点先验的 68 点、106 点，以及没有五点先验的 106 点。模型大小都是 5MB 左右， 在小米 mix2 上速度 20ms 左右。demo 效果大致如下：\n结语 关于训练数据集其实还有非常多要写，比如我这里只用了 300-w 数据集，在这基础上翻转、模糊、调对比度，还有关于如何将 68 点数据集扩增到 106 点的，也参考复现了几篇论文。还有的话就是安卓端的部署，包括算法的移植、接口封装，运算速度的优化，模型的压缩等等。之后有空的话可能会整理补上。\n[参考文献]:\n[1] 《Supervised Descent Method and its Applications to Face Alignment》\n[2] github: patrikhuber/superviseddescent [3] 《Extended Supervised Descent Method for Robust Face Alignment》\n","date":"2018-07-15T00:00:00Z","image":"/p/%E4%BC%A0%E7%BB%9F%E6%96%B9%E6%B3%95%E7%9A%84%E4%BA%BA%E8%84%B8%E5%AF%B9%E9%BD%90/1_hu5b645d07b6b062a706eff8df6053ecbe_497642_120x120_fill_box_smart1_3.png","permalink":"/p/%E4%BC%A0%E7%BB%9F%E6%96%B9%E6%B3%95%E7%9A%84%E4%BA%BA%E8%84%B8%E5%AF%B9%E9%BD%90/","title":"传统方法的人脸对齐"},{"content":"使用傅里叶变换的简易记录。\n原理 首先是公式\n$$ \\mathcal{F}[f(t)]=\\int_{-\\inf}^{\\inf}f(t)e^{-j\\omega t}dt $$\n其中，角频率\n$$ \\omega=k\\omega_0=k\\frac{2\\pi}{T} $$\n对于离散情况，将周期 T 用 N 点来表示，在采样频率下采一个周期共 N 点，时间 t 则用 n 表示，再根据欧拉公式\n$$ e^{jx}=cosx+jsin(x) $$\n可写成\n$$ a_k = \\mathcal{F}[f(n)]=\\sum_{n=0}^{N-1}f(n)[cos(2\\pi k\\frac{n}{N})-jsin(2\\pi k\\frac{n}{N})] $$\n同样的，逆变换： $$ f(n) = \\frac{1}{N}\\sum_{k=0}^{N-1}a_k[cos(2\\pi k\\frac{n}{N})+jsin(2\\pi k\\frac{n}{N})] $$\n实践 对称性 一般我主要是对图像或者其他实信号进行离散傅里叶变换，而因为\n$$ \\begin{align*} a_k-a_{N-k} \u0026amp;= \\sum_{n=0}^{N-1}f(n)\\cdot e^{-jk\\frac{2\\pi}{N}n} - \\sum_{n=0}^{N-1}f(n)\\cdot e^{-j(N-k)\\frac{2\\pi}{N}n} \\ \u0026amp;= \\sum_{n=0}^{N-1}f(n) [e^{-jk\\frac{2\\pi}{N}n}-1\\cdot e^{jk\\frac{2\\pi}{N}n}] \\ \u0026amp;= \\sum_{n=0}^{N-1}f(n)\\cdot 2jsin(2\\pi k\\frac{n}{N}) \\end{align*} $$\n当 f(n) 为实数时，值的实数为 0，说明傅里叶系数关于 N/2 对称，我们只需要计算前 N/2 个值就可以扩充为 N 个值。\n时域和频域 对于空域上的操作可以换算成频域上的操作，反过来也是。所以对频域上进行值加减可以等价换算成傅里叶变换后在空域全图上进行加减：\n$$ \\begin{align*} Y(j\\omega) \u0026amp;= \\sum_{n=0}^{N-1}(f(n)+g(n))\\cdot e^{-j\\omega n} \\ \u0026amp;= \\sum_{n=0}^{N-1}f(n)\\cdot e^{-j\\omega n} + \\sum_{n=0}^{N-1}g(n)\\cdot e^{-j\\omega n} \\ \u0026amp;= F(j\\omega) + G(j\\omega) \\end{align*}$$ 乘法则是对应卷积。\n变换和逆变换 对于实信号而言，若只取实部做分析，在正变换和逆变换上主要是数值大小上会差 N 倍（有些库的实现会自动做这个缩放），所以无论是对时域信号还是频域信号，做正变换或逆变换得到的结果理论上都是一样的（个别库会有差别）。\n如果是连续两次相同的变换则会导致相位上偏转 90 度。 $$ \\begin{align*} \\mathcal{F^{-1}}{\\mathcal{F}[f(n)]} \u0026amp;= \\sum_{k=0}^{N-1}\\sum_{n=0}^{N-1}[f(n)\\cdot e^{-jk \\frac{2\\pi}{N}n}]e^{jk \\frac{2\\pi}{N}i} \\ \u0026amp;= \\sum_{k=0}^{N-1}\\sum_{n=0}^{N-1}f(n)\\cdot e^{jk \\frac{2\\pi}{N}(i-n)} \\end{align*}$$\n","date":"2018-04-19T00:00:00Z","permalink":"/p/%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2%E9%9A%8F%E8%AE%B0/","title":"傅里叶变换随记"},{"content":" 这里是对零核现象的观察实验记录. 具体来说, 就是在训练卷积神经网络的过程中发现模型中有大量卷积核的 L1 变为 0 的情况, 这里为了方便简称零核现象. 最初遇到这种问题是在 DAN 训练时发现的, 当时觉得是太大学习率, ReLU 死亡, 后面降低了 lr 就没出现过了.\n直到后来做分割观察 ENet 的预训练模型时又发现了几百个零核的现象, 而且自己用 ShuffleNet 做的分割网络也出现了非常多零核, 这对模型性能显然是有很大影响的, 所以就下定决心解决这个问题.\n根据之前 DAN 的经验, 我自然先试了一下降低学习率, 结果没用, 虽然零增长的速度变慢了, 但还是会出现, 而且随着训练过程零核几乎线性增加 ( 像上图那样 ).\n把每层的零核数作纵坐标, 层数作横坐标, 打印成曲线出来就是这个样子:\n一共 2312 个零核, 简直壮观. 因为零核多集中在 depthwise 卷积上, 所以感觉上可能是由于 depthwise 卷积核太薄, 容易训练时掉坑回不来. 后面在网上也没找到多少关于这个的讨论, 唯一一个是在知乎上 关于 MobileNet V2 的回答, 也是差不多的解释, 不过我后来去掉了后面所有的 ReLU, 也是得到了很多空核, 也是个迷.\n无奈之下开始各种调超参. 一是把 batch size 加大, 讲道理更新得会稳一些, 然而并没有用, 零核依然会出现. 二是换了优化器, 用回朴素的 SGD momentum. 这时神奇的事情发生了, 不管怎么训练, 怎么调大 lr, 调小 batch size, 零核都没有出现了\u0026hellip;\nAdam SGD 最终对比了几个数据集, 从结果上来看 SGD 版比 Adam 版泛化性更强, 性能在个别数据集上也提升很大, 测试指标的标准差更是明显低于 Adam 版的, 做分割出来的边界也变得更加平滑了. 毕竟 Adam 版零核集聚在高层次上, 泛化方面有所缺陷的也是正常的. 后续 后面有空在 mnist 上做了些实验, 发现优化器中只有 Adam 和 RMSProp 肯定会产生零核, 而用其他 SGD, AdaDelta, AdaGrad 都不会产生零核. 既然如此的话, 似乎是可以从 RMSProp 中找到启示的, 但如果要验证还是得具体分析下更新过程才行, 只能暂时留坑了.\n接续 好吧, 上一次记录的结果是错误的, 并非只有 Adam 和 RMSProp 肯定会产生零核, 理论上零核的产生依旧是和参数更新的速率密不可分, 所以所有优化方法都可能产生零核. 之所以之前产生错误的结论, 是因为统计零核时采用了 L1 + 阈值 的方法, 而实际上零核表现出来的是并未完全收敛于 0, L1 差均值 10 倍以内, 但相较于其他滤波器而言判别力非常低的情况. 比如下面这种.\n下面是用 AdaGrad 优化一个普通卷积接 depthwise 卷积重复三次的简单网络, 数据集用的 fashion-mnist, 图为其中两层相邻卷积的可视化, 红橙黄绿蓝靛紫, 代表卷积核的绝对值大小, 左边为普通卷积沿通道绝对值叠加得来, 右边为 depthwise 卷积取绝对值得来.\n其后两层 可以看到, 部分卷积核已经几乎一片红了, 对其上层卷积核也产生了影响. 相比之下, SGD 训练的好的情况: 虽然还无法解明什么, 但至少说明了 depthwise 卷积不太好训练, 通过观察训练过程卷积核的变化, 可以看到 SGD + momentum 相对还是比较平稳的, 尽管有些时候可能也会漏网 ( 实际上之前用 MobileNetV2 做分割在 COCO 上预训练也有出现十几个零核... ). 先到这里, 之后一年的时间因为要专心学习, 所以大概要全面搁置了, 可能会整理记录下之前的项目. 就等之后爬上好的平台再说吧.","date":"2018-04-19T00:00:00Z","permalink":"/p/%E9%9B%B6%E6%A0%B8%E7%8E%B0%E8%B1%A1/","title":"零核现象"},{"content":"这部分是关于语义分割网络 DeepLab 系列的三篇论文。尽管经验性的技巧很多，但就效果而言还是很不错的，有不少值得参考的地方。\nDeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs 三个贡献:\n明确表明了上采样滤波器或者叫 \u0026lsquo;空洞卷积\u0026rsquo; 是 dense prediction 任务中的重要工具. 空洞卷积允许明确控制滤波器在计算特征响应后的分辨率, 也允许有效放大滤波器的感受野, 从而无计算量和参数量增加地聚合更大的上下文信息 提出了空间金字塔空洞池化 ( ASPP ), 能在多尺度上分割物体 结合 DCNN 和概率图模型提升边缘准确度 方法简述：将后面一层 pooling 和 conv stride 改为 1, 换成 atrous conv, 最后并行多个不同 rate 的 atrous conv, 然后 fuse 在一起. 加上多尺度输入, COCO 预训练, randomly rescaling 扩增, CRF, 最终得出结果。\nRethinking Atrous Convolution for Semantic Image Segmentation 针对多尺度分割, 设计了级联或并联的 atrous conv 模块, 改进了 ASPP, 没有 CRF 后处理也达到了 SOTA\nMulti-grid Method 一个 block 内几个卷积, 分不同的 dilation rate, 比如三个卷积则原来 { 1,1,1 } 可以变为 { 1,2,4 }, 然后乘上该 block 的 dilation rate. 也就是说本来要 stride 2 的, 改用 atrous conv 后, 后面 block 卷积的 dilation rate 为 2 x { 1,2,4 } = { 2,4,8 } 但是 ResNet 的 block 难道不是只有一个 3x3 ??? 1x1 哪来的 dilation ???? 原文没有提及, github 有相关讨论, 大致可能一是 google 所用 ResNet Block 加了三个 3x3 卷积, 二是指多个 bottleneck 内的 3x3 卷积做 multi-grid ASPP 加上 batch normalization dilation rate 过大会导致 valid 的 weights 减少 ( 非 pad 0 区域与 filters 区域相交减小 ), 这会导致大 dilation rate 的 filters 退化, 为解决这个问题, 且整合 global context, 使用了 image-level 的 feature. 特别的, 在模型最后的 feature map 采用 global average pooling, 然后送进 256 个 1x1 卷积 BN 中, 然后双线性插值到需要的维度. 最终 ASPP 有: 一个 1x1, 三个 3x3 dilation rate { 6,12,18 } ( 缩小 16 倍时 ), 以及 image-level feature, 最终 concat 在一起做 1x1 卷积. 其中卷积都是 256 output channel 和 BN. 原文提及主要性能提升来自于 Batch Normalization 的引入及 COCO 预训练. 实际本人尝试在小数据集上从 16s 到 8s 冻结 Batch Normalization 做 finetune, 最终效果并没有提升, 可能真的要在大量数据下才能得到较好的 Batch Normalization 参数做初始化吧\nEncoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation 融合了 Encoder-Decoder 和 SPP, 加上 depthwise 卷积的大量使用\nMethods Atrous Convolution 的 Encoder-Decoder Encoder 阶段改进 Xception + DeepLabv3, 主要改进点为 atrous separable convolution, 其实就是都大量使用 depthwise 卷积替换 ASPP 等卷积结构, 可以显著的降低计算复杂度并保持相似或更好的性能 Decoder 阶段在 DeepLabv3 输出端上采样 ( output_stride = 16 下 4 倍 ), 然后 concat 一个经过 1x1 降维后的网络低层特征, 再做 3x3 卷积, 然后上采样到原图大小. 这里也可以采用 depthwise 卷积提升效率. 修改 Xception 除输入流外加入了更多的层数 去掉 max-pooling, 以 stride depthwise 卷积替代 所有 3x3 depthwise 后加 batchnorm 和 ReLU ( 这个 ReLU 效果存疑 ) Experiments 关于 Decoder 的设计实验 这部分作者实验发现 Decoder 引入 before striding 的同分辨率 feature map 然后做 1x1 卷积压缩到 48 channel 再进行 concat 效果最好, 不过 mIoU 差距都比较小, 而且 64 channel 效果更差可以看出该选择可能与 Encoder 输出通道比例有关联, 玄学成分多些 还有就是 concat 后两个 3x3 256 卷积性能最好, 而且只做一个 skip-connection 会更高效 关于 Network Backbone 这里比较重要的一点就是 Decoder 的加入会带来 1%~2% 的性能提升, 这点在训练和测试相同 output stride 的情况下会比较明显, 不同的情况下比较不明显. ___( 这里总体计算量会增加几十 B, 个人认为在轻量级网络下不划算, 还是希望能有不会明显增加计算量的 decoder ) ___ 另外, 作者使用 Xception 实验时发现 multi-grid 不会提升性能, 于是没有使用\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;.. 关于 pretrain, 在 COCO 上 pretrain 会带来大约 2% 的提升, 在 JFT 上 pretrain backbone 会再额外带来大约 1% 的提升 Conclusion DeepLabv3+ 采用 Encoder-Decoder 结构, 将 DeepLabv3 作为 Encoder, 并引入简单高效的 skip-connection 来恢复边缘. 还有就是改进 Xception 以及采用 atrous separable convolution 降低计算量. 总的来说没什么新鲜的, 都是整合之前的方法然后通过大量实验找到的最优最高效的结构, 或许这就是炼金术吧\u0026hellip;\u0026hellip;\n","date":"2018-03-01T00:00:00Z","permalink":"/p/deeplab%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E7%AE%80%E7%95%A5%E8%AE%B0%E5%BD%95/","title":"DeepLab系列论文简略记录"},{"content":"这是关于轻量级网络 MobileNet 的改进版论文，作为万众瞩目的高效率骨干网络架构，它的更新意味着移动端网络的又一次改进。\n原文链接： Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification, Detection and Segmentation\nMain Contribution inverted residual with linear bottleneck. 输入低维压缩表征, 扩增到高维并进行 depthwise 卷积, 再通过线性卷积投射回低维表征. 该卷积结构因为不需要完全实现大的中间 feature map, 还能显著降低内存占用\nPre-Knowledge Depthwise Separable Convolutions\n广泛应用的标准卷积替代品. 将标准卷积分解为两部分, 第一部分为 depthwise convolution, 即每一个输入通道使用对应的一个卷积核来滤波, 第二部分为 pointwise convolution, 即使用 1x1 卷积将上层特征线性组合得出新的特征.\n\\(d_o\\) 个 \\(k \\cdot k\\) 标准卷积花费 \\(h_i\\cdot w_i\\cdot d_i\\cdot d_o\\cdot k^2\\), 而相对应的 depthwise 分离卷积花费 \\(h_i\\cdot w_i\\cdot d_i\\cdot k^2 + h_i\\cdot w_i\\cdot d_i\\cdot d_o\\), 相当于减少了近 \\(k^2\\) 倍的计算量 ( 实际是乘了 \\(\\frac{1}{d_o}+\\frac{1}{k^2}\\) 倍 )\nLinear Bottlenecks\n这里讨论激活层的基本属性, 文中将激活层的 feature map 看作维度 \\(h_i\\times w_i\\times d_i\\) 的激活张量.\n正式来说, 对于 \\(L_i\\) 层, 输入一组图像, 其激活组成了一个 \u0026quot; manifold of interest \u0026quot; ( 感兴趣流形? )\n这里提出了关于 manifold of interest 的一些假设, 并通过实验证明在 bottleneck 中使用非线性会损坏信息, 给出了通过在卷积 block 内插入 linear bottleneck 可以达到 capture 低维 manifold of interest 的目的\nInverted residuals\n受 bottlenecks 实际上包含所有必要信息的直觉启发, 文中直接将 bottlenecks 间作为 shortcuts\n最终该设计网络的层可以去除而不需要重新训练, 只减少一点点准确率\n信息流的解释\n文中阐述了该结构能将 building blocks 的输入和输出域很自然的分割开来, 作为网络每层的容量, 以及输入输出间的非线性函数作为表达力 ?\nArchitecture Bottleneck \u0026amp; Network:\n在作者实验中, t 在 5~10 之间得出的结果大部分差不多, 只是在小网络中小的 t 值会稍微好一点, 大的网络中大的 t 值会稍微好一点 在整体网络结构中, 第一个 bottleneck t 值为 1 会比较好, 另外在小的网络宽度下, 保留最后几层卷积层的卷积核数会提高小网络的性能 Implementation Notes 这部分主要是关于推理时内存占用的优化问题, 大致来讲, 传统的结构的内存占用由并行结构主导 ( 即残差连接之类的 ), 而这类结构需要内存为通过计算的输入和输出 tensor 的总和\n而文中提出的 building block 的内部操作均为 per-channel 的, 且随后的非 per-channel 操作具有很大的输入输出 size 比率 ( 即 bottleneck 的输入 channel 显著大于输出时的 channel ). 这样在内部操作中需要的内存占用仅仅为一个 channel 的 size, 而输出残差连接也因为 Invert Residual 减少了 channel 从而减少内存占用\nExperiments 比较关心分割方面的, 所以这里只记录关于分割的实验\n实验在 PASCAL VOC 2012 上进行, 主要是比较各种 feature extractors 下的 DeepLab V3, 以及简化 DeepLab V3 的方法, 推理时采取不同的策略来提升性能. 结论如下\n推理策略加入 multi-scale 和 left-right flipped 会显著地增加计算量, 所以在终端设备应用上不考虑 output_stride 为 16 比 8 更高效 在倒数第二层基础上做 DeepLab V3 会更高效, 因为 channel 小, 而且得到的性能相似 去掉 ASPP 会减小很多计算量而且只损失一点性能 ( ASPP 没有进行 depth-wise 改进, 参见 DeepLab V3, 所以该结论在优化后的 ASPP 上实际效果存疑 ) Ablation study 两个方面\nInvert residual connections 的有效性 违反常理的 linear bottleneck 能提升性能, 给 non-linearity 操作在 bottleneck 低维空间内损失信息的假设提供了支持 ","date":"2018-03-01T00:00:00Z","permalink":"/p/mobilenet-v2/","title":"MobileNet V2"},{"content":"这部分是关于轻量级网络 ShuffleNet 的论文记录，主要是基于 channel shuffle 的想法来减少 CNN 中占大头的 1x1 卷积的计算量。\n原文链接：ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices\n原文理解 介绍一种极其计算高效的 CNN 结构 ShuffleNet, 计算量可以控制在 10-150M FLOPs, 利用了 1x1 group conv, depth-wise conv 和 shuffle channel op.\n比起现有的方法专注于对基础网络进行 pruning, compressing, low-bit representing, 本文追求在非常有限的计算资源下 ( 几十或几百 MFLOPs ) 达到最好的效果. 目标是探索针对所需计算量范围定制非常高效的基础网络.\nResNext 和 Xception 由于 1x1 dense 卷积过多在小网络中效率不高, 本文提出 pointwise group convolution 减少 1x1 卷积的计算复杂度, 同时经过 channel shuffle 来克服边缘效应\nchannel shuffle 在小网络下 1x1 卷积数量大很昂贵, 减少通道可能会对精度产生很大的损害. 最直观的解决方法是采用 channel sparse connections, 比如 group conv. 但这又导致输出部分仅与其相应输入的部分通道产生, 导致信息流通阻塞.\n通过 channel shuffle : 输出 g x n 通道, 对输入先 reshape 到 (g, n), 转置然后 flatten. 可以优雅地解决问题.\nShuffleNet Unit 基于 bottleneck 的结构 (b) (c):\n为了简单, channel shuffle 只加在第一个 1x1 后. 由于在低功耗设备下 depth-wise conv 的 computation/memory access ratio 可能比 dense op 要差, 实际上并不能高效实施, 所以故意只在 bottleneck 中使用.\nNetwork Architecture 参考设计: 类似 ResNet, bottleneck channel 是 output channel 的 1/4, 下层 channel 翻一倍. 在 ShuffleNet 中, 大致相同计算量下, 明显可以看出, group 数越大, output channel 就要越多, 这有助于 encode 更多信息, 尽管有可能会导致单个卷积滤波器的作用降级\n另外, 在第一次 point-wise conv 时不采用 group conv, 因为输入通道数相对较小\nExperiments 和 MobileNet 一样, 这里训练使用了不那么 aggressive 的 scale augmentation, 因为小网络通常会欠拟合而不是过拟合 ( 在 ENet 的实验中也印证了这点 ), 实验结论如下:\npointwise group convolution 有效, group 比不 group 好, Smaller models tend to benefit more from groups. 0.5x 情况下 group 比较大时会饱和, 准确率甚至下降. 但再减小 channel 到 0.25x, 发现增大 group 没有饱和, 反而收益更多了. ( 此处实验不充分, 不足以证明什么 ) Channel Shuffle 很有效, 特别在大 group 时. 实际加速由于内存访问等原因在移动平台上的呈现 理论加速 4 倍 = 实际加速 2.6 倍 ","date":"2018-03-01T00:00:00Z","permalink":"/p/shufflenet/","title":"ShuffleNet"},{"content":"这篇论文是在学习压缩模型时无意中看到的，发表在 ICCV 2017。因为看到它的 motivation 觉得挺有意思的（昴星团瞩目），刚好还有代码，于是就学习了一下，顺带看看能不能用在项目上。\n原文链接： Coordinating Filters for Faster Deep Neural Networks\n原文理解 压缩和加速 DNN 模型的工作 常规压缩 sparsity-based 方法 Low-Rank Approximations ( LRA ): 可以不必经过仔细的硬件/软件设计就能压缩和加速 DNN 原理在于滤波器之间冗余(相关性), 把大的矩阵近似成两个小矩阵相乘 此工作专注于压缩已经训练好的模型来达到最大化减小计算复杂性, 然后 retrain 来保持精度 本工作注重训练出 Lower-Rank Space 的 DNN, 提出了 Force Regularization: 主要是通过引入额外梯度 ( attractive forces ) 微调参数来增强滤波器的相关性, 从而使得 LRA 后能获得更小的参数量 方法 首先介绍 cross-filter LRA :\nLRA 为将大矩阵 $$W\\in\\mathbb{R}^{N \\times C \\times H \\times W}$$ 分解为一个低秩矩阵和一个1x1的卷积偏移 $$\\beta_m\\in\\mathbb{R}^{M \\times C \\times H \\times W}, b\\in\\mathbb{R}^{1 \\times C \\times H \\times W}$$ 那么输出的 feature map 为:\n$$O_n\\approx(\\sum^M_{m=1}b_m^{(n)}\\beta_m)*I = \\sum^M_{m=1}(b_m^{(n)}F_m)$$\n这里的 $$F_m = \\beta_m * I$$ 所以输出即低秩矩阵与输入的卷积的线性组合\n然后是 Force Regularization:\n从数学层面上看 Force Regularization $$\\Delta W_i = \\sum^N_{j=1}\\Delta W_{ij} = ||W_i||\\sum^N_{j=1}(f_{ji}-f_{ji}w_i^Tw_i)$$ $$W_i \\gets W_i-\\eta \\cdot (\\frac{\\partial E(W)}{\\partial W_i}-\\lambda_s \\cdot \\Delta W_i)$$ 这里E(W)为损失，λs 为 trade off 因子，\\(f_{ji}\\) 如下：\n在物理层面上看 Force Regularization , 像是引力将参数聚集在一起\nSuppose each vector \\(w_i\\) is a rigid stick and there is a particle fixed atthe endpoint. The particle has unit mass, and the stick is massless and can freely spin around the origin. Given the pair-wise attractive forces (e.g.,universal gravitation) f_ji, Eq. (2) is the acceleration of particle \\(i\\). As the forces are attractive, neighbor particles tend to spin around the origin to assemble together.\n作者认为, 增加 Force Regularization 可以让一簇滤波器趋向于有相同的方向, 而由于数据损失梯度的存在使得该正则项不影响原本滤波器提取有判别力的特征的能力(存疑)\n实验 实验使用 baseline 作为 pretrained model. 原因是在相同最大迭代次数下, 从 baseline 开始训练比从头开始要有更好的精准度和速度提升的tradeoff, 因为 pretrained model 提供了精准度和高关联性的初始化条件.\n实验结论:\nForce Regularization 能在低层卷积保持低秩特性, 然后在高层卷积时有很大的压缩, 总体上看 rank ratio (低秩和全秩比) 大约为50%. L2norm 在高 rank ratio 时表现得比较好, L1norm 在潜在低 rank ratio 时表现得更好. ","date":"2017-11-15T00:00:00Z","image":"/p/coordinating-filters-for-faster-deep-neural-networks/ForceRegularization_hucc896f4ea16421fc559b00a1346895c8_187355_120x120_fill_box_smart1_3.png","permalink":"/p/coordinating-filters-for-faster-deep-neural-networks/","title":"Coordinating Filters for Faster Deep Neural Networks"},{"content":"看了 Kaggle 亚马逊雨林卫星图分类比赛第一名 Planet: Understanding the Amazon from Space, 1st Place Winner\u0026rsquo;s Interview, 学到了些 trick, 这里记录一下\n关于 precision 和 recall 的权衡, 可以在原有的 log loss 上增加 F_beta - score loss 来达到, 如: $$F_1 = 2\\cdot\\frac{precision\\cdot recall}{precision+recall}$$ 而 $$F_\\beta = (1+\\beta^2)\\cdot\\frac{precision\\cdot recall}{(\\beta^2\\cdot precision)+recall}$$\n使用 F2-score 时, 代表 recall 比 precision 重要（由于 β 因子大于 1，recall 的增幅对 score 要更明显）, F0.5-score, 代表 precision 比 recall 重要（同理）\n关于分类标签变量存在相关或部分对立的情况下的预测, 可以对最后一层输出的概率使用 ridge regression ( 岭回归, 即加了 L2 正则项的最小二乘, 鼓励回归参数尽可能利用到所有的相关变量, 惩罚可能存在的个别变量 dominant 的大正负值参数 cancel 得出结果的情况 ). 该方法也可以用在 ensemble 多个模型上, 最终对输出结果做 ridge regression\n","date":"2017-10-17T00:00:00Z","permalink":"/p/kaggle-amazon-from-space-%E7%BB%8F%E9%AA%8C%E5%88%86%E4%BA%AB/","title":"Kaggle \"Amazon from Space\" 经验分享"},{"content":"这部分是关于 GoogLeNet 系列网络的两篇论文，涵盖了 Inception v1 到 v3。作为 CNN 发展进程中经典的模型，它通过大量实验思考总结了很多关于 CNN 在设计方面应当注意的事项，尽管没有 VGG 那般简洁好用易训练，而且工程设计感很浓重，但其中涉及到的各种实验和结果都是实打实的，对这些实验的解读可以印证和加深自己对深度神经网络的理解，建议参考原文。\nGoing deeper with convolutions\nRethinking the Inception Architecture for Computer Vision\nGoing deeper with convolutions 介绍 介绍一种新的结构, Inception, 想法基于Hebbian principle和multi-scale processing的直觉, 可以在保持计算量恒定下加深和加宽网络 ILSVRC 2014: 相比AlexNet, GoogLeNet取得小12倍的参数以及高准确率 动机和思考 性能瓶颈, 增强性能要增加模型大小, 但又加大了过拟合可能以及对数据的需求和计算量 解决之根在于稀疏化结构, 联合 Hebbian principle (neurons that fire together, wire together) : Their main result states that if the probability distribution of the data-set is representable by a large, very sparse deep neural network, then the optimal network topology can be constructed layer by layer by analyzing the correlation statistics of the activations of the last layer and clustering neurons with highly correlated outputs. ( : \u0026ldquo;Provable bounds for learning some deep representations\u0026rdquo; )\n问题在于当前计算设备对非固定形状的稀疏矩阵的数值计算不够高效. 使用 lookups 和 cache 带来的巨大开销使得稀疏计算得不偿失. 那么是否能够利用计算设备对 dense matrices 高效计算的优势, 在稀疏的架构上进行 dense matrices 的计算? 大量文献表明: clustering sparse matrices into relatively dense submatrices tends to give state of the art practical performance for sparse matrix multiplication. ( : \u0026ldquo;On two-dimensional sparse matrix partitioning: Models, methods, and a recipe.\u0026rdquo; )\n相信不久的未来类似的方法会应用到自动构建的非固定形状的深度学习架构上\n还有一个问题, 尽管推出的结构在计算机视觉上取得了成功, 但仍然有疑惑这种成功是否能归因于指导这种网络构建的思想. 这个问题即是在于自动系统是否能使用相似的算法在其他领域创建一个总体架构看上去相差很远, 但有相似的增益效果的网络拓扑 结构细节 Inception 核心思想基于如何在卷积视觉网络中找到最优局部稀疏结构, 这种结构能被 dense components 近似和覆盖\n上层多输入聚合到下层单输出, 能够被1x1卷积表示, 加上大卷积聚合多尺度区域, 为了方便, Inception 取 1x1 3x3 和 5x5, 加入额外的并行pooling 通道也会对效果有所帮助.\n高层后空间聚合需要减少, 于是要提高 3x3 5x5 卷积的比例 为了不让后期 5x5 与 pooling聚合(限定输入输出同 channel ) 导致的计算量高昂, 加入 1x1 卷积降维 ( even low dimensional embeddings might contain a lot of information about a relatively large image patch ) 训练细节 Still, one prescription that was verified to work very well after the competition includes sampling of various sized patches of the image whose size is distributed evenly between 8% and 100% of the image area and whose aspect ratio is chosen randomly between 3/4 and 4/3. Also, we found that the photometric distortions by Andrew Howard [8] were useful to combat overfitting to some extent.\n最后 该结构在detection中也很有竞争力, 尽管没有做其他优化, 说明 Inception 结构的有效性, 会启发以后的模型结构趋向于 sparse.\nRethinking the Inception Architecture for Computer Vision 介绍: 上一篇文章提出的 Inception 略复杂很难去修改网络, 如果只是简单地加大网络会导致计算性能优势瞬间消失. 而且上一篇文章并没有详细表明设计时各部分的影响因素, 所以很难让它用于新的领域同时保持高效率. 比如, 如果要加 Inception-style 的网络的容量, 最简单的只能加倍滤波器数量, 但这样会导致计算量和参数的4倍增加. 这很不友好, 特别如果效果甚微的话. 所以这篇文章开始讲述一些通用的原理和优化 ideas, 这些都是已经验证过对高效加大网络有用的方法. 广义上的 Inception-style 结构的搭建对这些约束是可以比较灵活自然的. 这是因为它大量使用了降维和并行结构, 这让它减轻了临近结构改变带来的影响. 当然, 为了保持模型的高质量, 使用 Inception 还是需要遵循一些指导原则. 通用设计原理: 这里会描述一些通过对不同结构的 CNN 的大规模实验验证过的设计原理. 尽管有点投机的意味, 但严重背离这些原则趋向于恶化网络的质量, 改了之后一般都会有所提升.\n避免表达瓶颈 ( representational bottlenecks ), 特别是在网络的早期. 前向网络能表达为一个信息由输入到分类或回归器的有向无环图 ( acyclic graph ), 所以信息流方向是确定的, 输入到输出之间存在大量信息, 应该避免因极度压缩导致的瓶颈. 一般 表征 ( representation ) 的大小应该由输入到输出平缓地降低直到得到要直接用于任务的大小. 理论上, 信息内容不能仅仅以表征的维度来评判, 因为它舍弃了比如关联结构 ( correlation structure ) 等重要因素；维度仅仅用于提供信息内容的粗略估计. 高维表征更容易在网络局部中被处理. 增加每个卷积网络的 tile 的 activations 允许更多的解开的 ( disentangled ) 特征, 能加快网络训练. 空间聚合能通过较低维嵌入完成, 而且还不会损失表达力. 比如, 在进行一个更分散 ( more spread out ) 的卷积 ( 比如3x3 ) 之前, 在空间聚合之前降低输入表征的维度不会导致严重的不良影响. 我们推测临近的单元存在很强的关联, 如果输出是用于空间聚合的话, 导致降维损失的信息会很少. 鉴于这些信号应该很容易压缩，维度的减小甚至加快了学习. 平衡网络的宽度. 最优化网络的性能可以通过平衡每阶段的滤波器的数量和网络的深度来完成. 增加宽度和深度会使网络更高质量, 但要恒定计算量最优化提升需要两者并行增加. 计算预算因此应该用于平衡网络深度和宽度. 尽管好像挺有道理, 但以上的原则并不能直接拿来即用, 仅仅用在模棱两可的情况下会比较明智.\n分解卷积: 分解成小卷积：通过使用临近结果将 5x5 分解成两个 3x3, 并在分解卷积间使用非线性激活. 非对称卷积：将 nxn 卷积分解成 1xn + nx1 卷积可以进一步分解卷积, 但该方法在网络刚开始的地方不那么 work well, 但在中等大小的 feature map ( mxm 大小, m 在 12 到 20 之间 ) 上能取得很好的效果, 这种时候使用 1x7 和 7x1 卷积能达到很好的效果. 辅助分类器的效用 发现 辅助分类器存在与否在模型达到高准确率之前没有影响, 两者训练过程几乎完全相同. 只有在接近训练的尾声时, 带有辅助分类器的模型才会超过没有辅助分类器的模型, 最终达到一个稍高一点的收敛结果. 此外, 此前的 GoogLeNet 使用了两个辅助分类器, 而接近输入端的辅助分类器实际上去掉也不会有负面影响. 所以之前关于它们对低层特征的帮助的假设更可能时错误的. 不过作者认为辅助分类器发挥了正则化的作用, 因为主分类器的性能会由于旁路实施了 batch-normalized 或者 dropout.\n高效的网格缩小 传统的卷积网络用 pooling 来减少特征图网格大小, 为了避免表达瓶颈, 可以使用先拓宽维度再 pooling 的方法. 比如, 输入 d x d 大小 k 通道, 输出 d/2 x d/2 大小 2k 通道, 那就先做 2k stride 1 的卷积, 然后再接上 pooling. 但这样计算消耗太大, 于是作者提出分成两支, 使用 stride 2 并行 pooling 和 卷积, 最后 concat 在一起.\nInception-v2 基于上述内容作者提出了改进版的 Inception, 网络大致变化是:\n开局 7x7 变成 3 个 3x3. Inception 分成三类: 第一类用于 35x35 的 feature map, 即分解 5x5 后的传统 Inception 第二类用于 17x17 的 feature map, 即非对称卷积版 Inception, 卷积核采用 7x7 大小 第三类用于 8x8 高维特征的 feature map, 即使用并行非对称卷积加大宽度后的 Inception 每类 Inception 间采用上述的高效的网格缩小方法 使用 Label Smoothing 正则化模型 一般 label 是离散且 one-hot 的, 而使用最小化 cross entropy 来训练的模型的话相当于对 label 做最大拟然估计, 其数学形式是 Dirac delta :\n$$q(k)=\\delta_{k,y}$$ 在有限参数的情况下这是无法实现完全拟合的, 但模型却会趋向于这种形式, 这会导致两个问题: 一是不能保证泛化性能导致过拟合, 二是这种方式鼓励加大正确的预测和其他不正确预测之间的间隔, 加上在冲激处很大的梯度, 会导致模型的适应力下降. 直观来说就是对其预测太过自信了.\n为了降低模型的自信, 可以采用一个很简单的方法, 即将 label 的分布函数\n$$q(k|x)=\\delta_{k,y}$$\n改成\n$$q\u0026rsquo;(k|x)=(1-\\epsilon)\\delta_{k,y}+\\epsilon u(k)$$ 就是混上了一个固定的分布函数 u(k), 再经过平滑因子 ϵ 权重. 作者此处使用了平均分布 u(k)=1/K, 所以 $$q\u0026rsquo;(k|x)=(1-\\epsilon)\\delta_{k,y}+\\frac{\\epsilon}{K} $$ 这样每个 k 都有一个最低值, 而最高值的影响被平滑影响. 在损失函数的角度上看也可以认为 u(k) 提供了正则化项. 在 ILSVRC 2012 中, 作者采用了 u(k) = 1/1000 以及 ϵ = 0.1. 最终得到了恒定的 0.2% 的 top-1 和 top-5 效果提升.\n低分辨率输入的性能 为了比较不同输入分辨率对精准度的影响, 作者做个实验. 对三种输入分出三种网络配置 ( 为了公平对比保持计算量恒定) :\n299 × 299 receptive field with stride 2 and maximum pooling after the first layer. 151 × 151 receptive field with stride 1 and maximum pooling after the first layer. 79 × 79 receptive field with stride 1 and without pooling after the first layer. 结果如下:\nReceptive Field Size Top-1 Accuracy (single frame) 79 × 79 75.2% 151 × 151 76.4% 299 × 299 76.6% ","date":"2017-09-01T00:00:00Z","permalink":"/p/googlenet%E7%B3%BB%E5%88%97/","title":"GoogLeNet系列"},{"content":"这里是 ResNet 的论文，作为被广泛应用的骨干网络，它提出的几个概念可以说是 着实可靠 地拓宽了网络设计的思路，对于广大摸着石头过河的工程师和研究者来说就是指出了一条明路。网络本身也高效、简洁且实用，可以作为 VGG 的上位替代。\n原文链接：Deep Residual Learning for Image Recognition\n介绍 网络深度对很多视觉任务都有很强的重要性, 但是仅靠简单的层堆积会存在梯度消失/爆炸问题. 虽然使用一些归一化初始技巧和中间归一化层可以很大程度解决这些问题, 但随着深度继续增加, 网络的精准度会逐渐饱和, 接着就会快速劣化, 这就是 degradation problem. 出乎意料的是, 这些问题不是因为过拟合导致的, 实验证明增加更多的层数会导致更高的 训练误差.\n本文, 作者通过引入一个深度残差学习框架解决了这个 degradation problem. 与其让每几个堆叠的层直接学习潜在的映射, 我们显式地让这些层去学习残差映射. 正式来讲, 即输出 H(x), 输入 x, 让非线性层学习 F(x):=H(x)-x, 那么原本的映射就变成了 F(x)+x. 这在前向网络中会被视为捷径 ( \u0026ldquo;shortcut connections\u0026rdquo; ), 允许网络跳级连接, 在本文中可以简单看成 恒等映射 ( \u0026ldquo;identity mapping\u0026rdquo; ).\n经过实验发现, 深度残差网很容易训练, 且很容易让精度随网络深度的增加而增加.\n残差学习 假设多次非线性函数能够渐进逼近复杂函数, 那么在维度不变的情况下该假设与渐进逼近残差是一样的.\n之所以改为学习残差, 是因为 degradation problem 这个反常的现象. 在残差学习的恒等映射情况下, 一个更深的模型的训练误差是应该不超过比它浅的模型的. degradation problem 告诉我们 solver 使用多次非线性函数在渐进逼近恒等映射时可能存在困难, 而残差学习则提供了将权重趋向 0 来达到恒等映射的拟合.\n当然, 在现实中, 恒等映射不太可能是最优的. 但这个改动可能有助于先决这个问题. 如果最优函数接近于恒等映射而非零映射, 那么 solver 应该很容易学习扰动来得到最优函数. 通过实验发现, 通常残差函数只有很小的响应, 这表明恒等映射的可以提供合理的先决条件.\nshortcut 恒等映射 本文采用每叠一些层就使用残差学习, 也就是建立一个 block: $$y=F(x,W)+x$$ 其中: $$F = W_2\\sigma(W_1x)$$ σ 为 ReLU, 省略了 bias. 在得到 y 后再做一次 ReLU.\n如果需要变化维度, 则: $$y=F(x,W)+W_sx$$ W_s 为投影矩阵, 用来改变维度, 当然它也可以是个方阵用作线性变换, 但后续实验证明恒等变换就足够解决 degradation problem 了.\nF 是可以灵活改变的, 可以含有多层, 或者不同类型的层, 比如卷积层. 但如果 F 仅含有一层映射的话, 那还没有得到有效的效果验证.\n网络结构 为了对比, 作者构建了两个相同计算量的网络, 两种网络均为类似 VGG 的网络. 开始 7x7 stride 2, 然后 3x3, 每降低一次 feature map, double 一次 filters 数量, 使用 stride 2 卷积来降低 feature map, 堆叠 34 层和类似的 18 层. 最后做全局平均池化, 然后接 1000 分类器, 卷积后面都采用了 batch normalization 保证训练不失败.\n不同的是 ResNet 版加入了跳级连接, 降低 feature map 提升维度处分成两种策略, 一种填充 0 来扩增维度, 另一种上述提到的 1x1 升维, 两种方法都 stride 2 降低空间大小.\n实验得知, VGG 34 层表现比 18 层要差. 而且经过更多 iter 的训练发现问题没有解决.\n对于 ResNet 版, 采取第一种策略的, 34 层比 18 层错误率要少 2.8%, 更重要的, 34 层表现出明显更低的训练误差, 并且能很好泛化验证数据.\n对比两种网络的 18 层版, 发现 ResNet 能在训练早期更快的收敛.\nImageNet 上 10-crop top-1 error 实验结果如下:\nplain ResNet 18 layers 27.94 27.88 34 layers 28.54 25.03 (PS: 看上去似乎浅层下 ResNet 影响不大 ) 恒等和投影 shortcuts 对比 ResNet 不同策略, 分为三种对比:\n零填充升维, 所有 shortcuts 都没有参数 升维时使用 1x1 卷积升维, 其他恒等 所有 shortcuts 都带有 1x1 卷积\n实验表明, 2 稍微比 1 好些 (top-1 25.03-\u0026gt;24.52), 作者认为这是因为 1 在升维时的零填充部分实际并没有进行残差学习; 3 轻微比 2 好些 (top-1 24.52-\u0026gt;24.19), 作者认为这是因为 3 在 shortcuts 处引入了额外参数. 微小的差异表明投影 shortcuts 对与解决 degradation problem 不重要. 更深的瓶颈结构 为了节省训练时间, 作者把上面的 block 修改为 瓶颈 ( bottleneck ) 设计. 即在每个 $F$ 内堆叠三层, 分别为 1x1, 3x3, 1x1 卷积层. 其中 1x1 用于升降维度, 使 3x3 作为有更低输入输出维度的瓶颈.\n对于瓶颈结构来说, shortcuts 策略的选择对模型大小和时间复杂度都有很重要的影响, 无参数恒等 shortcuts 可以让瓶颈模型更高效. ( ? )\n作者在 50/101/152 层模型中使用了瓶颈结构和策略2的升降维.其各个配置如下:\n（PS: 值得注意的是 ResNet 50 层瓶颈版的 FLOPs 与 34 层相差不大, 网络的堆叠配置是一样的）\n在 ImageNet 验证集上的 10-crop 实验结果如下:\nmethod top-1 err. top-5 err. plain-34 28.54 10.02 ResNet-34 1 25.03 7.76 ResNet-34 2 24.52 7.46 ResNet-34 3 24.19 7.40 ResNet-50 22.85 6.71 ResNet-101 21.75 6.05 ResNet-152 21.43 5.71 在 ImageNet 验证集上单模型实验结果如下:\nmethod top-1 err. top-5 err. ResNet-34 2 21.84 5.71 ResNet-34 3 21.53 5.60 ResNet-50 20.74 5.25 ResNet-101 19.87 4.60 ResNet-152 19.38 4.49 最终在 ILSVRC 2015 提交中使用了两个 152 层模型的 ensembles, 得到 top-5 error 3.57. ","date":"2017-09-01T00:00:00Z","permalink":"/p/resnet/","title":"ResNet"},{"content":"8#L$h@2RxtG3!15)kX\n","date":"0001-01-01T00:00:00Z","permalink":"/p/%E6%B0%B4%E5%8C%BA/","title":"水区"}]
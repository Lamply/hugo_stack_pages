<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>基础原理 on 次二小栈</title><link>/categories/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/</link><description>Recent content in 基础原理 on 次二小栈</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><lastBuildDate>Fri, 24 Feb 2023 00:00:00 +0000</lastBuildDate><atom:link href="/categories/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/index.xml" rel="self" type="application/rss+xml"/><item><title>熵</title><link>/p/%E7%86%B5/</link><pubDate>Fri, 24 Feb 2023 00:00:00 +0000</pubDate><guid>/p/%E7%86%B5/</guid><description>&lt;p>信息量为 I(x) = log(1/p(x))，其中 log 通常是 2 底，表示为 bit 单位。其中直觉在于，当信号 x 出现的概率 p(x) 越高，则含有的信息量应当越少，而 p(x) 越低，其含有的信息量应当越大。&lt;/p>
&lt;p>熵为信息量 H(x) = E(I(x)) 的期望，可表示成：&lt;br>
$$H(x)=\sum_ip(x_i)*log\frac{1}{p(x_i)}$$&lt;/p>
&lt;p>关于一个样本集, 一般有两种分布, 预测的概率分布 q 和 真实的概率分布 p 。&lt;/p>
&lt;h2 id="交叉熵">交叉熵&lt;/h2>
&lt;p>为了编码样本集, 需要的平均编码长度为信息量的期望
$$H(X)=\sum_ip(x_i)*I(x_i)=\sum_ip(x_i)*log\frac{1}{p(x_i)}$$
因为不清楚真实的概率分布, 所以用模型预测的概率分布 q 对比采样的真实分布 p 来降低泛化误差。
$$H(p,q)=\sum_xp(x)*log\frac{1}{q(x)}$$
$H(p,q)$即为_交叉熵_, $H(p,q) &amp;gt;= H(p)$。它表示了用 q 来表示 p 时所需的信息量均值（？）。此处 x 代表 p、q 的支撑集，也就是非零的部分。&lt;/p>
&lt;h2 id="相对熵">相对熵&lt;/h2>
&lt;p>用非真实分布 q 编码所需长度多出来的 bit 就是 &lt;em>相对熵&lt;/em>, 也叫 &lt;em>KL散度&lt;/em> (&lt;em>Kullback–Leibler divergence&lt;/em>)
$$D(p||q)=H(p,q)-H(p)=\sum_ip(i)*log\frac{p(i)}{q(i)}$$&lt;br>
真实分布不改变的情况下，即 H(p) = Constant，最小化相对熵相当于最小化交叉熵。因为交叉熵计算更简单，而且训练集代表的 p 分布一般不变，所以在机器学习中通常使用交叉熵来作为代价函数。需要注意的是，KL 散度是不可互换的，在一些情况下这种性质会带来学习的不平衡（GAN）。&lt;/p>
&lt;h2 id="js-散度">JS 散度&lt;/h2>
&lt;p>$$JS(p||q)=\frac{1}{2}KL(p||\frac{p+q}{2}) + \frac{1}{2}KL(q||\frac{p+q}{2})$$&lt;br>
使得 p、q 可以互换。但是如果支撑集不重叠，则 JS 散度为常量。&lt;/p></description></item></channel></rss>
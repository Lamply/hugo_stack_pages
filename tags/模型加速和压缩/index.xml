<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>模型加速和压缩 on 次二小栈</title><link>/tags/%E6%A8%A1%E5%9E%8B%E5%8A%A0%E9%80%9F%E5%92%8C%E5%8E%8B%E7%BC%A9/</link><description>Recent content in 模型加速和压缩 on 次二小栈</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><lastBuildDate>Tue, 28 Jun 2022 00:00:00 +0000</lastBuildDate><atom:link href="/tags/%E6%A8%A1%E5%9E%8B%E5%8A%A0%E9%80%9F%E5%92%8C%E5%8E%8B%E7%BC%A9/index.xml" rel="self" type="application/rss+xml"/><item><title>TensorRT视频流推理速度异常记录</title><link>/p/tensorrt%E8%A7%86%E9%A2%91%E6%B5%81%E6%8E%A8%E7%90%86%E9%80%9F%E5%BA%A6%E5%BC%82%E5%B8%B8%E8%AE%B0%E5%BD%95/</link><pubDate>Tue, 28 Jun 2022 00:00:00 +0000</pubDate><guid>/p/tensorrt%E8%A7%86%E9%A2%91%E6%B5%81%E6%8E%A8%E7%90%86%E9%80%9F%E5%BA%A6%E5%BC%82%E5%B8%B8%E8%AE%B0%E5%BD%95/</guid><description>&lt;p>links: [[2022-06-27-Week]], [[103-问题汇总#^7d1ab9]]&lt;/p>
&lt;p>这个问题是在优化云端模型时出现的，设计了个更小的模型准备替换大的模型，结果发现模拟视频推理时耗时会出现异常波动，最终达不到优化预期。经过分析发现只在两次推理之间存在一点时间间隔时才会出现这个问题，很是诡异。因为是在云端容器上跑的，没法进行 Nsight 分析，而且我也不懂 GPU，只能默默记录下来。&lt;/p>
&lt;p>环境：&lt;/p>
&lt;ul>
&lt;li>Ubuntu 16.04&lt;/li>
&lt;li>TensorRT 7.2.1&lt;/li>
&lt;li>CUDA 10.2&lt;/li>
&lt;li>cuDNN 8.0&lt;/li>
&lt;li>Python 3.6&lt;/li>
&lt;li>PyTorch 1.7.1&lt;/li>
&lt;li>torch2trt 0.1.0&lt;/li>
&lt;li>Tesla T4&lt;/li>
&lt;li>Intel(R) Xeon(R) Platinum 8255C CPU @ 2.50GHz（up to 10-core）&lt;/li>
&lt;/ul>
&lt;p>模型构成：&lt;/p>
&lt;ol>
&lt;li>localL（优化 nnt 版本） 和 localC 模型转换成 TensorRT 引擎&lt;/li>
&lt;li>FP16 和 INT8&lt;/li>
&lt;li>1024x576 和 768x448 推理分辨率&lt;/li>
&lt;li>初始化预热 20 次&lt;/li>
&lt;/ol>
&lt;p>分解测试流程为几个步骤：&lt;/p>
&lt;ol>
&lt;li>OpenCV 从 nas 盘读取视频（I/O），转换成 YUV 字节流，以及其余模拟线上环境的配置和 log（I/O）&lt;/li>
&lt;li>脚本前处理，涉及 to device 数据传输和少量 CUDA 计算&lt;/li>
&lt;li>模型推理，里面是黑箱，涉及显卡内部计算和传输，不涉及 to host 数据传输&lt;/li>
&lt;li>脚本后处理，涉及 to host 数据传输和少量 CUDA 计算&lt;/li>
&lt;li>后续 log（I/O），以及调用 &lt;code>time.sleep&lt;/code> 系统睡眠模拟流式处理保持帧率&lt;/li>
&lt;/ol>
&lt;p>存在三种场景，未能达成可预见的统一：&lt;/p>
&lt;ol>
&lt;li>测试模型推理，采用相同输入的连续推理的形式&lt;/li>
&lt;li>测试脚本，不涉及线上环境模拟部分&lt;/li>
&lt;li>测试线上环境&lt;/li>
&lt;/ol>
&lt;p>测试方法只记录脚本内部（步骤 2，3，4）耗时，采用 &lt;code>torch.cuda.synchronize()&lt;/code>+&lt;code>timeit.default_timer()&lt;/code> 的方式严格记录每一步耗时&lt;/p>
&lt;h2 id="实验一">实验一&lt;/h2>
&lt;p>localL 1024x576 FP16 和 INT8 做 baseline&lt;/p>
&lt;p>测试线上环境，52 帧视频流：&lt;/p>
&lt;ul>
&lt;li>FP16 耗时 19ms（模型 17ms），GPU 占用 40% 左右，模型推理在一定范围内波动
&lt;ul>
&lt;li>&lt;img src="/p/tensorrt%E8%A7%86%E9%A2%91%E6%B5%81%E6%8E%A8%E7%90%86%E9%80%9F%E5%BA%A6%E5%BC%82%E5%B8%B8%E8%AE%B0%E5%BD%95/20220628163611.webp"
width="374"
height="247"
srcset="/p/tensorrt%E8%A7%86%E9%A2%91%E6%B5%81%E6%8E%A8%E7%90%86%E9%80%9F%E5%BA%A6%E5%BC%82%E5%B8%B8%E8%AE%B0%E5%BD%95/20220628163611_hu9b4516e03a97615eb57f73f74e85dd06_9778_480x0_resize_q75_h2_box_2.webp 480w, /p/tensorrt%E8%A7%86%E9%A2%91%E6%B5%81%E6%8E%A8%E7%90%86%E9%80%9F%E5%BA%A6%E5%BC%82%E5%B8%B8%E8%AE%B0%E5%BD%95/20220628163611_hu9b4516e03a97615eb57f73f74e85dd06_9778_1024x0_resize_q75_h2_box_2.webp 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="151"
data-flex-basis="363px"
>&lt;/li>
&lt;li>前后处理耗时在个别时候有剧烈变化&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>INT8 耗时 15ms（模型 13.2ms），模型推理波动很小
&lt;ul>
&lt;li>&lt;img src="/p/tensorrt%E8%A7%86%E9%A2%91%E6%B5%81%E6%8E%A8%E7%90%86%E9%80%9F%E5%BA%A6%E5%BC%82%E5%B8%B8%E8%AE%B0%E5%BD%95/20220628170236.webp"
width="386"
height="244"
srcset="/p/tensorrt%E8%A7%86%E9%A2%91%E6%B5%81%E6%8E%A8%E7%90%86%E9%80%9F%E5%BA%A6%E5%BC%82%E5%B8%B8%E8%AE%B0%E5%BD%95/20220628170236_hu5b635d502ec56feee62cd684edacef57_7148_480x0_resize_q75_h2_box_2.webp 480w, /p/tensorrt%E8%A7%86%E9%A2%91%E6%B5%81%E6%8E%A8%E7%90%86%E9%80%9F%E5%BA%A6%E5%BC%82%E5%B8%B8%E8%AE%B0%E5%BD%95/20220628170236_hu5b635d502ec56feee62cd684edacef57_7148_1024x0_resize_q75_h2_box_2.webp 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="158"
data-flex-basis="379px"
>&lt;/li>
&lt;li>前后处理耗时在个别时候有剧烈变化&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>测试脚本，循环 100 次：&lt;/p>
&lt;ul>
&lt;li>FP16 耗时 20.5ms（模型 18.8ms），GPU 占用 90% 以上，模型推理耗时依然波动
&lt;ul>
&lt;li>&lt;img src="/p/tensorrt%E8%A7%86%E9%A2%91%E6%B5%81%E6%8E%A8%E7%90%86%E9%80%9F%E5%BA%A6%E5%BC%82%E5%B8%B8%E8%AE%B0%E5%BD%95/20220628162953.webp"
width="376"
height="249"
srcset="/p/tensorrt%E8%A7%86%E9%A2%91%E6%B5%81%E6%8E%A8%E7%90%86%E9%80%9F%E5%BA%A6%E5%BC%82%E5%B8%B8%E8%AE%B0%E5%BD%95/20220628162953_hu787089145284c611b1aed997b16e9843_6684_480x0_resize_q75_h2_box_2.webp 480w, /p/tensorrt%E8%A7%86%E9%A2%91%E6%B5%81%E6%8E%A8%E7%90%86%E9%80%9F%E5%BA%A6%E5%BC%82%E5%B8%B8%E8%AE%B0%E5%BD%95/20220628162953_hu787089145284c611b1aed997b16e9843_6684_1024x0_resize_q75_h2_box_2.webp 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="151"
data-flex-basis="362px"
>&lt;/li>
&lt;li>耗时比测试线上环境高，怀疑是 GPU 占用原因&lt;/li>
&lt;li>添加 20ms 延时，耗时降低回 16.8ms 水平（比线上稍低一些），波动更明显更规律&lt;/li>
&lt;li>&lt;img src="/p/tensorrt%E8%A7%86%E9%A2%91%E6%B5%81%E6%8E%A8%E7%90%86%E9%80%9F%E5%BA%A6%E5%BC%82%E5%B8%B8%E8%AE%B0%E5%BD%95/20220628163236.webp"
width="373"
height="248"
srcset="/p/tensorrt%E8%A7%86%E9%A2%91%E6%B5%81%E6%8E%A8%E7%90%86%E9%80%9F%E5%BA%A6%E5%BC%82%E5%B8%B8%E8%AE%B0%E5%BD%95/20220628163236_hu5ed591b04cfbdd40682693e4e19c2838_8062_480x0_resize_q75_h2_box_2.webp 480w, /p/tensorrt%E8%A7%86%E9%A2%91%E6%B5%81%E6%8E%A8%E7%90%86%E9%80%9F%E5%BA%A6%E5%BC%82%E5%B8%B8%E8%AE%B0%E5%BD%95/20220628163236_hu5ed591b04cfbdd40682693e4e19c2838_8062_1024x0_resize_q75_h2_box_2.webp 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="150"
data-flex-basis="360px"
>&lt;/li>
&lt;li>前后处理耗时在个别时候有剧烈变化&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>INT8 模型推理耗时有分层情况出现，稳定后和线上环境耗时一致，延长预热后分层消失
&lt;ul>
&lt;li>&lt;img src="/p/tensorrt%E8%A7%86%E9%A2%91%E6%B5%81%E6%8E%A8%E7%90%86%E9%80%9F%E5%BA%A6%E5%BC%82%E5%B8%B8%E8%AE%B0%E5%BD%95/20220628170625.webp"
width="388"
height="249"
srcset="/p/tensorrt%E8%A7%86%E9%A2%91%E6%B5%81%E6%8E%A8%E7%90%86%E9%80%9F%E5%BA%A6%E5%BC%82%E5%B8%B8%E8%AE%B0%E5%BD%95/20220628170625_huc68952150e9055d5b0ef071f74d68e63_6454_480x0_resize_q75_h2_box_2.webp 480w, /p/tensorrt%E8%A7%86%E9%A2%91%E6%B5%81%E6%8E%A8%E7%90%86%E9%80%9F%E5%BA%A6%E5%BC%82%E5%B8%B8%E8%AE%B0%E5%BD%95/20220628170625_huc68952150e9055d5b0ef071f74d68e63_6454_1024x0_resize_q75_h2_box_2.webp 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="155"
data-flex-basis="373px"
>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>测试模型推理，预热 20 次，循环 100 次：
- FP16 耗时 20.4ms，GPU 跑满
- INT8 耗时 15.4ms，GPU 跑满&lt;/p>
&lt;p>疑问：&lt;/p>
&lt;ol>
&lt;li>为什么推理时间会有规律性波动&lt;/li>
&lt;li>为什么两次推理之间的间隔会影响 GPU 推理速度（为什么连续推理会更慢）&lt;/li>
&lt;li>为什么前后处理耗时会偶现高峰值&lt;/li>
&lt;/ol>
&lt;p>自答：&lt;/p>
&lt;ul>
&lt;li>上述几个问题都表明了一个现象，就是 TensorRT 推理时延和 GPU 利用率相关，这也是几种测试场景下出现不一致的主要原因&lt;/li>
&lt;li>GPU 利用率是统计数据，真正的原因或许在于 GPU 内部管理程序有一定的耗时，两次推理之间必需要经过这一段时间，所以利用 GPU 的空闲时间可以掩盖掉这部分耗时，但当 GPU 相当繁忙时（常有的事）这部分耗时就会显露出来&lt;/li>
&lt;li>还有的原因或许和 TensorRT 对连续推理的优化有关，这部分暂且不清楚&lt;/li>
&lt;li>所以实际测试按照 GPU 利用率作为标准或许会更好点，但照目前观察，耗时基本可以代表 GPU 利用率&lt;/li>
&lt;li>当然感觉也有可能是显卡服务器平台策略有修改了什么，不好说&amp;hellip;&lt;/li>
&lt;/ul>
&lt;h2 id="实验二">实验二&lt;/h2>
&lt;p>localC 768x448 FP16 INT8，小模型推理&lt;/p>
&lt;p>测试线上环境，8000 帧视频流：&lt;/p>
&lt;ul>
&lt;li>INT8 模型推理时间呈现诡异的分层（50 iter 跃变，100 iter 稳定），稳定后 GPU 占用 33% 左右
&lt;ul>
&lt;li>&lt;img src="/p/tensorrt%E8%A7%86%E9%A2%91%E6%B5%81%E6%8E%A8%E7%90%86%E9%80%9F%E5%BA%A6%E5%BC%82%E5%B8%B8%E8%AE%B0%E5%BD%95/20220628195746.webp"
width="374"
height="252"
srcset="/p/tensorrt%E8%A7%86%E9%A2%91%E6%B5%81%E6%8E%A8%E7%90%86%E9%80%9F%E5%BA%A6%E5%BC%82%E5%B8%B8%E8%AE%B0%E5%BD%95/20220628195746_hue5550d4672a4aea17d49fbcade932959_3506_480x0_resize_q75_h2_box_2.webp 480w, /p/tensorrt%E8%A7%86%E9%A2%91%E6%B5%81%E6%8E%A8%E7%90%86%E9%80%9F%E5%BA%A6%E5%BC%82%E5%B8%B8%E8%AE%B0%E5%BD%95/20220628195746_hue5550d4672a4aea17d49fbcade932959_3506_1024x0_resize_q75_h2_box_2.webp 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="148"
data-flex-basis="356px"
>&lt;/li>
&lt;li>可多次复现，分层的时间节点大致相同只会略有偏移&lt;/li>
&lt;li>可以确定和延时长短有关，延时越长，分层整体越往左移&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>测试脚本，循环 800 次：
- INT8 模型推理在添加 20ms 延时下呈现诡异分层，去掉延时则模型推理耗时正常（4.66ms）
- FP16 模型推理在添加 20ms 延时下也呈现诡异分层，去掉延时则模型推理耗时正常（5.3ms）&lt;/p>
&lt;p>疑问：&lt;/p>
&lt;ol>
&lt;li>为什么推理耗时会表现出阶梯渐慢的样子&lt;/li>
&lt;li>为什么最终耗时会稳定在成倍增长的地步&lt;/li>
&lt;/ol>
&lt;p>自答：&lt;/p>
&lt;ul>
&lt;li>该表现可稳定复现且有明显规律，说明很可能是撞到了某内部机制的误区&lt;/li>
&lt;li>延时越短，表现越正常；延时越长，越早出现耗时跃变，分层时间也会缩短。也就是可能和实验一是同一类问题&lt;/li>
&lt;li>需要进一步验证&lt;/li>
&lt;/ul>
&lt;h2 id="实验三">实验三&lt;/h2>
&lt;p>寻找分层规律，测试所有模型&lt;/p>
&lt;p>测试模型推理，预热 50 次，循环 200 次（值为：毫秒）：&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">分辨率/模型&lt;/th>
&lt;th style="text-align:center">localC FP16&lt;/th>
&lt;th style="text-align:center">localC INT8&lt;/th>
&lt;th style="text-align:center">localL FP16&lt;/th>
&lt;th style="text-align:center">localL INT8&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">1024x576&lt;/td>
&lt;td style="text-align:center">8.2&lt;/td>
&lt;td style="text-align:center">7.2&lt;/td>
&lt;td style="text-align:center">19.8&lt;/td>
&lt;td style="text-align:center">15.2&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">768x448&lt;/td>
&lt;td style="text-align:center">5.4&lt;/td>
&lt;td style="text-align:center">4.7&lt;/td>
&lt;td style="text-align:center">12.6&lt;/td>
&lt;td style="text-align:center">9.9&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>测试脚本，循环 800 次，延时 20ms（值为：跃变时次数 / 稳定时次数，√ 为无分层）：&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">分辨率/模型&lt;/th>
&lt;th style="text-align:center">localC FP16&lt;/th>
&lt;th style="text-align:center">localC INT8&lt;/th>
&lt;th style="text-align:center">localL FP16&lt;/th>
&lt;th style="text-align:center">localL INT8&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">1024x576&lt;/td>
&lt;td style="text-align:center">50/300&lt;/td>
&lt;td style="text-align:center">60/300&lt;/td>
&lt;td style="text-align:center">√&lt;/td>
&lt;td style="text-align:center">√&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">768x448&lt;/td>
&lt;td style="text-align:center">60/275&lt;/td>
&lt;td style="text-align:center">60/175&lt;/td>
&lt;td style="text-align:center">50/125&lt;/td>
&lt;td style="text-align:center">60/300&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>分层稳定后耗时分别为（值为：毫秒）：&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">分辨率/模型&lt;/th>
&lt;th style="text-align:center">localC FP16&lt;/th>
&lt;th style="text-align:center">localC INT8&lt;/th>
&lt;th style="text-align:center">localL FP16&lt;/th>
&lt;th style="text-align:center">localL INT8&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">1024x576&lt;/td>
&lt;td style="text-align:center">12.2&lt;/td>
&lt;td style="text-align:center">12.1&lt;/td>
&lt;td style="text-align:center">16.8&lt;/td>
&lt;td style="text-align:center">13.2&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">768x448&lt;/td>
&lt;td style="text-align:center">11.4&lt;/td>
&lt;td style="text-align:center">10.1&lt;/td>
&lt;td style="text-align:center">12.4&lt;/td>
&lt;td style="text-align:center">12.4&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>后续实验：&lt;/p>
&lt;ul>
&lt;li>增长延时后全部模型都会分层，减小延时后全部模型不会分层&lt;/li>
&lt;li>分层最终时间也与延时有关，延时越长，稳定后推理耗时越长&lt;/li>
&lt;li>将延时函数加到 测试模型推理 场景（也就是步骤 3）中，分层现象依然可以复现，所以可以排除外设总线相关的原因，问题出现在显卡内部&lt;/li>
&lt;/ul>
&lt;p>自答：&lt;/p>
&lt;ul>
&lt;li>众所周知 TensorRT 在连续推理中做了优化，或许“连续”是指在一定时间范围内，而且这个时间范围会随模型计算量改变。也就是小模型只能在更短时间内才算连续推理，而大模型则宽裕一些。然后破坏连续推理的模式就会使模型推理速度变慢，但这依然没能解释为什么会经历推理耗时正常-线性减速-耗时稳定的现象&lt;/li>
&lt;li>所以只在这种程度上分析还是有所不足，如果能在本地采用 Nsight Compute 或许能有进一步了解&lt;/li>
&lt;/ul></description></item><item><title>模型嫁接</title><link>/p/%E6%A8%A1%E5%9E%8B%E5%AB%81%E6%8E%A5/</link><pubDate>Wed, 23 Jun 2021 00:00:00 +0000</pubDate><guid>/p/%E6%A8%A1%E5%9E%8B%E5%AB%81%E6%8E%A5/</guid><description>&lt;p>源自 &lt;a class="link" href="https://github.com/fxmeng/filter-grafting" target="_blank" rel="noopener"
>https://github.com/fxmeng/filter-grafting&lt;/a>&lt;/p>
&lt;p>一种通过使用不同超参训练多个模型，并在训练过程中相互融合模型参数以达到减少无用卷积核数的方法，原理比较玄学，原论文发表在 CVPR 2020 上&lt;/p>
&lt;h2 id="融合方法">融合方法&lt;/h2>
&lt;p>根据每层模型参数的熵来自动调节融合权重 alpha&lt;br>
首先：&lt;br>
$$H(x)=\sum_i^np(x_i, x_{i+1})*log\frac{1}{p(x_i, x_{i+1})}$$&lt;br>
将参数 x 根据值大小分为 n 等分，统计在 &lt;code>[i, i+1)&lt;/code> 区间的参数数量，得到概率进而计算熵 H(x)&lt;br>
权重则为：&lt;br>
$$ alpha = \frac{A}{\pi}\cdot arctan(c\cdot (E(W_i^{M2})-E(W_i^{M1})))+0.5 $$
其中 A 和 c 为超参数，得出的 alpha 范围应该在 &lt;code>[0.5-A/2, 0.5+A/2]&lt;/code>，arctan 大概是为了将输出值域限定，并用 A、c 来调节不同熵差下的输出斜率（c 越小在零点附近就越平滑，c 越大零点附近越陡峭）&lt;/p>
&lt;h2 id="实际效果">实际效果&lt;/h2>
&lt;p>[[模型改进实验-202110221125]]&lt;/p>
&lt;p>在小模型上部分时候指标会变好，效果看起来差不多，可能会产生行为有所不同的模型。总之在图生成领域不算多有效的方法，实验的模型和任务下零核数少也可能是原因，但有时候有好过没有&lt;/p></description></item><item><title>关于 LRA 和 Force Regularization 的探索</title><link>/p/%E5%85%B3%E4%BA%8E-lra-%E5%92%8C-force-regularization-%E7%9A%84%E6%8E%A2%E7%B4%A2/</link><pubDate>Tue, 07 Aug 2018 00:00:00 +0000</pubDate><guid>/p/%E5%85%B3%E4%BA%8E-lra-%E5%92%8C-force-regularization-%E7%9A%84%E6%8E%A2%E7%B4%A2/</guid><description>&lt;p>这部分是将《Coordinating Filters for Faster Deep Neural Networks》中提到的 &lt;em>Force Regularization&lt;/em> 和 &lt;em>LRA&lt;/em> 用于实际项目的效果，虽然现在看来不是很严谨，不过算是一次很好的尝试。&lt;/p>
&lt;h2 id="设定">设定&lt;/h2>
&lt;p>为了探索 &lt;em>Low-Rank Approximations ( LRA )&lt;/em> 和 &lt;em>Force Regularization&lt;/em> ( 参考 &lt;em>Wen. &amp;ldquo;Coordinating Filters for Faster Deep Neural Networks&amp;rdquo; ICCV 2017&lt;/em> ) 在我的工程上的实际效果, 进行了一些实际测试. 由于时间限制, 主要进行了两次探索, 分别为:&lt;/p>
&lt;ul>
&lt;li>LRA
&lt;ul>
&lt;li>单次大降秩(rank ratio 0.48)&lt;/li>
&lt;li>在大降秩的基础上小降秩(rank ratio 0.8)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>LRA + Force Regularization
&lt;ul>
&lt;li>多次迭代 LRA (每次 rank ratio 0.9), FR (0.003 Degradation)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>此外还有在 MNIST 上对 LeNet 的对照测试, 结果与文中叙述结论基本一致, Force Regularization 后 LRA 带来的压缩率有进一步的提高, 但主要在全连接层体现 (实验时卷积层个数完全没变), 尚未使用其他网络进行测试, 也没有观察出 Force Regularization 后卷积核的变化, 可能需要进一步实验 (调整 Force Regularization 参数, 用更好的可视化方法 t-SNE 等)&lt;/p>
&lt;h2 id="结果">结果&lt;/h2>
&lt;ul>
&lt;li>原模型结果&lt;/li>
&lt;/ul>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">测试&lt;/th>
&lt;th style="text-align:center">分数&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">原准确率&lt;/td>
&lt;td style="text-align:center">0.8993&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">原召回率&lt;/td>
&lt;td style="text-align:center">0.9017&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">F1-Score&lt;/td>
&lt;td style="text-align:center">0.8919&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;ul>
&lt;li>LRA&lt;br>
单次 0.48 大降秩后 finetune 17300 iters, 接着 0.8 小降秩 finetune 40000 iters:&lt;/li>
&lt;/ul>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">测试&lt;/th>
&lt;th style="text-align:center">0.48分数&lt;/th>
&lt;th style="text-align:center">0.48+0.8分数&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">准确率&lt;/td>
&lt;td style="text-align:center">0.8947&lt;/td>
&lt;td style="text-align:center">0.9181&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">召回率&lt;/td>
&lt;td style="text-align:center">0.8868&lt;/td>
&lt;td style="text-align:center">0.8157&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">F1-Score&lt;/td>
&lt;td style="text-align:center">0.8813&lt;/td>
&lt;td style="text-align:center">0.8489&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;ul>
&lt;li>LRA + Force Regularization&lt;br>
此版本由于原工程正在改进, 所以使用了新的方法 (修改了网络输出层的卷积个数以及输入的通道数, 准确率略微提高, 召回率变化不大)&lt;br>
在新方法训练的模型下进行 0.003 Degradation 的 Force Regularization, 400 iters ( 50 iters/epoch ) 后 0.9 rank ratio 降秩, finetune 1000 iters后 继续 0.9 rank ratio 降秩, finetune 1300 iters 后得到收敛结果&lt;/li>
&lt;/ul>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">测试&lt;/th>
&lt;th style="text-align:center">源模型 FR&lt;/th>
&lt;th style="text-align:center">第一次降秩&lt;/th>
&lt;th style="text-align:center">第二次降秩&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">准确率&lt;/td>
&lt;td style="text-align:center">0.9252&lt;/td>
&lt;td style="text-align:center">0.9207&lt;/td>
&lt;td style="text-align:center">0.9228&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">召回率&lt;/td>
&lt;td style="text-align:center">0.7729&lt;/td>
&lt;td style="text-align:center">0.8520&lt;/td>
&lt;td style="text-align:center">0.8443&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">F1-Score&lt;/td>
&lt;td style="text-align:center">0.8298&lt;/td>
&lt;td style="text-align:center">0.8731&lt;/td>
&lt;td style="text-align:center">0.8698&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="分析">分析&lt;/h2>
&lt;ol>
&lt;li>从 LRA 可以看出, 单次大降秩也能恢复到接近源模型的效果(召回率下降大约2%), 模型大小压缩明显(12.7M =&amp;gt; 4.8M), 但是再度降秩模型效果开始较大幅度下降(召回率再度下降7%), 且模型大小变化不大(4.8M =&amp;gt; 3.7M)&lt;/li>
&lt;li>FR 后模型的召回率迅速降低, 但理论上在此基础上再进行多次降秩并最终 finetune 应该是能恢复效果的, 问题 loss 已经几乎收敛, 无法看出有明显下降, 召回率仍然有较大损失, 所以怀疑可能需要降低训练 force regularization, 和 learning rate, 或者有可能是 FR 未足够 finetune 的问题??&lt;/li>
&lt;/ol>
&lt;h2 id="后续">后续&lt;/h2>
&lt;p>进行了FR再测试, 对原模型进行了更久的 finetune, 得到&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">测试&lt;/th>
&lt;th style="text-align:center">分数&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">准确率&lt;/td>
&lt;td style="text-align:center">0.9194&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">召回率&lt;/td>
&lt;td style="text-align:center">0.9018&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">F1-Score&lt;/td>
&lt;td style="text-align:center">0.9030&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>对于速度, 进行了几个小实验, 似乎该方法在小网络上会由于增加卷积层所以减慢前向速度, 而且比起低秩增速, 卷积层的增加带来的负面影响似乎更大. 至少以本项目来说是有些许降速的.&lt;/p></description></item></channel></rss>
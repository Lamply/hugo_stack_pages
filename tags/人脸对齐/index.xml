<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>人脸对齐 on 次二小栈</title><link>/tags/%E4%BA%BA%E8%84%B8%E5%AF%B9%E9%BD%90/</link><description>Recent content in 人脸对齐 on 次二小栈</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><lastBuildDate>Thu, 19 Jul 2018 00:00:00 +0000</lastBuildDate><atom:link href="/tags/%E4%BA%BA%E8%84%B8%E5%AF%B9%E9%BD%90/index.xml" rel="self" type="application/rss+xml"/><item><title>深度学习方法的人脸对齐</title><link>/p/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E7%9A%84%E4%BA%BA%E8%84%B8%E5%AF%B9%E9%BD%90/</link><pubDate>Thu, 19 Jul 2018 00:00:00 +0000</pubDate><guid>/p/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E7%9A%84%E4%BA%BA%E8%84%B8%E5%AF%B9%E9%BD%90/</guid><description>&lt;img src="/p/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E7%9A%84%E4%BA%BA%E8%84%B8%E5%AF%B9%E9%BD%90/1.png" alt="Featured image of post 深度学习方法的人脸对齐" />&lt;p>这部分是去年 9 月份开始的工作，算是第一次真正踏入深度学习的领域。具体工作也还算简单，就是复现一篇深度学习方法做的人脸对齐，当练练手。&lt;/p>
&lt;h2 id="引言">引言&lt;/h2>
&lt;p>因为深度学习的发展，很多传统的计算机视觉技术有了突破性进展，市面上也涌现了不少以前技术无法做到的产品，传统的像人脸检测、人脸对齐方面也有很大进步。这里就谈谈其中的一个，Deep Alignment Network [1]（下面简写 DAN）。&lt;/p>
&lt;p>DAN 是用卷积神经网络做人脸对齐的工作，大致思想就是级联卷积神经网络，每阶段都包含前一阶段的输出作为输入，输出 bias，加上 bias 并摆正人脸关键点和输入图，用 输出点生成的 heatmap + 最后一层卷积输出的特征 reshape 图 + 摆正后的原图 作为下一阶段的输入。这样就能不断修正，以达到 robust 的结果。&lt;/p>
&lt;h2 id="实现">实现&lt;/h2>
&lt;p>作者在 GitHub 上开源了代码 [2]，用的是 Theano 实现。除了验证集设置、initshape 部分冗余 和 测试的部分代码 外，其他部分应该都是没问题的，直接训练得到的结果除了 Challenging subset 稍微要差一些外，其他都和论文一致，算是比较好复现的一个了。&lt;/p>
&lt;p>我用的是 caffe 做复现，一方面是方便部署到安卓，另一方面是简单好用，改起来也容易。当时还没有 TensorFlow Lite，从各个方面来说 TensorFlow 都不太方便。当然，现在 TensorFlow 就更厉害了。&lt;/p>
&lt;p>大体上要做的事就是先实现一阶段，写好方便训练、测试用的 python 代码，把数据集封装成 hdf5。因为一阶段没用到自定义层，所以直接写出网络结构的 prototxt 和 solver 就能训练了，训练好后就能作为二阶的 pre-trained model。当然一阶相当于直接 VGG + 回归输出，所以也可以直接看到效果了，我训练出来测试结果如下（测试方法对应代码里的 centers，也就是 inter-pupil normalization error）:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">Full set (%)&lt;/th>
&lt;th style="text-align:center">Common subset (%)&lt;/th>
&lt;th style="text-align:center">Challenging subset (%)&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">6.09&lt;/td>
&lt;td style="text-align:center">5.29&lt;/td>
&lt;td style="text-align:center">9.37&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>因为训练过程稍有不同，参数也没怎么调，而且后面发现 heatmap 有一点小问题，这个结果和原代码一阶训练的结果有些差异（AUC 差大约 3%），不过无妨，这个结果已经比传统的方法要强得多了，我们继续二阶训练。&lt;/p>
&lt;p>二阶大部分代码可以和一阶共用，主要要做的部分就是把论文提到的几个自定义层实现，对应这四个地方：&lt;/p>
&lt;ol>
&lt;li>根据第一阶预测的结果和 mean shape 对比求出仿射变换参数&lt;/li>
&lt;li>根据仿射变换参数对输入图做仿射变换，也就是对正原图啦&lt;/li>
&lt;li>根据仿射变换参数对第一阶预测的结果做仿射变换，当然还要包括反变换的实现&lt;/li>
&lt;li>根据对正的一阶预测结果产生 heatmap&lt;/li>
&lt;/ol>
&lt;p>然后还有一些 caffe 不支持的又比较常用的层，也就是 resize 层（也有叫 Interp 层或者 upsample 层，都是做插值，我个人认为最好用和部署框架相同的算法）。还有 loss 层，这个会影响到测试的结果和实际效果，我用的是和测试方法一致的度量来做 loss。&lt;/p>
&lt;p>写好这些层的代码后还有两件事要做。一是单独测试每一层的输出，确保每层前向都各自没问题；二是要做 gradient check，保证反向传播的梯度数值正确。&lt;/p>
&lt;p>完成一切之后，用一阶段模型作 pre-trained model，进行训练：&lt;/p>
&lt;div align=center>
&lt;img src="3.jpg" width="80%">
&lt;p>训练过程&lt;/p>
&lt;/div>
&lt;h2 id="结果">结果&lt;/h2>
&lt;p>最终结果：&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">Full set (%)&lt;/th>
&lt;th style="text-align:center">Common subset (%)&lt;/th>
&lt;th style="text-align:center">Challenging subset (%)&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">5.02&lt;/td>
&lt;td style="text-align:center">4.30&lt;/td>
&lt;td style="text-align:center">7.95&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>可以看到和论文结果已经很接近了，这个任务也就大致完成了。比较遗憾的是这个网络不太好替换，后来我尝试把 backbone 从 VGG 更换成其他的轻量型网络，效果都不太理想，而且一到二阶段时由于三张原尺寸图 concat 做输入导致网络参数和运算量剧增也是一个很大的问题。另外，训练过程也可以看到存在非常大的过拟合。虽然有很多地方可以改进，不过毕竟不是首要的研发项目，所以后面就没有做下去了。&lt;/p>
&lt;p>整个网络的结构框图如下：&lt;/p>
&lt;figure align=center>
&lt;img src="2.png" width="40%">
&lt;/figure>
&lt;p>[参考文献]:&lt;br>
[1] &lt;a class="link" href="https://arxiv.org/pdf/1706.01789.pdf" target="_blank" rel="noopener"
>《Deep Alignment Network: A convolutional neural network for robust face alignment》&lt;/a>&lt;br>
[2] github: &lt;a class="link" href="https://github.com/MarekKowalski/DeepAlignmentNetwork" target="_blank" rel="noopener"
>MarekKowalski/DeepAlignmentNetwork
&lt;/a>&lt;/p></description></item><item><title>传统方法的人脸对齐</title><link>/p/%E4%BC%A0%E7%BB%9F%E6%96%B9%E6%B3%95%E7%9A%84%E4%BA%BA%E8%84%B8%E5%AF%B9%E9%BD%90/</link><pubDate>Sun, 15 Jul 2018 00:00:00 +0000</pubDate><guid>/p/%E4%BC%A0%E7%BB%9F%E6%96%B9%E6%B3%95%E7%9A%84%E4%BA%BA%E8%84%B8%E5%AF%B9%E9%BD%90/</guid><description>&lt;img src="/p/%E4%BC%A0%E7%BB%9F%E6%96%B9%E6%B3%95%E7%9A%84%E4%BA%BA%E8%84%B8%E5%AF%B9%E9%BD%90/1.png" alt="Featured image of post 传统方法的人脸对齐" />&lt;p>这里是关于应用传统方法做人脸对齐的经验总结，是在去年5月到7月的工作，也是我入职后的第一个正式项目，用的是 SDM (Supervised Descent Method) [1] 的方法，具体细节可能不太记得，所以会慢慢补完。&lt;/p>
&lt;h2 id="引言">引言&lt;/h2>
&lt;p>在深度学习杀到这领域前，有两种主流的人脸对齐方法，一个是14年的号称能达到 3000FPS 的 LBF，还有一种就是13年的 SDM。由于 3000fps 复现效果不理想，实际上 SDM 比起 3000fps 精度要高一些，而且还有不错的现成代码，当时也只是想把静态的人脸对齐做了，所以就选用了 SDM。&lt;/p>
&lt;p>关于 SDM，其实是作者提出的一种非线性最小二乘优化的方法，类似牛顿步等方法，只不过回避了需要大计算量的 Hessian 矩阵和 Jacobian 矩阵的计算，人脸对齐算是它的一种应用。论文中的大致意思就是从牛顿步出发想办法把那两矩阵裹起来改为用迭代回归来学习，得到下降方向和大小，关于它更多的理论理解部分在官网上有很简单直观的介绍，这里暂时先放着。&lt;/p>
&lt;h2 id="实现过程">实现过程&lt;/h2>
&lt;p>我用的是 github 上 patrikhuber[2] 的开源代码，这个版本比论文的 SDM 稍微改进了一些，不过当然无论是模型大小、运算速度还是算法效果，都远不足以应用。所以我花了不少时间看论文，并在源代码基础上实现了一大堆算法模块，然后不停训练。当然很多想法实际上并没有奏效。&lt;/p>
&lt;p>那段时间现在看起来是挺盲目的，想到什么加什么，甚至还纠结用 HOG 还是 DSIFT 做特征提取，还是两者混合 SDM 迭代时切换特征提取器。在诸如此类想法上花了不少工程上的功夫以达到能够随时切换配置做训练，更重要的是浪费了训练时间。实际上这些都不是制约最终效果的瓶颈，因为当时的特征提取的范围是限定的，瓶颈并不在于特征提取器本身，况且 HOG 和 DSIFT 本身差别并不大，在瓶颈时如果不找出瓶颈下功夫而着眼于其他不确定的想法往往是不明智的。&lt;/p>
&lt;p>其实那时候很多的想法可能并不是没有奏效，而是被瓶颈掩盖了，造成了想法无效的错觉。换句话说，我们做算法的常常要评判什么想法是有效的，什么想法是无效的，而我觉得这种评判是要有所保留的，可能之所以想法无效是因为被什么我们没看到的因素所制约了，暂时行不通罢了。&lt;/p>
&lt;p>扯远了，总之当时的瞎蒙乱撞最终幸运的还是找到了几个有用的改进方法。类似《Extended Supervised Descent Method for Robust Face Alignment》[3] 里提到的，一个是关于特征提取范围以及cell数量的改进，改成了顺应 SDM 迭代过程从大到小的范围、从粗到细的计算；第二个就是分开全局和局部来进行回归。这一些改进其实算是人脸对齐中比较常见的改进了，没有太多新意，不过对于全局和局部区分回归来说，比起对最终效果的改进，这个方法对模型大小和运算速度的改进更为明显。最后为了工程上的应用，这些方法都需要大量调参。&lt;/p>
&lt;p>完成上面的这些其实也提升不了太多，关于这个项目最终提升最大的还是加入了人脸检测时得到的 5 个点做先验。也就是把 5 点扩展到 68 点，再加点 tricks 使这 68 点极其接近 ground truth，最后作为 init shape 输入到 SDM。这个方法让人脸对齐准确率和成功率大大增加，因为先验降低了回归的难度，把瓶颈推到了人脸检测时的五点回归成功率。虽然某些特殊情况可能会因为先验产生一些误导，不过也让通常情况下的对齐准确度达到非常高的地步，这是值得的。（后来我记得找到了一篇2017年7月的论文也有提到用类似我这个方法的改进版 SDM 和 LBF 来和他的深度学习方法对比的论文，只不过他用的是 JDA，我用的是 MTCNN，还有就是生成的 init shape 方法略有不同。）&lt;/p>
&lt;h2 id="最终效果">最终效果&lt;/h2>
&lt;p>至此，我的 SDM 人脸对齐的效果已经和当时竞品水平相当了，最终在 300w 上测试的结果为（用的 inter-pupil normalization error）&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">Full set (%)&lt;/th>
&lt;th style="text-align:center">Common subset (%)&lt;/th>
&lt;th style="text-align:center">Challenging subset (%)&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">4.75&lt;/td>
&lt;td style="text-align:center">4.09&lt;/td>
&lt;td style="text-align:center">7.47&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>当然了，因为这个测试数字是用了五点 ground truth 做抖动的前提下得到的，所以没严格的参考价值，无法和其他公开方法做比较，不过也能反映这种方法的有效性，因为除了一些较为困难的情况，大多数时候通过 MTCNN 得到的五点都不会有太大偏差。&lt;/p>
&lt;p>最终做了三个模型，分别是有五点先验的 68 点、106 点，以及没有五点先验的 106 点。模型大小都是 5MB 左右， 在小米 mix2 上速度 20ms 左右。demo 效果大致如下：&lt;/p>
&lt;figure align=center>
&lt;img src="2.png" width="60%">
&lt;/figure>
&lt;h2 id="结语">结语&lt;/h2>
&lt;p>关于训练数据集其实还有非常多要写，比如我这里只用了 300-w 数据集，在这基础上翻转、模糊、调对比度，还有关于如何将 68 点数据集扩增到 106 点的，也参考复现了几篇论文。还有的话就是安卓端的部署，包括算法的移植、接口封装，运算速度的优化，模型的压缩等等。之后有空的话可能会整理补上。&lt;/p>
&lt;p>[参考文献]:&lt;br>
[1] &lt;a class="link" href="https://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Xiong_Supervised_Descent_Method_2013_CVPR_paper.pdf" target="_blank" rel="noopener"
>《Supervised Descent Method and its Applications to Face Alignment》&lt;/a>&lt;br>
[2] github: &lt;a class="link" href="https://github.com/patrikhuber/superviseddescent" target="_blank" rel="noopener"
>patrikhuber/superviseddescent
&lt;/a>&lt;br>
[3] &lt;a class="link" href="http://pdfs.semanticscholar.org/5c82/0e47981d21c9dddde8d2f8020146e600368f.pdf" target="_blank" rel="noopener"
>《Extended Supervised Descent Method for Robust Face Alignment》&lt;/a>&lt;/p></description></item></channel></rss>